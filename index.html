<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.542">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Nikhitha Amireddy, Ishrath Jahan, Sai Kumar Miryala, Muhammad Usman Aslam">
<meta name="dcterms.date" content="2024-04-22">

<title>Innovative Data Exploration with LASSO: Unveiling Patterns in Earnings and Education</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="index_files/libs/clipboard/clipboard.min.js"></script>
<script src="index_files/libs/quarto-html/quarto.js"></script>
<script src="index_files/libs/quarto-html/popper.min.js"></script>
<script src="index_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="index_files/libs/quarto-html/anchor.min.js"></script>
<link href="index_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="index_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="index_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="index_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="index_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="fullcontent">

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">

<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Innovative Data Exploration with LASSO: Unveiling Patterns in Earnings and Education</h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Nikhitha Amireddy, Ishrath Jahan, Sai Kumar Miryala, Muhammad Usman Aslam </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">April 22, 2024</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<p><a href="Presentation.html">Slides</a></p>
<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>LASSO, or Least Absolute Shrinkage and Selection Operator, is a refined linear regression method introduced by Robert Tibshirani in 1996 <span class="citation" data-cites="Tibshirani1996">(<a href="#ref-Tibshirani1996" role="doc-biblioref">Tibshirani 1996</a>)</span>. This technique incorporates a penalty on the absolute values of regression coefficients, effectively reducing the risk of overfitting by eliminating less significant variables. LASSO’s ability to maintain model parsimony while enhancing interpretability makes it particularly valuable in handling large and complex datasets across various fields, including socioeconomic studies, public health, and environmental research <span class="citation" data-cites="Zhao2006 Fan2011">(<a href="#ref-Zhao2006" role="doc-biblioref">Zhao and Yu 2006</a>; <a href="#ref-Fan2011" role="doc-biblioref">Fan and Lv 2011</a>)</span>.</p>
<p>The development of LASSO has been influenced significantly by advancements such as Least Angle Regression (LARS) by Efron et al. <span class="citation" data-cites="Efron2004">(<a href="#ref-Efron2004" role="doc-biblioref">Efron et al. 2004</a>)</span>, which further highlights the method’s capability in feature selection and regularization. This innovation has led to LASSO’s widespread adoption in disciplines requiring a clear interpretation of intricate data dynamics, such as economic forecasting and health diagnostics <span class="citation" data-cites="Hastie2009 Belloni2013">(<a href="#ref-Hastie2009" role="doc-biblioref">T. Hastie, Tibshirani, and Friedman 2009</a>; <a href="#ref-Belloni2013" role="doc-biblioref">Belloni and Chernozhukov 2013</a>)</span>.</p>
<p>Moreover, the integration of logistic regression with LASSO has opened new avenues for modeling categorical outcomes, enhancing the predictive accuracy and interpretability in various applications from epidemiology to finance <span class="citation" data-cites="Simon2013 Li2022">(<a href="#ref-Simon2013" role="doc-biblioref">Simon and al. 2013</a>; <a href="#ref-Li2022" role="doc-biblioref">Li and al. 2022</a>)</span>. As the complexity of datasets increases in the 21st century, LASSO remains a cornerstone in the toolkit of data scientists, aiding in the precise and simplified exploration of multifaceted data landscapes <span class="citation" data-cites="Chintalapudi2022 Friedman2010">(<a href="#ref-Chintalapudi2022" role="doc-biblioref">Chintalapudi and al. 2022</a>; <a href="#ref-Friedman2010" role="doc-biblioref">Friedman, Hastie, and Tibshirani 2010</a>)</span>.</p>
</section>
<section id="literature-review" class="level2">
<h2 class="anchored" data-anchor-id="literature-review">Literature Review</h2>
<p>LASSO regression’s versatility across multiple fields illustrates its capability to manage complex datasets effectively, particularly with continuous outcomes. In the field of economics, Zhou et al.&nbsp;(2022)<span class="citation" data-cites="Zhou2022">(<a href="#ref-Zhou2022" role="doc-biblioref">Zhou and al. 2022</a>)</span> demonstrated LASSO’s effectiveness in isolating key economic predictors that are crucial for strategic decision-making. This application highlights its utility in economic analysis, where identifying factors that directly influence continuous outcomes like wages or economic growth is essential.</p>
<section id="applications-across-fields" class="level3">
<h3 class="anchored" data-anchor-id="applications-across-fields">Applications Across Fields</h3>
<ul>
<li><p><strong>Economics</strong>: Zhou et al.&nbsp;(2022)<span class="citation" data-cites="Zhou2022">(<a href="#ref-Zhou2022" role="doc-biblioref">Zhou and al. 2022</a>)</span> highlighted LASSO’s ability to identify key economic predictors that assist in strategic decision-making. This example underscores its utility in economic analysis, where it helps to isolate factors that directly influence continuous economic outcomes like wages, prices, or economic growth.</p></li>
<li><p><strong>Bioinformatics</strong>: Lu et al.&nbsp;(2011)<span class="citation" data-cites="Lu2011 Musoro2014">(<a href="#ref-Lu2011" role="doc-biblioref">Yiming Lu 2011</a>; <a href="#ref-Musoro2014" role="doc-biblioref">Musoro 2014</a>)</span> used LASSO regression to develop models based on gene expression data, advancing our understanding of genetic influences on continuous traits and diseases. Their work illustrates how LASSO can handle vast amounts of biological data to pinpoint critical genetic pathways.</p></li>
<li><p><strong>Environmental Science</strong>: Wang et al.&nbsp;(2017)<span class="citation" data-cites="Wang2017">(<a href="#ref-Wang2017" role="doc-biblioref">Wang and al. 2017</a>)</span> applied LASSO to predict fuel consumption—a continuous variable—in maritime operations. This research supports sustainability efforts by providing precise predictions that help reduce environmental impact.</p></li>
<li><p><strong>Public Health</strong>: McEligot et al.&nbsp;(2020)<span class="citation" data-cites="McEligot2020">(<a href="#ref-McEligot2020" role="doc-biblioref">McEligot et al. 2020</a>)</span> employed logistic LASSO to explore how dietary factors, which vary continuously, affect the risk of developing breast cancer. Their findings highlight LASSO’s strength in dealing with complex, high-dimensional datasets in health sciences.</p></li>
</ul>
</section>
<section id="continuous-data-analysis" class="level3">
<h3 class="anchored" data-anchor-id="continuous-data-analysis">Continuous Data Analysis</h3>
<p>The consistent evolution of LASSO regression <span class="citation" data-cites="Muthukrishnan2016 Friedman2010">(<a href="#ref-Muthukrishnan2016" role="doc-biblioref">Muthukrishnan and Rohini 2016</a>; <a href="#ref-Friedman2010" role="doc-biblioref">Friedman, Hastie, and Tibshirani 2010</a>)</span> reflects its increased adoption in fields that require robust statistical tools to analyze extensive and intricate data sets. Its effectiveness in managing continuous variables makes it particularly valuable in predictive modeling and data analysis.</p>
</section>
<section id="model-optimization-and-comparison" class="level3">
<h3 class="anchored" data-anchor-id="model-optimization-and-comparison">Model Optimization and Comparison</h3>
<p>To ensure the accuracy and reliability of our LASSO regression model in predicting continuous wage outcomes, we implemented k-fold cross-validation<span class="citation" data-cites="James2013">(<a href="#ref-James2013" role="doc-biblioref">James et al. 2013</a>)</span>. This technique is crucial for evaluating model performance on unseen data, helping to fine-tune the regularization parameter (lambda) which balances model complexity against predictive accuracy <span class="citation" data-cites="Hastie2009">(<a href="#ref-Hastie2009" role="doc-biblioref">T. Hastie, Tibshirani, and Friedman 2009</a>)</span>.</p>
<p>After refining our model, we compared its performance to that of Multiple Linear Regression (MLR). This comparison was essential to illustrate how each approach handles multicollinearity and overfitting, particularly in the context of continuous data <span class="citation" data-cites="Friedman2010">(<a href="#ref-Friedman2010" role="doc-biblioref">Friedman, Hastie, and Tibshirani 2010</a>)</span>. The analysis of model coefficients showed that LASSO effectively simplifies the model while maintaining excellent predictive capability, even with many predictors or potential for overfitting <span class="citation" data-cites="Tibshirani1996">(<a href="#ref-Tibshirani1996" role="doc-biblioref">Tibshirani 1996</a>)</span>.</p>
<p>Our comprehensive analysis underscores LASSO’s utility in producing models that are more parsimonious compared to MLR, particularly in scenarios with numerous predictors and potential overfitting <span class="citation" data-cites="Tibshirani1996">(<a href="#ref-Tibshirani1996" role="doc-biblioref">Tibshirani 1996</a>)</span>. The results affirm the value of LASSO in modern statistical analysis and predictive modeling, as it enhances model interpretability and ensures robustness against the complexities inherent in large datasets.</p>
</section>
</section>
<section id="methodology" class="level2">
<h2 class="anchored" data-anchor-id="methodology">Methodology</h2>
<section id="lasso-regression" class="level3">
<h3 class="anchored" data-anchor-id="lasso-regression">LASSO Regression</h3>
<p>LASSO (Least Absolute Shrinkage and Selection Operator) regression, introduced by Robert Tibshirani in 1996, enhances traditional linear regression by adding a penalty for the size of the coefficients. This approach is particularly valuable in complex datasets with many potential predictors, some of which may not significantly influence the outcome. By penalizing the magnitude of the coefficients, LASSO helps in selecting only the most significant variables, simplifying the model and improving interpretability. <span class="citation" data-cites="Tibshirani1996">(<a href="#ref-Tibshirani1996" role="doc-biblioref">Tibshirani 1996</a>)</span></p>
</section>
<section id="mathematical-formulation" class="level3">
<h3 class="anchored" data-anchor-id="mathematical-formulation">Mathematical Formulation</h3>
<p>The core of LASSO regression is captured by the following objective function:</p>
<p><span class="math display">\[
\min_{\beta} \left\{ \frac{1}{2n} \sum_{i=1}^{n} (y_i - \beta_0 - \sum_{j=1}^{p} \beta_j x_{ij})^2 + \lambda \sum_{j=1}^{p} |\beta_j| \right\}
\]</span></p>
<p>This formula aims to minimize the residual sum of squares (RSS) while imposing a penalty proportional to the sum of the absolute values of the coefficients, regulated by the parameter <span class="math inline">\(\lambda\)</span>. This balancing act helps manage model complexity and prevents overfitting, especially important in datasets with many predictors. <span class="citation" data-cites="Zhao2006 Friedman2010">(<a href="#ref-Zhao2006" role="doc-biblioref">Zhao and Yu 2006</a>; <a href="#ref-Friedman2010" role="doc-biblioref">Friedman, Hastie, and Tibshirani 2010</a>)</span></p>
</section>
<section id="understanding-the-outcome-variable" class="level3">
<h3 class="anchored" data-anchor-id="understanding-the-outcome-variable">Understanding the Outcome Variable</h3>
<p>A crucial aspect of our methodology was determining the nature of our outcome variable, <code>wage76</code>, which we identified as continuous. This was based on statistical analysis and visualization of the data, showing a range of wage values without distinct groupings or categories. This understanding is vital as it dictates the choice of LASSO regression, which is well-suited for models with continuous outcomes due to its ability to perform regularization and feature selection effectively.</p>
</section>
<section id="the-shrinkage-effect-and-its-benefits" class="level3">
<h3 class="anchored" data-anchor-id="the-shrinkage-effect-and-its-benefits">The Shrinkage Effect and Its Benefits</h3>
<p>LASSO’s dual capability of coefficient shrinkage and feature selection is instrumental in our analysis:</p>
<ul>
<li><strong>Feature Selection</strong>: In high-dimensional datasets, irrelevant predictors can obscure true relationships. LASSO counters this by reducing the coefficients of non-essential predictors to zero, thus highlighting the variables that genuinely impact the outcome. <span class="citation" data-cites="Meinshausen2006">(<a href="#ref-Meinshausen2006" role="doc-biblioref">Meinshausen and Bühlmann 2006</a>)</span></li>
<li><strong>Regularization</strong>: This process helps in avoiding overfitting, making the model more generalizable and reliable for predicting new data. It is especially critical when dealing with complex models derived from large datasets. <span class="citation" data-cites="Hastie2009">(<a href="#ref-Hastie2009" role="doc-biblioref">T. Hastie, Tibshirani, and Friedman 2009</a>)</span></li>
</ul>
</section>
<section id="role-of-the-regularization-parameter-lambda" class="level3">
<h3 class="anchored" data-anchor-id="role-of-the-regularization-parameter-lambda">Role of the Regularization Parameter (<span class="math inline">\(\lambda\)</span>)</h3>
<p>Selecting an appropriate <span class="math inline">\(\lambda\)</span> is key:</p>
<ul>
<li>At <span class="math inline">\(\lambda = 0\)</span>, LASSO equals an ordinary least squares regression, offering no coefficient shrinkage.</li>
<li>Increasing <span class="math inline">\(\lambda\)</span> enhances the penalty, pushing more coefficients to zero, which simplifies the model and focuses on the most impactful variables.</li>
<li>The optimal <span class="math inline">\(\lambda\)</span> is usually determined through cross-validation, ensuring the model strikes the right balance between bias and variance. <span class="citation" data-cites="Friedman2010">(<a href="#ref-Friedman2010" role="doc-biblioref">Friedman, Hastie, and Tibshirani 2010</a>)</span></li>
</ul>
</section>
<section id="practical-application-and-model-comparison" class="level3">
<h3 class="anchored" data-anchor-id="practical-application-and-model-comparison">Practical Application and Model Comparison</h3>
<p>In practice, LASSO’s ability to discern key predictors makes it invaluable across various fields, including finance and healthcare, where precise models can significantly impact decision-making.</p>
<p>Additionally, comparing LASSO with Multiple Linear Regression (MLR) illustrates its superiority in handling datasets with numerous predictors. While MLR provides a baseline by fitting data without regularization, LASSO advances this by selectively including only the most relevant variables, thereby preventing overfitting and enhancing the model’s predictive accuracy. <span class="citation" data-cites="Park2008 Belloni2013">(<a href="#ref-Park2008" role="doc-biblioref">Park and Hastie 2008</a>; <a href="#ref-Belloni2013" role="doc-biblioref">Belloni and Chernozhukov 2013</a>)</span></p>
</section>
<section id="advantages-of-lasso-regression" class="level3">
<h3 class="anchored" data-anchor-id="advantages-of-lasso-regression">Advantages of LASSO Regression</h3>
<p>LASSO regression is highly valued in fields ranging from healthcare to finance due to its ability to simplify complex models without sacrificing accuracy. The method’s key strengths include:</p>
<ul>
<li><strong>Feature Selection</strong>: LASSO can set some coefficients exactly to zero, effectively choosing the most relevant variables from many possibilities. This automatic feature selection helps focus the model on the truly impactful factors. <span class="citation" data-cites="Park2008">(<a href="#ref-Park2008" role="doc-biblioref">Park and Hastie 2008</a>)</span></li>
<li><strong>Model Interpretability</strong>: By eliminating irrelevant variables, LASSO makes the resulting models easier to understand and communicate, enhancing their practical use. <span class="citation" data-cites="Belloni2013">(<a href="#ref-Belloni2013" role="doc-biblioref">Belloni and Chernozhukov 2013</a>)</span></li>
<li><strong>Mitigation of Multicollinearity</strong>: LASSO addresses issues that arise when predictor variables are highly correlated. It selects one variable from a group of closely related variables, which simplifies the model and avoids redundancy. <span class="citation" data-cites="Efron2004">(<a href="#ref-Efron2004" role="doc-biblioref">Efron et al. 2004</a>)</span></li>
</ul>
</section>
<section id="cross-validation-for-model-tuning" class="level3">
<h3 class="anchored" data-anchor-id="cross-validation-for-model-tuning">Cross-Validation for Model Tuning</h3>
<p>To ensure that our LASSO model performs well on new, unseen data, we used a technique called k-fold cross-validation. This approach tests the model’s effectiveness across different subsets of the dataset:</p>
<ul>
<li><strong>Methodology</strong>: We divide the data into ‘k’ equal parts, or folds. The model is trained on ‘k-1’ of these folds, with the remaining part used as a test set. This process repeats so that each fold serves as a test set once, allowing us to use all data for both training and validation.</li>
<li><strong>Optimization</strong>: We calculate the cross-validation error for each value of the regularization parameter <span class="math inline">\(\lambda\)</span>. This error is the average of the errors obtained from each fold and helps us find the best <span class="math inline">\(\lambda\)</span> that minimizes these errors:</li>
</ul>
<p><span class="math display">\[
CVE(\lambda) = \frac{1}{k} \sum_{i=1}^{k} MSE_i(\lambda)
\]</span></p>
<p>Where <span class="math inline">\(MSE_i(\lambda)\)</span> represents the mean squared error on the ith fold. <span class="citation" data-cites="Hastie2015 Tibshirani2021">(<a href="#ref-Hastie2015" role="doc-biblioref">Trevor Hastie, Tibshirani, and Wainwright 2015</a>; <a href="#ref-Tibshirani2021" role="doc-biblioref">Tibshirani 2021</a>)</span></p>
</section>
<section id="comparing-lasso-regression-to-multiple-linear-regression" class="level3">
<h3 class="anchored" data-anchor-id="comparing-lasso-regression-to-multiple-linear-regression">Comparing LASSO Regression to Multiple Linear Regression</h3>
<p>To highlight LASSO’s effectiveness, we compare it with Multiple Linear Regression (MLR), which models the relationship between a dependent variable and multiple predictors without regularization:</p>
<p><span class="math display">\[
y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \ldots + \beta_n x_n + \epsilon
\]</span></p>
<p>MLR provides a baseline by estimating how each predictor affects the dependent variable, assuming all other variables are held constant. Here’s how MLR breaks down:</p>
<ul>
<li><strong>Intercept (<span class="math inline">\(\beta_0\)</span>)</strong>: Represents the expected value of <span class="math inline">\(y\)</span> when all predictors are zero.</li>
<li><strong>Coefficients (<span class="math inline">\(\beta_1, \beta_2, ..., \beta_n\)</span>)</strong>: Each coefficient indicates the change in <span class="math inline">\(y\)</span> associated with a one-unit change in the respective predictor.</li>
<li><strong>Error Term (<span class="math inline">\(\epsilon\)</span>)</strong>: Captures the variability in <span class="math inline">\(y\)</span> not explained by the predictors.</li>
</ul>
<p>In scenarios with high-dimensional data, LASSO’s ability to reduce some coefficients to zero provides a clear advantage over MLR. It simplifies the model, which can improve both interpretability and prediction accuracy, avoiding the common pitfalls of overfitting that MLR might suffer from.</p>
</section>
</section>
<section id="analysis-and-results" class="level2">
<h2 class="anchored" data-anchor-id="analysis-and-results">Analysis and Results</h2>
<section id="data-description" class="level3">
<h3 class="anchored" data-anchor-id="data-description">Data Description</h3>
<p>Understanding the variables in the <code>RetSchool</code> dataset is crucial for our analysis. These variables give us insights into the socio-economic and educational environment of 1976, helping us explore factors that influence wages and educational outcomes. Below is a table summarizing the key variables and their roles:</p>
<table class="table">
<colgroup>
<col style="width: 25%">
<col style="width: 31%">
<col style="width: 19%">
<col style="width: 23%">
</colgroup>
<thead>
<tr class="header">
<th>Variable</th>
<th>Description</th>
<th>Type</th>
<th>Relevance</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>wage76</code></td>
<td>Wages of individuals in 1976</td>
<td>Continuous</td>
<td>Primary measure of economic status, used to explore wage disparities.</td>
</tr>
<tr class="even">
<td><code>age76</code></td>
<td>Age of individuals in 1976</td>
<td>Continuous</td>
<td>Helps analyze the age distribution’s impact on wages.</td>
</tr>
<tr class="odd">
<td><code>grade76</code></td>
<td>Highest grade completed by 1976</td>
<td>Continuous</td>
<td>Indicates educational attainment and its correlation with economic success.</td>
</tr>
<tr class="even">
<td><code>col4</code></td>
<td>Whether individuals received college education by 1976</td>
<td>Binary</td>
<td>Differentiates the impact of higher education on wages.</td>
</tr>
<tr class="odd">
<td><code>exp76</code></td>
<td>Years of work experience by 1976</td>
<td>Continuous</td>
<td>Examines how work experience influences wages.</td>
</tr>
<tr class="even">
<td><code>momdad14</code></td>
<td>Whether lived with both parents at age 14</td>
<td>Binary</td>
<td>Assesses the impact of family structure on early life outcomes.</td>
</tr>
<tr class="odd">
<td><code>sinmom14</code></td>
<td>Whether lived with a single mother at age 14</td>
<td>Binary</td>
<td>Similar to <code>momdad14</code>, focuses on single-mother households.</td>
</tr>
<tr class="even">
<td><code>daded</code></td>
<td>Education level of father</td>
<td>Continuous</td>
<td>Explores how paternal education affects offspring’s outcomes.</td>
</tr>
<tr class="odd">
<td><code>momed</code></td>
<td>Education level of mother</td>
<td>Continuous</td>
<td>Similar to <code>daded</code>, but focuses on maternal education.</td>
</tr>
<tr class="even">
<td><code>black</code></td>
<td>Whether individuals are identified as black</td>
<td>Binary</td>
<td>Used to analyze racial disparities within the dataset.</td>
</tr>
<tr class="odd">
<td><code>south76</code></td>
<td>Whether individuals resided in the South in 1976</td>
<td>Binary</td>
<td>Important for regional economic analysis.</td>
</tr>
<tr class="even">
<td><code>region</code></td>
<td>Geographic region classification</td>
<td>Categorical</td>
<td>Enhances the analysis of regional influences on outcomes.</td>
</tr>
<tr class="odd">
<td><code>smsa76</code></td>
<td>Residence within a Standard Metropolitan Statistical Area in 1976</td>
<td>Binary</td>
<td>Relevant for examining urban versus rural disparities.</td>
</tr>
</tbody>
</table>
<p>This table categorizes the variables based on their type and relevance to our study, setting the stage for a detailed exploration of how these factors interplay to shape economic outcomes during the mid-1970s.</p>
<p><strong>Data Cleaning and Exploration</strong></p>
<p>Initial exploration of the <code>RetSchool</code> dataset revealed a diverse range of variables, necessitating meticulous data cleaning to ensure accuracy in our subsequent analyses. Missing values were addressed through imputation or removal, refining our dataset for a comprehensive examination.</p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>knitr<span class="sc">::</span>opts_chunk<span class="sc">$</span><span class="fu">set</span>(<span class="at">echo =</span> <span class="cn">FALSE</span>, <span class="at">message =</span> <span class="cn">FALSE</span>, <span class="at">warning =</span> <span class="cn">FALSE</span>, <span class="at">results =</span> <span class="st">'hide'</span>)</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(corrplot)</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(gridExtra)</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>df <span class="ot">&lt;-</span> <span class="fu">read_csv</span>(<span class="st">"https://vincentarelbundock.github.io/Rdatasets/csv/Ecdat/RetSchool.csv"</span>, <span class="at">show_col_types =</span> <span class="cn">FALSE</span>)</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>df_clean <span class="ot">&lt;-</span> df <span class="sc">%&gt;%</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>    <span class="at">grade76 =</span> <span class="fu">ifelse</span>(<span class="fu">is.na</span>(grade76), <span class="fu">median</span>(grade76, <span class="at">na.rm =</span> <span class="cn">TRUE</span>), grade76),</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>    <span class="at">exp76 =</span> <span class="fu">ifelse</span>(<span class="fu">is.na</span>(exp76), <span class="fu">median</span>(exp76, <span class="at">na.rm =</span> <span class="cn">TRUE</span>), exp76)</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">na.omit</span>()</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"Rows after cleaning and imputation:"</span>, <span class="fu">nrow</span>(df_clean), <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Rows after cleaning and imputation: 3078 </code></pre>
</div>
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the response variable y</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> df_clean<span class="sc">$</span>wage76</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="distribution-of-key-variables" class="level3">
<h3 class="anchored" data-anchor-id="distribution-of-key-variables">Distribution of Key Variables</h3>
</section>
<section id="work-experience-distribution" class="level3">
<h3 class="anchored" data-anchor-id="work-experience-distribution">Work Experience Distribution</h3>
<pre><code>::: {.cell}
::: {.cell-output-display}
![Figure 1: Work Experience Distribution in 1976](index_files/figure-html/work-distribution-1.png){width=672}
:::
:::



The right-skewed distribution of `exp76` in the `RetSchool` dataset indicates a predominantly young and less experienced workforce in 1976. This skewness suggests a workforce that was entering or early in their careers. For more details, see [Figure 1: Work Experience Distribution in 1976](/p_experience.png) the distribution of work experience in 1976 indicates a predominantly young workforce:

- **Younger Workforce**: A significant entry of younger individuals into the job market, possibly influenced by demographic shifts or growth in new industries.
- **Impact on Wages**: Lower experience levels correspond with lower wages, contributing to observed wage disparities.
- **Economic Context**: The skew provides insights into the economic environment of the era, reflecting labor market conditions and potential impacts of educational and economic policies.

#### Wage Distribution Analysis


::: {.cell}
::: {.cell-output-display}
![Figure 2: Wage Distribution in 1976](index_files/figure-html/wage-distribution-1.png){width=672}
:::
:::

The histogram and density plot for `wage76` reveal the distribution of wages among workers in 1976, showing a typical pattern that indicates economic inequalities [Figure 2: Wage Distribution in 1976](p_wage.png):

- **General Trend**: The distribution suggests that most workers earned lower wages, while a smaller group had significantly higher earnings. This spread highlights the income disparities among different economic groups.
  
- **Skewness**: A right-skewed distribution indicates that while the majority of the workforce earned below the median wage level, there was a smaller segment with much higher wages. This skewness is critical for understanding the extent of wage disparities.
  
- **Economic Well-Being Insights**: By examining where most wages lie, we gain insights into the economic well-being of the population. This analysis provides a clearer picture of the economic conditions in 1976, reflecting the standard of living and financial stability of the workers during that time.


#### Correlation Matrix


::: {.cell}
::: {.cell-output-display}
![Figure 3: Correlation Matrix of Selected Variables](index_files/figure-html/correlation-matrix-1.png){width=672}
:::
:::

The correlation matrix is a powerful tool used to visualize and quantify the relationships between key variables such as `wage76`, `grade76`, `exp76`, and `age76` in the `RetSchool` dataset. Here’s what we learn from it [Figure 3: Correlation Matrix of Selected Variables](corrplot_plot.png):

- **Visual and Statistical Insights**: The matrix uses color intensity to indicate the strength of the relationships between variables. Correlation coefficients displayed in the matrix range from -1 to 1, where values near ±1 indicate strong positive or negative correlations, and values near 0 indicate weak or no linear relationship.

- **Influential Factors on Wages**: The correlations help identify significant predictors of wages:
  - A strong positive correlation between `wage76` and `grade76` suggests that higher educational attainment is likely linked to higher wages.
  - The relationship between `wage76` and `exp76` indicates how accumulated work experience correlates with wage levels, potentially showing that more experience leads to higher earnings.

- **Economic Analysis Applications**: By analyzing these correlations, we gain insights into the economic dynamics affecting the workforce in 1976. This helps in understanding how education, experience, and age were interacting to shape the economic landscape and influence wage disparities at that time.

**Why LASSO for the RetSchool Dataset?**


LASSO regression is particularly well-suited for our study on wage disparities due to its robust features that simplify complex data analysis:

- **Feature Selection**: Our initial look at the data showed many variables that could affect wages. LASSO helps by automatically selecting the most important factors, such as education level and region. This selection makes the model easier to understand and focuses on what really affects wages. [@Zhao2006]

- **Handling Multicollinearity**: Our analysis indicated that some variables, like education and work experience, might be overlapping in their effects on wages. LASSO tackles this issue by reducing the influence of less critical variables to zero. This is key to making sure our model remains stable and reliable. [@Tibshirani1996]

- **Simplicity and Clarity**: We want our model not just to be accurate, but also easy to interpret. LASSO strikes a good balance by simplifying the model, which helps in formulating clear and actionable insights for wage-related policies. [@Fan2011]

- **Predictive Accuracy**: Beyond just analyzing historical data, we aim to predict future trends accurately. LASSO improves the model’s ability to perform well with new, unseen data by avoiding overfitting. We use a method called k-fold cross-validation to fine-tune the model, ensuring it predicts accurately outside our sample. [@James2013]

- **Comparison with Traditional Regression**: We compared LASSO with regular linear regression to test its effectiveness. The results showed that LASSO was better at managing complex issues like multicollinearity and selecting key features, which are essential for robust analysis of wage factors. 

### Understanding Wages as a Continuous Variable

Through our LASSO analysis, we also confirmed that `wage76`, our main variable of interest, is continuous. This was evident as LASSO helped highlight the relationships between wages and various predictors without breaking them into arbitrary categories, preserving the natural continuous scale of wage data. This continuous perspective is crucial for accurate modeling and meaningful conclusions about how various factors influence wages.

Given these strengths, LASSO regression is an ideal choice for navigating the complexities of the RetSchool dataset, providing a clear, robust model that is well-suited for making informed decisions on wage policy and understanding the economic landscape of 1976.


## Statistical Modeling

Given the robust nature of LASSO (Least Absolute Shrinkage and Selection Operator) regression for handling complex datasets, it becomes an indispensable tool in our analysis of the RetSchool dataset. Here, we focus on refining our approach through meticulous data preparation and precise model fitting.

### Understanding Key Variables

The main goal of our analysis is to figure out what drives differences in wages in 1976. To do this, we look at various socio-economic factors that might impact wages.

#### Predictor Variables:

- **Educational Background (`grade76`, `col4`)**: Education level, which we expect to significantly affect wages. Higher education often correlates with higher earnings.
- **Work Experience (`exp76`)**: The number of years a person has worked, which should relate directly to their skills and salary potential.
- **Demographic and Regional Factors (`age76`, `black`, `south76`, `region`, `smsa76`)**: Includes age, race, geographical location, and whether a person lived in an urban area, all of which can influence wages.

#### Target Variable:

- **Wage (`wage76`)**: This is the main variable we are studying—it represents the income of individuals in 1976 and varies continuously across the dataset.

### Visualizing Key Variables

Understanding how these variables interact is easier when we visualize their distributions and relationships [Figure 4:Visualizations of Key Variables](d_plot.png).


::: {.cell}
::: {.cell-output-display}
![Figure 4: Visualizations of Key Variables](index_files/figure-html/visualizations-key-variables-1.png){width=960}
:::
:::




### Data Preparation and Feature Scaling

Feature scaling is a critical preparatory step in our analysis, essential for the application of LASSO regression. LASSO's approach to regularization is sensitive to the scale of variables, as it penalizes the absolute size of the regression coefficients. This sensitivity can lead to disproportionate influence from variables with larger numeric ranges, skewing the model and potentially leading to misleading conclusions about the factors influencing wages.





**Demonstrating the Impact of Feature Scaling**
We focus specifically on scaling `exp76`—years of experience—because it is a key predictor of wages, which is our continuous outcome variable. Proper scaling is vital to fairly assess how work experience impacts wages, alongside other factors such as education and demographic variables.

To illustrate the importance of this process, consider the variable `exp76`. Before scaling, `exp76` might display a wide range of values that reflect the diverse work experiences in our dataset. However, without scaling, this range could disproportionately affect the LASSO model’s ability to apply regularization fairly across all predictors [Figure 5: Distribution of Feature 'exp76' Before and After Scaling](combined_plot.png).


::: {.cell lable='fig:data-feature-visualization'}
::: {.cell-output-display}
![Figure 5: Distribution of Feature 'exp76' Before and After Scaling](index_files/figure-html/data-feature-visualization-1.png){width=960}
:::
:::


After scaling, `exp76` is transformed to have a standard scale similar to other variables in the model. This normalization allows for a fair comparison and interaction within the model, ensuring that no single variable unduly influences the outcome due to its scale. 

The transformation that occurs when features like `exp76` are normalized is pivotal for models like LASSO regression, which rely on regularization techniques sensitive to the scale of input variables. By ensuring that the regularization penalty is applied uniformly across all features, scaling enhances the model's ability to identify truly significant predictors and avoids undue influence from variables simply because of their scale.

This step is crucial for maintaining the integrity and accuracy of our analysis, particularly in studies focused on understanding continuous outcomes such as wage disparities, where the precise quantification of each variable’s impact is essential.

**Visualizing the Impact of ($\lambda$) on Model Performance**

The regularization parameter ($\lambda$) in LASSO regression critically influences model accuracy by balancing model complexity and predictive performance. Selecting the optimal ($\lambda$) is essential to minimize overfitting and ensure the model generalizes well [Figure 6: Cross-Validation CurveFigure 6: Cross-Validation Curve](Cross_Validation_plot.png).



::: {.cell}

```{.r .cell-code}
library(glmnet)
library(ggplot2)
library(dplyr)

set.seed(123) # For reproducibility
cv_outcome &lt;- cv.glmnet(features_scaled, y, alpha = 1, nfolds = 10)

# Creating a data frame from the glmnet cross-validation output
cv_data &lt;- as.data.frame(cv_outcome$cvm)
names(cv_data) &lt;- c("mse")
cv_data$lambda &lt;- log(cv_outcome$lambda)
cv_data$lambda_min &lt;- log(cv_outcome$lambda.min)
cv_data$lambda_1se &lt;- log(cv_outcome$lambda.1se)

# Plotting with ggplot2
Cross_Validation_plot &lt;- ggplot(cv_data, aes(x = lambda, y = mse)) +
  geom_line(color = "blue", linewidth = 1) +
  geom_vline(xintercept = cv_data$lambda_min, linetype = "dashed", color = "red", linewidth = 1) +
  geom_vline(xintercept = cv_data$lambda_1se, linetype = "dashed", color = "green", linewidth = 1) +
  labs(title = "Cross-Validation Curve for LASSO Regression",
       subtitle = "Dashed lines represent the lambda.min and lambda.1se",
       x = "Log(Lambda)",
       y = "Mean Squared Error",
       caption = "Red: lambda.min, Green: lambda.1se") +
  theme_minimal() +
  theme(plot.title = element_text(size = 20, face = "bold"),
        plot.subtitle = element_text(size = 14),
        plot.caption = element_text(size = 12),
        axis.title = element_text(size = 16),
        axis.text = element_text(size = 12))
Cross_Validation_plot</code></pre>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="index_files/figure-html/cross-validation-visualization-1.png" class="img-fluid figure-img" width="960"></p>
<figcaption>Figure 6: Cross-Validation Curve</figcaption>
</figure>
</div>
</div>
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggsave</span>(<span class="st">"Cross_Validation_plot.png"</span>, <span class="at">plot =</span> Cross_Validation_plot, <span class="at">width =</span> <span class="dv">10</span>, <span class="at">height =</span> <span class="dv">5</span>, <span class="at">dpi =</span> <span class="dv">300</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<p>:::</p>
</section>
<section id="role-of-cross-validation-in-determining-lambda" class="level3">
<h3 class="anchored" data-anchor-id="role-of-cross-validation-in-determining-lambda">Role of Cross-Validation in Determining (<span class="math inline">\(\lambda\)</span>)</h3>
<p>Cross-validation is essential for selecting the right (<span class="math inline">\(\lambda\)</span>) in LASSO regression. It helps balance the model by preventing overfitting with low (<span class="math inline">\(\lambda\)</span>) values and underfitting with high (<span class="math inline">\(\lambda\)</span>) values. This method splits the data into subsets, testing how each (<span class="math inline">\(\lambda\)</span>) performs, thus ensuring the model’s effectiveness on new, unseen data.</p>
</section>
<section id="optimal-lambda-choices" class="level3">
<h3 class="anchored" data-anchor-id="optimal-lambda-choices">Optimal (<span class="math inline">\(\lambda\)</span>) Choices</h3>
<p>Cross-validation identifies crucial (<span class="math inline">\(\lambda\)</span>) values for optimal model performance:</p>
<ul>
<li><strong>Optimal (<span class="math inline">\(\lambda\)</span>) (<code>lambda.min</code>)</strong>: Achieves the lowest mean squared error (MSE), indicating the best balance between accuracy and complexity.</li>
<li><strong>Conservative (<span class="math inline">\(\lambda\)</span>) (<code>lambda.1se</code>)</strong>: Slightly higher than <code>lambda.min</code>, it provides a simpler model that is robust and stable, useful for generalizing well to new data.</li>
</ul>
</section>
<section id="model-coefficients-and-interpretation" class="level3">
<h3 class="anchored" data-anchor-id="model-coefficients-and-interpretation">Model Coefficients and Interpretation</h3>
<p>The optimized LASSO model clearly delineates which factors significantly impact wages, simplifying the analysis by reducing less impactful predictors. This streamlined approach not only enhances interpretability but also focuses on the most influential variables affecting wages in 1976.</p>
<p><a href="cv_outcome_plot.png">Figure 7: Visualization of LASSO Coefficients</a> visualizes these coefficients, highlighting the relative importance of each predictor in the context of wage determination.</p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(knitr)</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(glmnet)</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Ensure the cv_outcome object is available and correctly specified</span></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> (<span class="fu">exists</span>(<span class="st">"cv_outcome"</span>)) {</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Extract coefficients using a safe method</span></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>  coef_optimal <span class="ot">&lt;-</span> <span class="fu">tryCatch</span>({</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>    <span class="fu">coef</span>(cv_outcome, <span class="at">s =</span> <span class="st">"lambda.min"</span>, <span class="at">exact =</span> <span class="cn">FALSE</span>)</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>  }, <span class="at">error =</span> <span class="cf">function</span>(e) {</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>    <span class="fu">message</span>(<span class="st">"Failed to extract coefficients: "</span>, e<span class="sc">$</span>message)</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>    <span class="cn">NULL</span>  <span class="co"># Return NULL if there's an error</span></span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>  })</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> (<span class="sc">!</span><span class="fu">is.null</span>(coef_optimal) <span class="sc">&amp;&amp;</span> <span class="fu">inherits</span>(coef_optimal, <span class="st">"dgCMatrix"</span>)) {</span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Convert the matrix to a data frame for better handling</span></span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a>    coef_matrix <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(coef_optimal)</span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a>    coef_df <span class="ot">&lt;-</span> <span class="fu">as.data.frame</span>(coef_matrix, <span class="at">stringsAsFactors =</span> <span class="cn">FALSE</span>)</span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a>    coef_df<span class="sc">$</span>Variable <span class="ot">&lt;-</span> <span class="fu">rownames</span>(coef_matrix)  <span class="co"># Add variable names as a new column</span></span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a>    <span class="fu">rownames</span>(coef_df) <span class="ot">&lt;-</span> <span class="cn">NULL</span>  <span class="co"># Clean up row names</span></span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Rename the coefficient column for clarity</span></span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a>    <span class="fu">names</span>(coef_df)[<span class="dv">1</span>] <span class="ot">&lt;-</span> <span class="st">"Coefficient"</span></span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a>    cv_outcome_plot <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(coef_df, <span class="fu">aes</span>(<span class="at">x =</span> <span class="fu">reorder</span>(Variable, Coefficient), <span class="at">y =</span> Coefficient, <span class="at">fill =</span> Coefficient <span class="sc">&gt;</span> <span class="dv">0</span>)) <span class="sc">+</span></span>
<span id="cb6-24"><a href="#cb6-24" aria-hidden="true" tabindex="-1"></a>        <span class="fu">geom_col</span>() <span class="sc">+</span></span>
<span id="cb6-25"><a href="#cb6-25" aria-hidden="true" tabindex="-1"></a>        <span class="fu">coord_flip</span>() <span class="sc">+</span>  <span class="co"># Makes the plot horizontal for better readability</span></span>
<span id="cb6-26"><a href="#cb6-26" aria-hidden="true" tabindex="-1"></a>        <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">"Visualization of LASSO Coefficients"</span>, <span class="at">x =</span> <span class="st">"Predictors"</span>, <span class="at">y =</span> <span class="st">"Coefficient Value"</span>) <span class="sc">+</span></span>
<span id="cb6-27"><a href="#cb6-27" aria-hidden="true" tabindex="-1"></a>        <span class="fu">scale_fill_manual</span>(<span class="at">values =</span> <span class="fu">c</span>(<span class="st">"red"</span>, <span class="st">"blue"</span>), <span class="at">name =</span> <span class="st">"Sign of Coefficient"</span>,</span>
<span id="cb6-28"><a href="#cb6-28" aria-hidden="true" tabindex="-1"></a>                          <span class="at">labels =</span> <span class="fu">c</span>(<span class="st">"Negative"</span>, <span class="st">"Positive"</span>)) <span class="sc">+</span></span>
<span id="cb6-29"><a href="#cb6-29" aria-hidden="true" tabindex="-1"></a>        <span class="fu">theme_minimal</span>()</span>
<span id="cb6-30"><a href="#cb6-30" aria-hidden="true" tabindex="-1"></a>  } <span class="cf">else</span> {</span>
<span id="cb6-31"><a href="#cb6-31" aria-hidden="true" tabindex="-1"></a>    <span class="fu">message</span>(<span class="st">"Coefficient data is not available or invalid."</span>)</span>
<span id="cb6-32"><a href="#cb6-32" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb6-33"><a href="#cb6-33" aria-hidden="true" tabindex="-1"></a>} <span class="cf">else</span> {</span>
<span id="cb6-34"><a href="#cb6-34" aria-hidden="true" tabindex="-1"></a>  <span class="fu">message</span>(<span class="st">"cv_outcome is not available. Please check model fitting."</span>)</span>
<span id="cb6-35"><a href="#cb6-35" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb6-36"><a href="#cb6-36" aria-hidden="true" tabindex="-1"></a><span class="fu">ggsave</span>(<span class="st">"cv_outcome_plot.png"</span>, <span class="at">plot =</span> cv_outcome_plot, <span class="at">width =</span> <span class="dv">10</span>, <span class="at">height =</span> <span class="dv">5</span>, <span class="at">dpi =</span> <span class="dv">300</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p><strong>Significant Predictors and Their Effects</strong></p>
<p>The LASSO model has highlighted key factors that influence wages, efficiently pinpointing the most impactful variables:</p>
<ul>
<li><strong>Baseline Wages</strong>: The intercept sets the starting point for wage predictions, adjusted for average levels of predictors.</li>
<li><strong>Educational Attainment (<code>grade76</code>)</strong>: Higher education levels are strongly associated with increased wages, confirming the significant return on investment in education.</li>
<li><strong>Racial Disparity (<code>black</code>)</strong>: There is a noticeable wage gap affecting black individuals, indicating persistent racial inequalities in earnings.</li>
<li><strong>Geographic Influence (<code>south76</code>)</strong>: Residing in the South is linked to lower wages, reflecting regional economic differences.</li>
<li><strong>Urban Premium (<code>smsa76</code>)</strong> and <strong>Long-term Urban Advantage (<code>smsa66</code>)</strong>: Both highlight the wage benefits of living in urban areas, both currently and historically.</li>
<li><strong>Family Stability (<code>momdad14</code>)</strong>: Growing up with both parents is correlated with higher wages, suggesting the economic benefits of a stable family during childhood.</li>
<li><strong>Parental Education (<code>momed</code>)</strong>: Higher maternal education levels positively affect wages, underscoring the influence of parental education.</li>
<li><strong>Age and Experience (<code>age76</code>)</strong>: Older age, often accompanied by more experience, typically leads to higher wages, reflecting the value of longevity in the workforce.</li>
<li><strong>Higher Education (<code>col4</code>)</strong>: Possessing a college degree shows a positive but modest correlation with higher wages.</li>
</ul>
<p><strong>Variables with Minimal Impact</strong></p>
<p>Several variables demonstrated minimal to no influence on wages, exemplifying Lasso’s ability to streamline the model by eliminating non-impactful predictors. These include <code>exp76</code>, <code>region</code>, <code>sinmom14</code>, <code>nodaded</code>, <code>nomomed</code>, <code>daded</code>, and <code>famed</code>. This reduction in variables enhances the interpretability of our model without compromising the accuracy of our predictions.</p>
</section>
</section>
<section id="results-conclusion" class="level2">
<h2 class="anchored" data-anchor-id="results-conclusion">Results &amp; Conclusion</h2>
<section id="comparative-analysis-of-coefficient-impact" class="level3">
<h3 class="anchored" data-anchor-id="comparative-analysis-of-coefficient-impact">Comparative Analysis of Coefficient Impact</h3>
<p>Our analysis with the LASSO model has effectively highlighted the most significant factors influencing wages, using a method that focuses on simplicity and accuracy by penalizing less impactful predictors. However, to deepen our understanding of these results, we also employed Multiple Linear Regression (MLR) as a comparative tool. This step allows us to observe the differences in how each model handles the complexity of the dataset and the continuous nature of the wage variable.</p>
<p>Comparing LASSO to MLR provides several benefits:</p>
<ul>
<li><strong>Baseline Comparison</strong>: MLR, which does not include a regularization term, serves as a baseline to appreciate how much LASSO’s penalties affect the coefficients of each predictor.</li>
<li><strong>Insight into Overfitting</strong>: By observing how predictors behave in MLR, which is prone to overfitting especially in datasets with many variables, we can better understand the necessity and effectiveness of LASSO’s regularization approach.</li>
<li><strong>Variable Importance</strong>: This comparison clearly delineates which variables are genuinely important for predicting wages, as significant predictors in MLR that are penalized to zero in LASSO may not be as crucial as initially thought.</li>
</ul>
</section>
<section id="significant-predictors-of-wages" class="level3">
<h3 class="anchored" data-anchor-id="significant-predictors-of-wages">Significant Predictors of Wages</h3>
<p>The analysis of both LASSO and MLR models sheds light on the robustness of each predictor’s impact on wages. Here, we provide a side-by-side view of the coefficients, illustrating how each model values the same predictors:</p>
<ul>
<li><strong>Consistency and Differences</strong>: Where LASSO might zero out a predictor, MLR may still attribute it with a significant coefficient. Such differences are key in understanding the potential for overfitting in MLR versus the more conservative and potentially more reliable predictions from LASSO.</li>
<li><strong>Focus on Continuous Outcomes</strong>: Both models emphasize the importance of continuous predictors like <code>age76</code> and <code>grade76</code>, but LASSO does so while maintaining model parsimony, avoiding the pitfalls of including too many variables as MLR might.</li>
</ul>
<p><strong>Visual Comparison of Model Outcomes</strong></p>
<p>Visualizing the differences between the coefficients in Multiple Linear Regression (MLR) and LASSO offers a clear, intuitive understanding of how regularization in LASSO affects each predictor’s influence compared to traditional regression methods.</p>
<p>The chart below, titled “<a href="Predictors_plot.png">Figure 8: Comparison of Coefficients from MLR and LASSO</a>,” illustrates how LASSO’s regularization alters the coefficients:</p>
<ul>
<li><p><strong>Reduction in Coefficient Size</strong>: Often, LASSO’s coefficients are smaller than those in MLR. This reduction indicates that LASSO, through its shrinkage technique, lessens the influence of many predictors on the wage outcome. It’s a conservative approach that helps prevent the model from fitting too closely to the training data, thus reducing the risk of overfitting.</p></li>
<li><p><strong>Selective Feature Retention</strong>: LASSO may reduce some predictors’ coefficients to zero—effectively removing them from the model—if they are not statistically significant. This feature selection is critical in complex datasets like RetSchool, where simplifying the model can lead to clearer, more actionable insights.</p></li>
<li><p><strong>Stability Across Models</strong>: Predictors whose coefficients are consistent across both MLR and LASSO are likely very reliable indicators of wage variations, underscoring their importance regardless of the modeling approach used.</p></li>
<li><p><strong>Enhanced Interpretability</strong>: LASSO improves the model’s clarity by focusing only on significant predictors. This makes it easier for analysts and decision-makers to understand which factors truly influence wages, facilitating more informed decisions.</p></li>
</ul>
<section id="advantages-of-lasso-over-mlr-in-educational-data-analysis" class="level4">
<h4 class="anchored" data-anchor-id="advantages-of-lasso-over-mlr-in-educational-data-analysis">Advantages of LASSO Over MLR in Educational Data Analysis</h4>
<p>In the context of the RetSchool dataset, which explores factors influencing educational returns in the labor market, LASSO offers several distinct advantages:</p>
<ul>
<li><p><strong>Effective Feature Selection</strong>: Unlike MLR, which might incorporate every predictor into the analysis, LASSO strategically eliminates less relevant variables. This focus helps distill the model to the most impactful factors, reducing complexity and improving the clarity of results.</p></li>
<li><p><strong>Managing Multicollinearity</strong>: LASSO addresses the challenge of multicollinearity common in educational data, where variables such as years of education, urban residency, and parental background might be interrelated. By penalizing coefficients, LASSO minimizes redundant information, ensuring that each included predictor contributes uniquely to understanding wage disparities.</p></li>
<li><p><strong>Improving Model Interpretability</strong>: The simplification LASSO brings allows stakeholders to better understand the dynamics within the data. For example, it becomes clearer how factors like education level, race, and geographic location impact wages independently of each other.</p></li>
</ul>
</section>
</section>
<section id="conclusion-and-insights-from-the-return-to-school-dataset" class="level3">
<h3 class="anchored" data-anchor-id="conclusion-and-insights-from-the-return-to-school-dataset">Conclusion and Insights from the Return to School Dataset</h3>
<p>Our application of LASSO regression has uncovered significant factors that influenced wages in 1976, notably highlighting how educational attainment and age directly impact earnings. These findings not only confirm the importance of these predictors but also help us understand the extent to which they affect wage disparities.</p>
<section id="key-insights" class="level4">
<h4 class="anchored" data-anchor-id="key-insights">Key Insights:</h4>
<ul>
<li><p><strong>Educational Impact on Earnings</strong>: Higher educational levels consistently lead to increased wages, affirming that education is a crucial investment with substantial returns.</p></li>
<li><p><strong>Age and Earnings</strong>: Older individuals generally earn more, likely reflecting the combined effects of increased experience and education over time.</p></li>
</ul>
</section>
<section id="advantages-of-using-lasso-regression" class="level4">
<h4 class="anchored" data-anchor-id="advantages-of-using-lasso-regression">Advantages of Using LASSO Regression:</h4>
<p>LASSO regression has effectively simplified the analysis by focusing on the most impactful factors, reducing complexity and enhancing our model’s interpretability:</p>
<ul>
<li><p><strong>Selective Feature Retention</strong>: By minimizing the influence of less significant variables, LASSO has allowed us to concentrate on factors that truly affect wages.</p></li>
<li><p><strong>Overfitting Mitigation</strong>: This approach ensures our predictions are robust, avoiding the common pitfall of overfitting associated with more traditional regression models.</p></li>
</ul>
</section>
<section id="visualization-of-continuous-variables" class="level4">
<h4 class="anchored" data-anchor-id="visualization-of-continuous-variables">Visualization of Continuous Variables:</h4>
<p>We have illustrated the impacts of education and age through detailed visualizations:</p>
<ul>
<li><p><strong>Impact of Education</strong>: Demonstrates a clear, positive correlation between education and wages, with incremental educational achievements leading to higher earnings.</p></li>
<li><p><strong>Role of Age in Earnings</strong>: Shows that wage increases with age, highlighting the value of experience and possibly greater educational attainment over time.</p></li>
</ul>
<div class="cell">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="index_files/figure-html/continuous-impact-1.png" class="img-fluid figure-img" width="960"></p>
<figcaption>Figure 9:Impact of Continuous Variables on Wages</figcaption>
</figure>
</div>
</div>
</div>
<p>These visual aids <a href="grade76_plot.png">Figure 9:Impact of Continuous Variables on Wages</a> are crucial for demonstrating how incremental changes in age and education relate to changes in wages, providing a compelling argument for the continuous benefits of education.</p>
</section>
</section>
<section id="implications-and-future-directions" class="level3">
<h3 class="anchored" data-anchor-id="implications-and-future-directions">Implications and Future Directions</h3>
<p>This analysis provides valuable insights for policymakers and educational institutions, suggesting that enhancing access to education can lead to significant economic benefits. Future research could explore the long-term trends in these variables or investigate other factors that might influence wages, such as technological changes or economic conditions.</p>
<p>In conclusion, the detailed examination of the Return to School dataset using LASSO regression offers actionable insights that could significantly influence future educational investments and economic policy planning. Our study confirms the powerful impact of demographic factors on wages and sets the stage for further research in this vital area.</p>

</section>
</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-Belloni2013" class="csl-entry" role="listitem">
Belloni, A., and V. Chernozhukov. 2013. <span>“Least Squares After Model Selection in High-Dimensional Sparse Models.”</span> <em>Bernoulli</em> 19 (2): 521–47.
</div>
<div id="ref-Chintalapudi2022" class="csl-entry" role="listitem">
Chintalapudi, N., and et al. 2022. <span>“LASSO Regression Modeling on Prediction of Medical Terms Among Seafarers’ Health Documents Using Tidy Text Mining.”</span> <em>Bioengineering</em> 9 (1): 47.
</div>
<div id="ref-Efron2004" class="csl-entry" role="listitem">
Efron, B., T. Hastie, I. Johnstone, and R. Tibshirani. 2004. <span>“Least Angle Regression.”</span> <em>Annals of Statistics</em> 32 (2): 407–99. <a href="https://projecteuclid.org/euclid.aos/1083178935">https://projecteuclid.org/euclid.aos/1083178935</a>.
</div>
<div id="ref-Fan2011" class="csl-entry" role="listitem">
Fan, J., and J. Lv. 2011. <span>“Nonconcave Penalized Likelihood with NP-Dimensionality.”</span> <em>IEEE Transactions on Information Theory</em> 57 (8): 5467–84. <a href="https://ieeexplore.ieee.org/document/6025319">https://ieeexplore.ieee.org/document/6025319</a>.
</div>
<div id="ref-Friedman2010" class="csl-entry" role="listitem">
Friedman, Jerome, Trevor Hastie, and Robert Tibshirani. 2010. <span>“Regularization Paths for Generalized Linear Models via Coordinate Descent.”</span> <em>Journal of Statistical Software</em> 33 (1): 1–22. <a href="https://www.jstatsoft.org/article/view/v033i01">https://www.jstatsoft.org/article/view/v033i01</a>.
</div>
<div id="ref-Hastie2015" class="csl-entry" role="listitem">
Hastie, Trevor, Robert Tibshirani, and Martin Wainwright. 2015. <em>Statistical Learning with Sparsity: The Lasso and Generalizations</em>. CRC Press. <a href="https://www.crcpress.com/9781498712163">https://www.crcpress.com/9781498712163</a>.
</div>
<div id="ref-Hastie2009" class="csl-entry" role="listitem">
Hastie, T., R. Tibshirani, and J. Friedman. 2009. <span>“The Elements of Statistical Learning: Data Mining, Inference, and Prediction.”</span> <em>Springer Series in Statistics</em>. <a href="https://link.springer.com/book/10.1007/978-0-387-84858-7">https://link.springer.com/book/10.1007/978-0-387-84858-7</a>.
</div>
<div id="ref-James2013" class="csl-entry" role="listitem">
James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2013. <em>An Introduction to Statistical Learning: With Applications in r</em>. New York: Springer.
</div>
<div id="ref-Li2022" class="csl-entry" role="listitem">
Li, Y., and et al. 2022. <span>“Applying Logistic LASSO Regression for the Diagnosis of Atypical Crohn’s Disease.”</span> <em>Scientific Reports</em> 12: 12345.
</div>
<div id="ref-McEligot2020" class="csl-entry" role="listitem">
McEligot, Archana J. et al. 2020. <span>“Logistic LASSO Regression for Dietary Intakes and Breast Cancer.”</span> <em>Nutrients</em> 12 (9): 2652. <a href="https://www.mdpi.com/2072-6643/12/9/2652">https://www.mdpi.com/2072-6643/12/9/2652</a>.
</div>
<div id="ref-Meinshausen2006" class="csl-entry" role="listitem">
Meinshausen, N., and P. Bühlmann. 2006. <span>“High-Dimensional Graphs and Variable Selection with the Lasso.”</span> <em>Annals of Statistics</em> 34 (3): 1436–62. <a href="https://projecteuclid.org/euclid.aos/1152540754">https://projecteuclid.org/euclid.aos/1152540754</a>.
</div>
<div id="ref-Musoro2014" class="csl-entry" role="listitem">
Musoro, et al., J. Z. 2014. <span>“Validation of Prediction Models Based on Lasso Regression with Multiply Imputed Data.”</span> <em>BMC Medical Research Methodology</em>.
</div>
<div id="ref-Muthukrishnan2016" class="csl-entry" role="listitem">
Muthukrishnan, R., and R. Rohini. 2016. <span>“LASSO: A Feature Selection Technique in Predictive Modeling for Machine Learning.”</span> <a href="https://doi.org/10.1109/ICACA.2016.7887916">https://doi.org/10.1109/ICACA.2016.7887916</a>.
</div>
<div id="ref-Park2008" class="csl-entry" role="listitem">
Park, M. Y., and T. Hastie. 2008. <span>“Penalized Logistic Regression for Detecting Gene Interactions.”</span> <em>Biostatistics</em> 9 (1): 30–50.
</div>
<div id="ref-Simon2013" class="csl-entry" role="listitem">
Simon, N., and et al. 2013. <span>“Regularization Paths for Cox’s Proportional Hazards Model via Coordinate Descent.”</span> <em>Journal of Statistical Software</em> 39 (5): 1–13.
</div>
<div id="ref-Tibshirani1996" class="csl-entry" role="listitem">
Tibshirani, Robert. 1996. <span>“Regression Shrinkage and Selection via the Lasso.”</span> <em>Journal of the Royal Statistical Society. Series B (Methodological)</em> 58 (1): 267–88. <a href="https://www.jstor.org/stable/2346178">https://www.jstor.org/stable/2346178</a>.
</div>
<div id="ref-Tibshirani2021" class="csl-entry" role="listitem">
———. 2021. <span>“Regression Shrinkage and Selection via the Lasso: A Retrospective.”</span> <em>Journal of the Royal Statistical Society: Series B</em>. <a href="https://rss.onlinelibrary.wiley.com/doi/10.1111/rssa.12356">https://rss.onlinelibrary.wiley.com/doi/10.1111/rssa.12356</a>.
</div>
<div id="ref-Wang2017" class="csl-entry" role="listitem">
Wang, S., and et al. 2017. <span>“Predicting Ship Fuel Consumption Based on LASSO Regression.”</span> <em>Marine Pollution Bulletin</em> 119 (1): 145–54.
</div>
<div id="ref-Lu2011" class="csl-entry" role="listitem">
Yiming Lu, Y. Z. 2011. <span>“A Lasso Regression Model for the Construction of microRNA-Target Regulatory Networks.”</span> <em>Bioinformatics</em>, 2406–13.
</div>
<div id="ref-Zhao2006" class="csl-entry" role="listitem">
Zhao, P., and B. Yu. 2006. <span>“On Model Selection Consistency of Lasso.”</span> <em>Journal of Machine Learning Research</em> 7: 2541–63. <a href="http://www.jmlr.org/papers/volume7/zhao06a/zhao06a.pdf">http://www.jmlr.org/papers/volume7/zhao06a/zhao06a.pdf</a>.
</div>
<div id="ref-Zhou2022" class="csl-entry" role="listitem">
Zhou, C., and et al. 2022. <span>“Influencing Factors of the High-Quality Economic Development in China Based on LASSO Model.”</span> <em>Energy Reports</em> 8: 1055–65.
</div>
</div></section></div></main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      return note.innerHTML;
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>
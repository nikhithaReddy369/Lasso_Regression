---
title: "Innovative Data Exploration with LASSO: Unveiling Patterns in Earnings and Education"
authors:
  - name: Muhammad Usman Aslam
  - name: Sai Kumar Miryala
  - name: Nikhitha Amireddy
  - name: Ishrath Jahan
  
date: 'today'
format:
  revealjs:
    incremental: true   
    slide-number: true
    show-slide-number: print
    smaller: true
    scrollable: true
    theme: moon
course: STA 6257 - Advanced Statistical Modeling
self-contained: true
execute: 
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
---

##  What is LASSO Regression?

LASSO (Least Absolute Shrinkage and Selection Operator), introduced by Robert Tibshirani in 1996 [@Tibshirani1996].

LASSO regression, also known as L1 regularization, is a popular technique used in statistical modeling and machine learning to estimate the relationships between variables and make predictions. 

Primary goal of LASSO is to shrink some coefficients to exactly zero, effectively performing variable selection by excluding irrelevant predictors from the model which helps to find a balance between model simplicity and accuracy.

## Applications Across Fields 

LASSO regression’s versatility across multiple fields illustrates its capability to manage complex datasets effectively, particularly with continuous outcomes.

::: {.panel-tabset}

### Economics

Zhou et al. [Zhou2022] highlighted LASSO’s ability to identify key economic predictors that assist in strategic decision-making. 

This example underscores its utility in economic analysis, where it helps to isolate factors that directly influence continuous economic outcomes like wages, prices, or economic growth.

### Bioinformatics

Lu et al. and Musoro [@Lu2011; @Musoro2014] used LASSO regression to develop models based on gene expression data, advancing our understanding of genetic influences on continuous traits and diseases. Their work illustrates how LASSO can handle vast amounts of biological data to pinpoint critical genetic pathways.


### Public Health

McEligot et al. (2020)[@McEligot2020] employed logistic LASSO to explore how dietary factors, which vary continuously, affect the risk of developing breast cancer. Their findings highlight LASSO's strength in dealing with complex, high-dimensional datasets in health sciences.

:::

## Advantages of LASSO Regression

LASSO regression is highly valued in fields ranging from healthcare to finance due to its ability to simplify complex models without sacrificing accuracy. This method's key strengths include:

-**Feature Selection**: Feature selection helps focus the model on the truly impactful factors. [@Park2008]

-**Model Interpretability**: By eliminating irrelevant variables, LASSO makes the resulting models easier to understand and communicate, enhancing their practical use. [@Belloni2013]

-**Mitigation of Multicollinearity**: LASSO addresses issues that arise when predictor variables are highly correlated. 

-It selects one variable from a group of closely related variables, which simplifies the model and avoids redundancy. [@Efron2004]



## Methodology Overview

LASSO enhances linear regression by adding a penalty on the size of the coefficients, aiding in feature selection and improving model interpretability.

LASSO's objective function:

$$
\min_{\beta} \left\{ \frac{1}{2n} \sum_{i=1}^{n} (y_i - \beta_0 - \sum_{j=1}^{p} \beta_j x_{ij})^2 + \lambda \sum_{j=1}^{p} |\beta_j| \right\}
$$

**Components of the Formula**:

-**1. Beta Coefficients ($\beta$)**: These are the parameters of the model, where $\beta_0$ is the intercept, and $\beta_j$ are the coefficients for the predictors.

-**2. Observed Values ($y_i$)**: These are the responses observed for each observation in the dataset.

-**3. Predictor Values ($x_{ij}$)**: These are the values of the predictors for each observation.

-**4. Residual Sum of Squares (RSS)**: It measures the discrepancies between observed values and predictions, normalized by $\frac{1}{2n}$ for computational convenience.

-**5. Regularization Parameter ($\lambda$)**: This parameter controls the trade-off between fitting the model accurately and keeping the model coefficients small and balances model complexity against overfitting

-**6. L1 Penalty**: This term encourages the sparsity of the model by allowing some coefficients to shrink to zero.


## How LASSO Regression Works?

::: {.panel-tabset}

### Linear regression model

LASSO regression starts with the standard linear regression model, which assumes a linear relationship between the independent variables (features) and the dependent variable (target).

$$
y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \ldots + \beta_n x_n + \epsilon
$$
y is the dependent variable (target).
β₀, β₁, β₂, ..., βₚ are the coefficients (parameters) to be estimated.
x₁, x₂, ..., xₚ are the independent variables (features).
ε represents the error term.

### Role of $\lambda$

LASSO regression introduces an additional penalty term based on the absolute values of the coefficients.

The choice of the regularization parameter λ is crucial in LASSO regression:

-At λ=0, LASSO equals an ordinary least squares regression, offering no coefficient shrinkage.

-**Variable Selection**: As λ increases, more coefficients shrink to zero.

-**Optimization**: Achieved through cross-validation to find the optimal λ.

### The Shrinkage Effect and Its Benefits

-**Feature Selection**: Reduces coefficients of non-essential predictors to zero.

-**Regularization**: Enhances model generalizability, critical for complex datasets.

### Practical Application and Model Comparison

-**Fields of Application**: Finance, healthcare, where accurate prediction is crucial.

-**Comparison with MLR**: Demonstrates LASSO's superiority in handling high-dimensional data by selectively including only relevant variables.

:::

## Objective

Our project aims to explore the impact of various factors on wages using the RetSchool dataset, focusing on how education and demographic variables influence earnings in 1976. We have chosen LASSO regression to address our research questions due to its unique capabilities in dealing with complex datasets and its methodological strengths in feature selection and model accuracy.


## Dataset Description 

-Overview of `RetSchool` Dataset Variables:

Understanding the variables in the `RetSchool` dataset, crucial for analyzing socio-economic and educational influences on wages in 1976.


| Variable   | Description                         | Type        | Relevance                                              |
|------------|-------------------------------------|-------------|--------------------------------------------------------|
| `wage76`   | Wages of individuals in 1976        | Continuous  | Primary measure of economic status                     |
| `age76`    | Age of individuals                  | Continuous  | Analyzes age impact on wages                           |
| `grade76`  | Highest grade completed             | Continuous  | Indicates educational attainment                       |
| `col4`     | College education                   | Binary      | Impact of higher education on wages                    |
| `exp76`    | Work experience                     | Continuous  | Examines experience influence on wages                 |
| `momdad14` | Lived with both parents at age 14   | Binary      | Family structure's impact on early life outcomes      |
| `sinmom14` | Lived with a single mother at age 14| Binary      | Focuses on single-mother household impact             |
| `daded`    | Father's education level            | Continuous  | Paternal education impact on offspring's outcomes     |
| `momed`    | Mother's education level            | Continuous  | Maternal education impact                             |
| `black`    | Racial identification as black      | Binary      | Used to analyze racial disparities                    |
| `south76`  | Residency in the South              | Binary      | For regional economic analysis                         |
| `region`   | Geographic region                   | Categorical | Regional influences on outcomes                       |
| `smsa76`   | Urban residency                     | Binary      | Urban versus rural disparities                        |


## Data Exploration

Initial data cleaning included addressing missing values through imputation or removal to refine the dataset for detailed analysis.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE, results = 'hide')
library(tidyverse)
library(ggplot2)
library(corrplot)
library(gridExtra)
df_clean <- read_csv("https://vincentarelbundock.github.io/Rdatasets/csv/Ecdat/RetSchool.csv", show_col_types = FALSE)
```

::: {.panel-tabset}

### Work Experience Distribution

```{r work-distribution, fig.cap="Figure 1: Work Experience Distribution in 1976"}
library(ggplot2)
# Generate the plot
p_experience <- ggplot(df_clean, aes(x = exp76)) +
  geom_histogram(aes(y = ..density..), bins = 30, fill = "orange", color = "black") +
  geom_density(color = "purple", fill = "purple", alpha = 0.2) +
  labs(title = "Work Experience Distribution in 1976", x = "Years of Experience", y = "Density")
p_experience
# Save the plot as an image
ggsave("p_experience.png", plot = p_experience, width = 10, height = 5, dpi = 300)
```

-**Visualization**: The right-skewed distribution of `exp76` suggests a young, less experienced workforce.
-**Implications**: Reflects entry-level workers predominating in 1976, impacting wage levels and economic conditions.

### Wage Distribution

```{r wage-distribution, fig.cap="Figure 2: Wage Distribution in 1976"}
p_wage <- ggplot(df_clean, aes(x = wage76)) +
  geom_histogram(aes(y = ..density..), binwidth = 1, fill = "blue", color = "black") +
  geom_density(color = "red", fill = "red", alpha = 0.2) +
  labs(title = "Wage Distribution in 1976", 
       x = "Wage (Dollars)", 
       y = "Density",
       subtitle = "Histogram and density plot showing the distribution of wages") +
  theme_minimal() +
  theme(plot.title = element_text(size = 16, face = "bold"), 
        plot.subtitle = element_text(size = 12))
p_wage
ggsave("p_wage.png", plot = p_wage, width = 10, height = 5, dpi = 300)
```
-**Visualization**: A histogram and density plot show most workers earned lower wages, with a minority earning significantly more.

-**Economic Insights**: Highlights income disparities and provides insights into the financial stability of the population.



### Correlation Matrix

```{r correlation-matrix,  echo=FALSE, message=FALSE, warning=FALSE, fig.cap="Figure 3: Correlation Matrix of Selected Variables"}
library(corrplot)
library(dplyr)

# Calculate the correlation matrix for selected variables, omitting missing values
df_selected <- df_clean %>% select(wage76, grade76, exp76, age76) %>% na.omit()
cor_matrix <- cor(df_selected)

# Open a PNG device
png(filename = "corrplot_plot.png", width = 1000, height = 800, res = 300)

# Generate and display the correlation matrix plot
corrplot(cor_matrix, method = "color", 
         addCoef.col = "black",  
         type = "upper",         
         tl.col = "black",       
         tl.srt = 45,            
         tl.cex = 0.6,           
         number.cex = 0.6,       
         mar = c(0, 0, 2, 0))

# Close the device to save the plot
dev.off()
corrplot(cor_matrix, method = "color", 
         addCoef.col = "black",  
         type = "upper",         
         tl.col = "black",       
         tl.srt = 45,            
         tl.cex = 0.6,           
         number.cex = 0.6,       
         mar = c(0, 0, 2, 0))
```

-**Analysis Tool**: Visualizes relationships between key variables like `wage76`, `grade76`, `exp76`, and `age76`.

-**Findings**: Identifies strong predictors of wages and helps understand economic dynamics of the era.

:::


## Why LASSO for the RetSchool Dataset?

::: {.panel-tabset}

### Enhanced Feature Selection

-**Insight**: LASSO's automatic feature selection is pivotal in isolating significant predictors like education level and regional differences, directly impacting wage analysis.

-**Benefit**: Simplifies the model by focusing only on impactful variables, thus enhancing interpretability, which is critical for formulating effective educational and economic policies.

### Effective Multicollinearity Management

-**Challenge**: Overlapping influences of educational attainment and work experience on wages could lead to skewed analytical results.

-**Solution**: By penalizing the coefficients of correlated predictors, LASSO ensures a more stable and reliable model, addressing multicollinearity without requiring manual intervention.

### Improved Predictive Accuracy

-**Technique**: Incorporates k-fold cross-validation within the LASSO framework to fine-tune the regularization parameter, optimizing model accuracy.

-**Advantage**: Enhances predictive reliability, crucial for accurately forecasting wage trends based on educational variables, thereby preventing model overfitting.

:::

## Statistical Modeling

::: {.panel-tabset}

### Data Preparation and Cleaning

Proper data preparation is critical to ensure the robustness of the statistical analysis:

-**Handling Missing Data**: Key variables with missing data, such as educational background and work experience, were imputed using the median of available data to minimize the impact of outliers.

-**Removing Incomplete Records**: After imputation, records that still contained missing values were removed to maintain the integrity and accuracy of the model analysis.

### Visualization of Data Cleaning


```{r setup_2}
library(tidyverse)
library(ggplot2)
library(gridExtra)

# Load data
df <- read_csv("RetSchool.csv", show_col_types = FALSE)
# Record initial number of rows
initial_count <- nrow(df)

# Data Cleaning
df_clean <- df %>%
  mutate(
    grade76 = ifelse(is.na(grade76), median(grade76, na.rm = TRUE), grade76),
    exp76 = ifelse(is.na(exp76), median(exp76, na.rm = TRUE), exp76)
  ) %>%
  na.omit()

# Record cleaned number of rows
cleaned_count <- nrow(df_clean)

# Data frame for visualization
counts_df <- tibble(
  Stage = factor(c("Before Cleaning", "After Cleaning"), levels = c("Before Cleaning", "After Cleaning")),
  Rows = c(initial_count, cleaned_count)
)

# Visualize row counts before and after cleaning
p_row_counts <- ggplot(counts_df, aes(x = Stage, y = Rows, fill = Stage)) +
  geom_bar(stat = "identity", width = 0.5, show.legend = FALSE) +
  scale_fill_manual(values = c("blue", "green")) +
  labs(title = "Row Counts Before and After Data Cleaning", x = NULL, y = "Number of Rows") +
  theme_minimal()

# Print the plot
print(p_row_counts)

```

:::


## Selection of Target and Predictor Variables

::: {.panel-tabset}

### Target Variable and Predictor Variables


-Target Variable:

The primary variable of interest, `wage76`, represents the wages of individuals in 1976 and serves as the dependent variable in our LASSO model.

```{r define-response-variable, echo=TRUE}
y <- df_clean$wage76
```

-Predictor Variables:

Variables selected based on their theoretical relevance to wage determination included education level (`grade76`, `col4`), work experience (`exp76`), and demographic factors (e.g., age, race, geographic location).


### Visualizing Key Variables

With the data now clean and the variables of interest identified, visualizing these can provide deeper insights into their distribution and relationships within the dataset. This helps in understanding the dynamics and potential influences on wages in 1976

```{r visualizations-key-variables, fig.cap="Figure 4: Visualizations of Key Variables"}
library(ggplot2)
library(patchwork)

# Wage Distribution
p_wage <- ggplot(df_clean, aes(x = wage76)) +
  geom_histogram(bins = 30, fill = "blue", color = "black", alpha = 0.7) +
  labs(title = "Distribution of Wages in 1976", x = "Wage", y = "Frequency")

# Education Distribution
p_education <- ggplot(df_clean, aes(x = grade76)) +
  geom_histogram(bins = 12, fill = "green", color = "black", alpha = 0.7) +
  labs(title = "Distribution of Education Levels in 1976", x = "Education Level", y = "Frequency")

# Experience Distribution
p_experience <- ggplot(df_clean, aes(x = exp76)) +
  geom_histogram(bins = 30, fill = "red", color = "black", alpha = 0.7) +
  labs(title = "Distribution of Work Experience in 1976", x = "Years of Experience", y = "Frequency")

# Combine plots using patchwork
d_plot <- p_wage + p_education + p_experience + plot_layout(ncol = 1)

# Display the combined plot
d_plot
```
:::


## Feature Scaling

::: {.panel-tabset}

### Why Feature Scaling

Effective feature scaling is essential before fitting the LASSO model to ensure each variable contributes equally to the analysis. This prevents any feature from disproportionately influencing the outcome due to scale variance.

-**Standardization Process**: All features are normalized to have zero mean and unit variance. This step is crucial for models that apply a penalty on the size of coefficients, such as LASSO.

```{r feature-scaling-setup, echo=TRUE}
library(caret)
library(glmnet)

# Selecting only numeric features and excluding the target variable 'wage76'
numeric_features <- select(df_clean, where(is.numeric), -wage76)

# Converting the selected features into a matrix, as required by glmnet
features <- data.matrix(numeric_features)
preProcValues <- preProcess(features, method = c("center", "scale"))
features_scaled <- predict(preProcValues, features)
```


### Before and After Feature Scaling

```{r data-feature-visualization, fig.cap="Figure 5: Distribution of Feature 'exp76' Before and After Scaling"}
library(ggplot2)
library(gridExtra)  # Ensure this package is installed for plot_layout

# Selecting a feature for comparison
feature_name <- "exp76"  # Example feature

# Plotting the distribution before scaling
p_before_scaling <- ggplot(df_clean, aes(x = get(feature_name))) +  # Using get() for dynamic variable name
  geom_histogram(binwidth = 1, fill = "#F8766D", color = "#FFFFFF", alpha = 0.8) +
  labs(title = paste("Distribution of", feature_name, "Before Scaling"),
       x = "Years of Experience",
       y = "Frequency") +
  theme_minimal() +
  theme(plot.title = element_text(size = 14, face = "bold", hjust = 0.5),
        axis.title.x = element_text(size = 12),
        axis.title.y = element_text(size = 12),
        axis.text.x = element_text(size = 11),
        axis.text.y = element_text(size = 11),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        plot.margin = unit(c(1, 1, 1, 1), "lines"))  # Increase margins around the plot

# Extracting the scaled values for the selected feature
scaled_feature <- features_scaled[, feature_name]

# Plotting the distribution after scaling
p_after_scaling <- ggplot(data.frame(scaled_feature = scaled_feature), aes(x = scaled_feature)) +
  geom_histogram(binwidth = 0.2, fill = "#00BFC4", color = "#FFFFFF", alpha = 0.8) +
  labs(title = paste("Distribution of", feature_name, "After Scaling"),
       x = "Scaled Years of Experience",
       y = "Frequency") +
  theme_minimal() +
  theme(plot.title = element_text(size = 14, face = "bold", hjust = 0.5),
        axis.title.x = element_text(size = 12),
        axis.title.y = element_text(size = 12),
        axis.text.x = element_text(size = 11),
        axis.text.y = element_text(size = 11),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        plot.margin = unit(c(1, 1, 1, 1), "lines"))  # Maintain consistent margins

# Combine the plots side by side
combined_plot <- p_before_scaling + p_after_scaling + plot_layout(ncol = 2)
print(combined_plot)
```
:::

## Cross Validation for Optimal $\lambda$

Selecting the optimal regularization parameter, λ, is crucial for balancing the complexity and accuracy of the LASSO model.

**Cross-Validation Technique**

It is used to determine the λ that minimizes prediction error. This technique ensures the model performs well on unseen data by validating the model across multiple data subsets.

```{r cross-validation-visualization, fig.cap="Figure 6: Cross-Validation Curve",fig.width=10, fig.height=5}
library(glmnet)
library(ggplot2)
library(dplyr)

# Define the response variable y
y <- df_clean$wage76
set.seed(123) # For reproducibility
cv_outcome <- cv.glmnet(features_scaled, y, alpha = 1, nfolds = 10)

# Creating a data frame from the glmnet cross-validation output
cv_data <- as.data.frame(cv_outcome$cvm)
names(cv_data) <- c("mse")
cv_data$lambda <- log(cv_outcome$lambda)
cv_data$lambda_min <- log(cv_outcome$lambda.min)
cv_data$lambda_1se <- log(cv_outcome$lambda.1se)

# Plotting with ggplot2
Cross_Validation_plot <- ggplot(cv_data, aes(x = lambda, y = mse)) +
  geom_line(color = "blue", linewidth = 1) +
  geom_vline(xintercept = cv_data$lambda_min, linetype = "dashed", color = "red", linewidth = 1) +
  geom_vline(xintercept = cv_data$lambda_1se, linetype = "dashed", color = "green", linewidth = 1) +
  labs(title = "Cross-Validation Curve for LASSO Regression",
       subtitle = "Dashed lines represent the lambda.min and lambda.1se",
       x = "Log(Lambda)",
       y = "Mean Squared Error",
       caption = "Red: lambda.min, Green: lambda.1se") +
  theme_minimal() +
  theme(plot.title = element_text(size = 20, face = "bold"),
        plot.subtitle = element_text(size = 14),
        plot.caption = element_text(size = 12),
        axis.title = element_text(size = 16),
        axis.text = element_text(size = 12))
Cross_Validation_plot
```


## Model Coefficients and Interpretation

::: {.panel-tabset}

### Interpretation

Analyzing the coefficients after fitting the model with the optimal λ reveals which variables significantly influence the dependent variable.

- **Significance of Coefficients**: Coefficients that remain significant (not shrunk to zero) are key predictors of wages.

- **Interpretation of Results**: The size and direction of these coefficients provide insights into how each predictor affects wage levels.

### Visualization of Model Outcomes

```{r optimal-coefficients-visualization,  fig.cap="Figure 7: Visualization of LASSO Coefficients"}
library(knitr)
library(glmnet)

# Ensure the cv_outcome object is available and correctly specified
if (exists("cv_outcome")) {
  # Extract coefficients using a safe method
  coef_optimal <- tryCatch({
    coef(cv_outcome, s = "lambda.min", exact = FALSE)
  }, error = function(e) {
    message("Failed to extract coefficients: ", e$message)
    NULL  # Return NULL if there's an error
  })

  if (!is.null(coef_optimal) && inherits(coef_optimal, "dgCMatrix")) {
    # Convert the matrix to a data frame for better handling
    coef_matrix <- as.matrix(coef_optimal)
    coef_df <- as.data.frame(coef_matrix, stringsAsFactors = FALSE)
    coef_df$Variable <- rownames(coef_matrix)  # Add variable names as a new column
    rownames(coef_df) <- NULL  # Clean up row names

    # Rename the coefficient column for clarity
    names(coef_df)[1] <- "Coefficient"
    cv_outcome_plot <- ggplot(coef_df, aes(x = reorder(Variable, Coefficient), y = Coefficient, fill = Coefficient > 0)) +
        geom_col() +
        coord_flip() +  # Makes the plot horizontal for better readability
        labs(title = "Visualization of LASSO Coefficients", x = "Predictors", y = "Coefficient Value") +
        scale_fill_manual(values = c("red", "blue"), name = "Sign of Coefficient",
                          labels = c("Negative", "Positive")) +
        theme_minimal()
    cv_outcome_plot
  } else {
    message("Coefficient data is not available or invalid.")
  }
} else {
  message("cv_outcome is not available. Please check model fitting.")
}
```
A positive coefficient indicates a direct relationship "where increases in the predictor correspond to increases in the target". Conversely, a negative coefficient signifies an inverse relationship.

:::

## LASSO VS MLR

Our analysis with the LASSO model has effectively highlighted the most significant factors influencing wages.

However, to deepen our understanding of these results, we employed Multiple Linear Regression (MLR) as a comparative tool. 

-**LASSO Regression**: Applies a penalty term to reduce the influence of less significant predictors, enhancing model simplicity and accuracy.

-**MLR**: Provides a baseline by including all predictors without regularization, illustrating potential overfitting issues.

## Comparative Analysis on LASSO and MLR Models

::: {.panel-tabset}

### Visual Outcome of LASSO Vs MLR analysis

Both models are applied to the same cleaned dataset to ensure a fair comparison.

```{r mlr-model, message=TRUE, warning=TRUE, results='asis'}
library(glmnet)
library(caret)
library(knitr)
library(readr)
library(dplyr)

# Ensure df_clean is prepared as earlier described
df_clean <- read_csv("RetSchool.csv", show_col_types = FALSE) %>%
  mutate(grade76 = ifelse(is.na(grade76), median(grade76, na.rm = TRUE), grade76),
         exp76 = ifelse(is.na(exp76), median(exp76, na.rm = TRUE), exp76)) %>%
  na.omit()

# Fitting Multiple Linear Regression (MLR)
model_mlr <- lm(wage76 ~ ., data = df_clean)
# Extract coefficients from the MLR model
coefficients_mlr <- coef(model_mlr)

# Fitting LASSO model
set.seed(123)  # for reproducibility
features_scaled <- as.matrix(df_clean[setdiff(names(df_clean), "wage76")])
cv_outcome <- cv.glmnet(features_scaled, df_clean$wage76, alpha = 1)

# Extracting coefficients from the LASSO model using predict with s="lambda.min"
lasso_coefs <- predict(cv_outcome, type="coefficients", s = "lambda.min")[1:ncol(features_scaled),,drop=FALSE]

# Filter non-zero coefficients
non_zero_lasso <- lasso_coefs[lasso_coefs[,1] != 0, , drop=FALSE]

# Align MLR coefficients with non-zero LASSO coefficients
non_zero_names <- rownames(non_zero_lasso)
matching_mlr_coefs <- coefficients_mlr[non_zero_names]

# Create a data frame for comparison
coefficients_comparison <- data.frame(
  Predictor = non_zero_names,
  Coefficient_MLR = as.numeric(matching_mlr_coefs),
  Coefficient_LASSO = as.numeric(non_zero_lasso)
)

kable(coefficients_comparison, format = "html", caption = "Comparison of MLR and LASSO Coefficients", align = 'c')
```

-**Intercept**

-MLR: 0.0041560

-LASSO: 0.1035662

-**Explanation**: The intercept represents the expected value of wage76 when all other predictors are zero. The significant increase in the intercept from MLR to LASSO suggests that LASSO adjusts to better account for the average baseline wage across all observations.


### Graphical Representation of the outcomes

```{r visualization, echo=FALSE, fig.cap="Figure 8: Comparison of Coefficients from MLR and LASSO"}
library(ggplot2)
library(dplyr)

# Assuming coefficients_comparison is a dataframe containing coefficients from both models
coefficients_long <- coefficients_comparison %>%
  pivot_longer(cols = c(Coefficient_MLR, Coefficient_LASSO), names_to = "Model", values_to = "Coefficient")

Predictors_plot <- ggplot(coefficients_long, aes(x = Predictor, y = Coefficient, fill = Model)) +
  geom_bar(stat = "identity", position = position_dodge()) +
  labs(title = "Coefficient Comparison between MLR and LASSO",
       x = "Predictors",
       y = "Coefficient Value") +
  scale_fill_manual(values = c("skyblue", "orange"), labels = c("MLR", "LASSO")) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 65, hjust = 1))

Predictors_plot
```

-**Stability Across Models**: Predictors whose coefficients are consistent across both MLR and LASSO are likely very reliable indicators of wage variations, underscoring their importance regardless of the modeling approach used.


:::


## Results 

### Application to the Return to School Dataset

Exploring the specific implications of our findings within the context of the Return to School dataset:

-**What We Analyzed**: We focused on understanding how various educational, demographic, and work experience factors influence wage disparities in 1976.

-**Why It Matters**: This analysis is crucial for identifying key areas where educational and economic policies can be targeted to reduce wage inequality.

-**How We Did It**: Using LASSO and MLR, we were able to discern which variables significantly impact wages, with LASSO providing a more streamlined model that avoids overfitting and highlights the most impactful factors.




## Conclusion

Our comprehensive analysis using LASSO regression has identified pivotal factors that influenced wages in 1976, with a focus on the impact of educational attainment and age.

::: {.panel-tabset}

### Key Insights

```{r continuous-impact, fig.cap="Figure 9:Impact of Continuous Variables on Wages",fig.width=10, fig.height=5}
plot_grade76 <- ggplot(df_clean, aes(x = grade76, y = wage76)) +
  geom_point(aes(color = age76), alpha = 0.6) +
  geom_smooth(method = "lm", color = "blue", se = TRUE) +
  labs(title = "Impact of Education on Wages by Age",
       subtitle = "Each point represents an observation colored by age",
       x = "Years of Education (grade76)",
       y = "Wage in 1976",
       color = "Age") +
  theme_minimal() +
  theme(legend.position = "bottom")

# Create a scatter plot of age76 vs. wage76 with a regression line
plot_age76 <- ggplot(df_clean, aes(x = age76, y = wage76)) +
  geom_point(aes(color = grade76), alpha = 0.6) +
  geom_smooth(method = "lm", color = "red", se = TRUE) +
  labs(title = "Impact of Age on Wages by Education",
       subtitle = "Each point represents an observation colored by years of education",
       x = "Age in 1976",
       y = "Wage in 1976",
       color = "Education Level (grade76)") +
  theme_minimal() +
  theme(legend.position = "bottom")
library(gridExtra)
grade76_plot <- grid.arrange(plot_grade76, plot_age76, ncol = 2)
```


- **Educational Impact on Earnings**: We found a strong correlation between higher education levels and higher wages, underscoring the substantial returns on educational investments. This insight supports the argument for increasing access to higher education as a means to elevate income levels.
- **Age and Earnings Correlation**: Our results show that older age groups tend to earn more, which reflects the cumulative benefits of experience and ongoing education over time.

### Future Research Directions

-This study opens the door for further research into additional socioeconomic factors that could affect wage disparities. 

-Future studies could explore the impact of technological advances, economic policies, and other demographic changes on wage trends. 

:::


## Appreciation and Open for Questions

Thank you Dr.Cohen for your guidance and support throughout the semester. We appreciate everyone for your attention and interest in our findings. We are now open to any questions you may have or discussions you would like to engage in. Your feedback and suggestions for further research areas are highly welcome.



















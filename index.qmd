---
title: "Innovative Data Exploration with LASSO: Unveiling Patterns in Earnings and Education"
author: "Nikhitha Amireddy, Ishrath Jahan, Sai Kumar Miryala, Muhammad Usman Aslam"
date: '`r Sys.Date()`'
format:
  html:
    code-fold: true
course: STA 6257 - Advanced Statistical Modeling
bibliography: references.bib # file contains bibtex for references
#always_allow_html: true # this allows to get PDF with HTML features
self-contained: true
execute: 
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
---

## Introduction

LASSO, Least Absolute Shrinkage and Selection Operator, is a penalized linear regression method that reduces the number of variables in the model, useful for avoiding overfitting. It works by minimizing the residual sum of squares subject to the sum of the absolute value of the coefficients being less than a constant, simplifying the model and potentially improving prediction accuracy.

In 1996, Robert Tibshirani [@Tibshirani1996] introduced LASSO regression to further the discussion about variable selection. This provided a new and sophisticated solution for the problems in the course of predictive modeling, especially when it came to big and complex datasets. It is therefore important that the model remains parsimonious in nature, explains easily interpretable phenomena, and reduces the risks associated with overfitting by applying a penalty to the absolute values of regression coefficients [@Zhao2006], [@Fan2011]. This innovation was therefore taken up by many scientific disciplines, characterizing the beginning of an era where high-dimensional datasets could be probed with more depth for complexities with better clarity [@Meinshausen2006] @Hastie2009]. The popularity of LASSO regression has risen with the birth of Least Angle Regression by Efron et al. [@Efron2004]  in 2004. 

LASSO regression has particularly become a key instrument with which to insight the complex dynamics underlying socioeconomic trends, public health outcomes, and environmental studies—hence, significantly enriching the toolset available to the data scientist [@Belloni2013], [@Park2008]. Logistic regression is further extended by integration with the capabilities of LASSO, offering a refined method in prediction for the categorical outcomes [@Li2022]. This synergy provides a response to long-term barriers to predictive modeling such as overfitting and multicollinearity with a scope of applications in, in particular, economic forecasting and health diagnostics, amongst many more [@Sauerbrei2015].

The 21st century continues, and the data revolution heightens. A dataset in relation to the augmented domain complexity continues to pervade. Here, LASSO regression does not exist just as a statistical tool but as an innovation beacon assisting researchers with precision and simplicity within the dynamic landscape [@Chintalapudi2022]. The introduction of logistic regression into the area of LASSO has been especially dynamic, providing a nuanced approach to modeling of categorical outcomes [@Simon2013]. This is necessary for areas from epidemiology to finance [@Friedman2010]. This thus places this combination as a great leap forward in the direction of data-driven insights aimed at action, where traditional analytical methods might possibly face challenges of interpretation, firmly situating logistic regression with LASSO among the basic constitutions of modern statistical analysis and predictive modeling [@Hastie2009]. 

Indeed, the way of LASSO development has been marked by a series of important developments that pinpoint the historical trajectory, including the emergence of Least Angle Regression by Efron and colleagues in pointing out the potential, influence, and continued utility in research [@Zhou2022; @Witten2010]. This cutting-edge technology is used throughout a wide spectrum of industries with the goal of dismantling complex data structures and, eventually, extracting meaningful patterns from trends as socioeconomic, public health, and environmental studies. LASSO regression adapted for logistic models was one of the best steps to ward off the impediments towards modeling categorical data with more precision [@Tibshirani2012].


## Literature Review

The versatility of LASSO regression is reflected in the extensive literature discussing its various applications. Notably, LASSO regression has made significant contributions to economics. For example, Zhou et al. (2022)[@Zhou2022] used the LASSO model to investigate what determines the growth of quality of life in China and found that the LASSO model is a good tool for selecting important economic predictors out of a large number of regressors. Thus, it makes important improvements on economic models useful for economic policy stakeholders and researchers in how to promote economic growth. The technique has also been critical for important developments in the area of bioinformatics. 

Lu, et al. (2011) [@Lu2011] showed how LASSO regression can be applied to constructing predictive models — based on gene expressions and other high-throughput data — to understand the relationships between gene expressions and multiple phenotypes, which are critical for understanding genetic pathways and cell systems. In this example, the LASSO regression was used to build a microRNA-target regulatory network, which provides insight into gene regulation mechanisms, as well as applications in personalized medicine and in the understanding of genetic diseases [@Musoro2014].
LASSO regression has expedited the processing and analysis of large datasets in the environmental and energy sectors. Wang et al. (2017) [@Wang2017] detailed its use in predicting ship fuel consumption for diminished environmental impact and increased fuel efficiency in maritime operations.

LASSO regression aids in selecting the most informative variables that influence fuel usage and empowers the maritime industry's sustainability and operational performance efforts. LASSO regression has been a meaningful addition to the analysis of multi-factorial diseases and health outcomes in public health research. In an analysis of dietary intakes for breast cancer, McEligot et al. (2020) [@McEligot2020] showed that logistic LASSO can handle high-dimensional data to uncover the key drivers of health, and this has broad applicability in public health for unraveling and prioritizing the risk factors for a variety of illnesses. His role as a facilitator for the development of machine learning models (especially in feature selection) has been pivotal to improving the accuracy of those predictions as LASSO was proving its worthiness time and time again.

Muthukrishnan and Rohini (2016) [@Muthukrishnan2016] showcased this by demonstrating the ability of LASSO regression “to select the relevant feature attributes of predictive modelling for machine learning”.
After gaining prominence within the traditional domain, the applications of LASSO regression have extended into new spheres such as financial modeling, where it assists analysts in singling out influential market predictors from large datasets characterized by volatility and noise, eventually translating into forecasts and investment strategies [@Friedman2010; @Hastie2015]. The adaptability and precision of LASSO regression within financial analysis underline its growing promise and popularity. Another field where LASSO regression's ability to parse through voluminous climate variable datasets to extract critical indicators of environmental change has been crucial is climate research and sustainability studies. The power of this application is not only a testament to the adaptability but also reflects the urgency in deploying LASSO regression in the face of challenges such as global warming and biodiversity loss, where parsing through complex environmental patterns becomes critical to devising strategies to mitigate its devastating impacts [@Chintalapudi2022; @Li2022]. 

Digital health technologies have opened a trove of opportunities for LASSO regression to analyze the complex interplay between individual lifestyle factors, genetic predispositions, and health outcomes. The inclusion of LASSO in this field of research is an acknowledgement that it is uniquely suited to handle complex, multidimensional data that is needed to understand and generate personalized medicine or public health interventions—a frontier of healthcare that holds promise for applications where LASSO's valuable data-driven insights can be channeled to address bespoke health solutions [@Simon2013; @Park2008] and Sohaee and Bohluli (2024b)[@2024b] employ Lasso polynomial regression to analyze the intricate relationship between socioeconomic, demographic, and technological factors and fatal traffic accidents, demonstrating its effectiveness in handling categorical variables and preventing overfitting. In their study on breast cancer screening adherence (Sohaee and Bohluli 2024a)[@2024a], they utilized Lasso regression to identify significant predictors from various socioeconomic factors, highlighting health disparities and advocating for targeted healthcare policies. Both studies underscore the importance of advanced statistical techniques, such as Lasso regression, in understanding complex datasets to inform policy decisions and improve outcomes in transportation safety and healthcare.

Efron et al. (2004)[@annalsofstatistics] introduced Least Angle Regression (LARS) as adept for high-dimensional data analysis, emphasizing its suitability for large datasets with thousands of predictors. LARS enables efficient feature selection and model building, particularly when adapted for Lasso regression. By combining LARS with Lasso, researchers can select relevant features, manage high-dimensional data, and improve prediction accuracy while mitigating multicollinearity and overfitting concerns. [@10341221] research introduced a novel approach to predicting heart disease by leveraging a sophisticated computer model. By integrating LASSO, a technique akin to a smart sieve for crucial information, with other methods, the accuracy of predictions is enhanced. It showcases how advanced computer methods can assist doctors in foreseeing health issues preemptively, potentially leading to improved prevention and treatment strategies for heart disease. Researchers analyzed extensive health data, identifying key factors using LASSO, and developed a highly effective predictive model. This could translate to earlier and personalized care for individuals at risk of heart disease, promising significant advancements in healthcare.

Indeed, these and many more diverse applications are a testament to the potency of LASSO regression as a powerful tool in the arsenal of researchers and practitioners across disciplines, as they grapple with increasingly high-resolution and large datasets. By simplifying complex models and identifying important predictors from large datasets, LASSO regression has cemented itself as an essential part of modern statistical analysis and predictive modeling [@Belloni2013; @Zou2005; @Witten2010; @Tibshirani2012].


## Methods

### Predictive Modeling and Feature Selection

In our study, we employ LASSO Regression (Least Absolute Shrinkage and Selection Operator) as a cornerstone for both predictive modeling and feature selection. This approach not only facilitates prediction but also imposes a penalty on the magnitude of regression coefficients, thereby enhancing feature selection through sparsity.

#### LASSO Regression Variants and Equations

- **Standard LASSO Regression:**

  The Standard LASSO regression introduces a regularization term (`λ`) to the cost function, which penalizes the absolute value of the coefficients. The objective function is defined as:
  
  $$ L(\beta) = \sum_{i=1}^{n} (y_i - \beta_0 - \sum_{j=1}^{p} \beta_j x_{ij})^2 + \lambda \sum_{j=1}^{p} |\beta_j| $$

  where \(y_i\) represents the observed outcomes, \(x_{ij}\) the predictor variables, \(\beta_j\) the coefficients, \(n\) the number of observations, \(p\) the number of predictors, and \(\lambda\) the regularization parameter controlling the degree of shrinkage applied to the coefficients.

- **Elastic Net:**

  Elastic Net is a blend of Lasso and Ridge regression, incorporating both L1 and L2 regularization terms to the cost function, beneficial for dealing with correlated predictors. The Elastic Net objective function is:
  
  $$ L(\beta) = \sum_{i=1}^{n} (y_i - \beta_0 - \sum_{j=1}^{p} \beta_j x_{ij})^2 + \lambda_1 \sum_{j=1}^{p} |\beta_j| + \lambda_2 \sum_{j=1}^{p} \beta_j^2 $$

- **Adaptive Lasso:**

  Adaptive Lasso varies the penalty across coefficients by introducing weights, aiming for variable selection consistency. Its equation is:
  
  $$ L(\beta) = \sum_{i=1}^{n} (y_i - \beta_0 - \sum_{j=1}^{p} \beta_j x_{ij})^2 + \lambda \sum_{j=1}^{p} w_j |\beta_j| $$

- **Group Lasso:**

  Designed for scenarios with clustered predictor variables, Group Lasso penalizes groups of coefficients collectively. Its formulation is:
  
  $$ L(\beta) = \sum_{i=1}^{n} (y_i - \beta_0 - \sum_{j=1}^{p} \beta_j x_{ij})^2 + \lambda \sum_{g=1}^{G} p_g ||\beta_g||_2 $$

### Model Formulation

The overarching objective of LASSO and its variants is to minimize the discrepancy between observed outcomes and model predictions, incorporating a penalty term that enforces coefficient sparsity.

$$ \min_{\beta} \left\{ \frac{1}{2n} \sum_{i=1}^{n} (y_i - \beta_0 - \sum_{j=1}^{p} \beta_j x_{ij})^2 + \lambda \sum_{j=1}^{p} |\beta_j| \right\} $$

### Detailed Methodological Techniques

1. **Integration of Multiple Response Variables:**

   Utilizes a composite score to encapsulate multiple dimensions of phenomena.

   $$ C = w_1 S_1 + w_2 S_2 + \ldots + w_k S_k $$

2. **Cross-Validation in LASSO Regression:**

   Employs systematic data partitioning to identify the optimal \(λ\).

   $$ CV(\lambda) = \frac{1}{K} \sum_{k=1}^{K} MSE_k(\lambda) $$

3. **Subgroup Analysis:**

   Investigates the variability of effects across demographics.

   $$ y = \beta_0 + \beta_1 x + \beta_2 z + \beta_3 (x \times z) + \epsilon $$

4. **Interaction Terms:**

   Explores the conditional effects between variables.

   $$ y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 (x_1 \times x_2) + \epsilon $$

5. **Handling Missing Data:**

   Adopts strategies such as imputation.

   $$ \hat{y} = \frac{1}{m} \sum_{i=1}^{m} \hat{y}_i $$

6. **Logistic Regression with LASSO:**

   Enhances categorical outcome prediction through feature selection.

   $$ \log \left( \frac{p}{1-p} \right) = \beta_0 + \lambda \sum_{j=1}^{p} |\beta_j| + \sum_{j=1}^{p} \beta_j x_{ij} $$


### Implications and Recommendations

The inclusion of equations and detailed explanations underscores the technical foundation of our methodology, illuminating the mechanisms through which LASSO regression and related techniques facilitate predictive modeling and feature selection. By rigorously applying these methods, our study aims to achieve a nuanced understanding of the dataset, guiding future research and practice in statistical modeling.



## Analysis and Results

## Data Description: 

RetSchool from R datasets:
[https://vincentarelbundock.github.io/Rdatasets/doc/Ecdat/RetSchool.html](https://vincentarelbundock.github.io/Rdatasets/doc/Ecdat/RetSchool.html)

The dataset contains 5,225 entries and 18 columns, indicating a fairly large size in terms of the number of records. Here are some key points observed from the initial examination:

•	Multiple Features: 

There are 18 columns in total, which include a variety of variables such as education level (grade76), experience (exp76), race (black), region indicators (south76, smsa76, region, smsa66), family background (momdad14, sinmom14, nodaded, nomomed, daded, momed, famed), age (age76), and college education indicator (col4). 

This variety suggests a potentially high-dimensional feature space, especially considering interactions or polynomial features could be explored.

•	Missing Values: Some columns have missing values (wage76, grade76, exp76, south76), which LASSO regression can handle well if preprocessing steps include imputation or if the analysis focuses on available data.

•	Binary and Continuous Variables: The dataset comprises both binary (e.g., black, momdad14, sinmom14, nodaded, nomomed, col4) and continuous variables (e.g., wage76, grade76, exp76, daded, momed, age76), suggesting that LASSO could be beneficial in handling a mix of variable types by identifying which variables contribute most to the prediction.
Given these characteristics, LASSO regression is a strong candidate for several reasons:

1.	Feature Selection: LASSO's ability to shrink coefficients of less relevant features to zero will help in identifying which variables are most predictive of the outcome, likely simplifying the model by excluding variables that do not contribute significantly.

2.	Handling Multicollinearity: If variables such as education level and experience are highly correlated, LASSO can help in mitigating multicollinearity by selecting one or a few variables from a group of correlated variables, which enhances model stability and interpretability.

3.	Model Interpretability: With many variables in play, reducing the model to only include significant predictors can make it easier to understand and explain, which is often crucial in research and policy analysis contexts.

4.	Robustness to Overfitting: The regularization aspect of LASSO helps in preventing overfitting, making the model more generalizable to unseen data, which is particularly valuable in predictive modeling.

### The choice of a dataset like "RetSchool.csv":

This dataset provides a thorough overview of the socioeconomic environment in 1976, emphasizing the critical roles of education and work experience in determining wages, while also highlighting the influence of demographic factors. The insights suggest that while individual effort and achievement (as measured by education and experience) are important, background factors such as race, region, and early family environment also play significant roles in shaping economic outcomes.


```{r}
library(readr)
data_RetSchool <- read_csv("RetSchool.csv")
print(data_RetSchool, n = 10)
summary(data_RetSchool)
```

This dataset provides a rich collection of socio-economic and demographic information about individuals, focusing on their wages, education levels, work experience, and other personal and family background characteristics.Here's an overview of the dataset and the key variables it includes:

## Key Variables:

•	wage76: The wage of individuals in 1976, providing insight into the economic status and disparities within the population.

•	grade76: The highest grade completed by 1976, indicating the educational attainment and its distribution among individuals.

•	exp76: Work experience in years by 1976, shedding light on the labor market engagement and its impact on wages.

•	black: A binary variable indicating whether an individual is black, offering a lens into the racial composition of the dataset.

•	south76: Indicates whether an individual lived in the South in 1976, allowing for regional analysis and its impact on socio-economic outcomes.

•	smsa76: A binary variable indicating residence in a Standard Metropolitan Statistical Area (SMSA) in 1976, relevant for urban versus rural disparities.

•	region: Categorical variable representing different regions, useful for understanding geographic influences on economic and educational outcomes.

•	momdad14, sinmom14: Variables indicating living situations at age 14, which can be crucial for studies on family structure and its effects on individual development.

•	daded, momed: Education levels of dad and mom, respectively, providing data on family background and its potential influence on educational attainment.

•	age76: Age of individuals in 1976, necessary for demographic analyses and understanding the age distribution of the workforce.

•	col4: An unspecified indicator variable, the context of which would need clarification for meaningful analysis.

## Insights and Implications from the dataset

The RetSchool dataset offers a comprehensive view of the interplay between education, work experience, and wages, highlighting the positive correlation between educational attainment and economic outcomes. It also underscores the significant impact of demographic factors such as race, region, and family background on these outcomes.
The diversity in educational attainment, work experience, and wages across different demographic groups suggests the presence of underlying socio-economic disparities. The dataset allows for an exploration of how early-life conditions, including family structure and parental education, affect later-life economic success.


### Data Visualization

```{r}
library(ggplot2)
library(patchwork)
data_RetSchool <- read.csv("RetSchool.csv")
data_RetSchool <- na.omit(data_RetSchool)
```


```{r}
library(ggplot2)
library(patchwork)

# Summary Statistics Plots
# Histograms and density plots for wage76, grade76, and exp76
p_wage_hist <- ggplot(data_RetSchool, aes(x = wage76)) +
  geom_histogram(bins = 30, fill = "skyblue", color = "black") +
  geom_density(alpha = .2, fill = "#FF6666") +
  labs(title = "Distribution of wage76")

p_grade_hist <- ggplot(data_RetSchool, aes(x = grade76)) +
  geom_histogram(bins = 30, fill = "skyblue", color = "black") +
  geom_density(alpha = .2, fill = "#FF6666") +
  labs(title = "Distribution of grade76")

p_exp_hist <- ggplot(data_RetSchool, aes(x = exp76)) +
  geom_histogram(bins = 30, fill = "skyblue", color = "black") +
  geom_density(alpha = .2, fill = "#FF6666") +
  labs(title = "Distribution of exp76")

# Box plots for wage76, grade76, and exp76
p_wage_box <- ggplot(data_RetSchool, aes(y = wage76)) +
  geom_boxplot(fill = "lightgreen") +
  labs(title = "Box Plot of wage76")

p_grade_box <- ggplot(data_RetSchool, aes(y = grade76)) +
  geom_boxplot(fill = "lightgreen") +
  labs(title = "Box Plot of grade76")

p_exp_box <- ggplot(data_RetSchool, aes(y = exp76)) +
  geom_boxplot(fill = "lightgreen") +
  labs(title = "Box Plot of exp76")

# Combine the plots into a single layout
summary_stats_layout <- (p_wage_hist | p_grade_hist | p_exp_hist) / 
                        (p_wage_box | p_grade_box | p_exp_box) + 
                        plot_annotation(title = "Summary Statistics of Key Numeric Variables")
print(summary_stats_layout)
```


### Summary Statistics of Key Numeric Variables
### Distribution and Box Plot Analysis:

•	Wage in 1976 (wage76):
o	The histogram for wage76 shows a right-skewed distribution, indicating that most individuals earned lower wages, with a few outliers earning significantly more. This pattern is typical for income data.
o	The box plot reveals a wide range of wages with several outliers on the higher end. The median wage is relatively low compared to the mean, further indicating the skewness towards lower wage values.

•	Education Level in 1976 (grade76):
o	The distribution of grade76 shows a multi-modal pattern, suggesting concentrations of individuals at specific education levels. The peaks likely represent completion of certain education milestones (e.g., high school, some college).
o	The box plot indicates a relatively normal distribution with a few outliers, suggesting most individuals had a similar level of education, with a median around high school completion.

•	Work Experience in 1976 (exp76):
o	The histogram for exp76 also exhibits a right-skewed distribution, suggesting that a larger number of individuals had lower work experience, with fewer individuals having high levels of experience.
o	The box plot for exp76 shows the presence of outliers with exceptionally high experience levels, and a median experience level that indicates the majority had moderate to low work experience.

These visualizations help in understanding the basic characteristics of the dataset, including the central tendency and variability of wages, education levels, and work experience among the individuals in 1976. The presence of skewness and outliers in these distributions indicates the diversity in the economic and educational background of the population studied.

```{r}
# Create the 'LivingSituationAt14' column based on existing data

data_RetSchool$LivingSituationAt14 <- ifelse(data_RetSchool$momdad14 == 1, 'Both Parents', 
                                   ifelse(data_RetSchool$sinmom14 == 1, 'Single Mom', 'Other'))
data_RetSchool$LivingSituationAt14 <- factor(data_RetSchool$LivingSituationAt14)

```

### Wage Distribution by Living Situation at Age 14:

```{r}
#Wage Distribution by Living Situation at Age 14
p_wage_distribution <- ggplot(data_RetSchool, aes(x = wage76)) + geom_histogram(aes(y = after_stat(density)), bins = 20, fill = "skyblue", color = "black") + geom_density(alpha = 0.2, fill = "#FF6666") + facet_wrap(~ LivingSituationAt14, scales = "free_y") + labs(title = "Wage Distribution by Living Situation at Age 14", x = "Wage in 1976 (wage76)", y = "Density") + theme_minimal()
#ggsave("wage_distribution_by_living_situation.png", p_wage_distribution, width = 12, height = 4)

print(p_wage_distribution)
```

The faceted charts provide a comparison of wage distribution in 1976 (wage76) across different living situations at age 14, namely "Both Parents", "Single Mom", and "Other". Each facet represents a histogram of wage distribution for a specific living situation, allowing for direct comparison. Key insights:

•	General Distribution: Across all living situations, the wage distribution is right-skewed, indicating a majority of individuals earning lower wages with a tail of higher earners.

•	Differences in Distribution: While the overall shape of the distribution appears similar across different living situations, there are subtle differences in the spread and peaks of the distributions. This suggests that living situation at age 14 may have some influence on wage outcomes, but the effect is not starkly different when visualized this way.

•	Variability: The presence of variability within each living situation category underscores the complexity of factors influencing wage, beyond just early life living conditions.


### Correlation Matrix Plot

```{r}

# Correlation Matrix Plot
library(corrplot)
cor_matrix <- cor(na.omit(data_RetSchool[sapply(data_RetSchool, is.numeric)]))

corrplot(cor_matrix, method = "color",
addCoef.col = "black",  # Color of the correlation coefficients
tl.col = "black",       # Color of the text labels
tl.srt = 45,            # Rotation of text labels
tl.cex = 0.7,           # Font size of text labels
number.cex = 0.5,       # Font size of the correlation coefficients
cl.cex = 0.7,           # Font size of the color legend
addgrid.col = "white",  # Color of the grid lines
tl.pos = "lt",          # Position of text labels (left and top sides)
diag = FALSE)           # Remove the diagonal
```

### Demographic Analysis: 

```{r}

#Demographic Analysis Plots
p_race <- ggplot(data_RetSchool, aes(x = as.factor(black))) + geom_bar(fill = "skyblue") + labs(title = "Distribution by Race", x = "Race (1=Black, 0=Not Black)", y = "Count") + theme_minimal()

p_region <- ggplot(data_RetSchool, aes(x = as.factor(region))) + geom_bar(fill = "lightgreen") + labs(title = "Distribution by Region", x = "Region", y = "Count") + theme_minimal()

p_living_situation <- ggplot(data_RetSchool, aes(x = LivingSituationAt14)) + geom_bar(fill = "lightcoral") + labs(title = "Living Situation at Age 14", x = "Living Situation", y = "Count") + theme_minimal()
#Combine the demographic plots into one figure

demographic_plot <- (p_race | p_region | p_living_situation) + plot_layout(guides = 'collect') & theme(plot.title = element_text(hjust = 0.5))

print(demographic_plot)

```
 

The bar charts provide insights into the demographic composition of the dataset based on race, region, and living situation at age 14.
 
Distribution by Race:

The dataset includes a larger number of individuals who are not black compared to those who are black. This distribution is crucial for understanding the racial composition of the study population and assessing the representativeness of the dataset across different racial groups.

Distribution by Region:

The participants are spread across different regions, with varying counts in each. The diversity in region distribution can help analyze regional influences on wages, education, and other factors of interest.

Living Situation at Age 14:

A significant portion of the participants lived with both parents at age 14, with fewer living with a single mom, and a small number categorized under "Other" living situations. This demographic information can be valuable for studies examining the impact of early family environment on education and economic outcomes.

Comprehensive Demographic Composition:

The combined demographic analysis elucidates the complex interplay between race, region, and early family environment, offering valuable insights for targeted interventions and policies aimed at improving educational and economic opportunities.

### Dataset Summary:

The dataset and its visualizations offer a wealth of insights into the socio-economic characteristics of individuals, focusing on aspects such as wages, education, work experience, race, regional distribution, and living situations at age 14. 

Here's a comprehensive summary of the insights derived from the data and visual analyses:

1. Wage, Education, and Experience Distribution:

Wages in 1976 are right-skewed, indicating that most individuals earned lower wages, with a few outliers earning significantly more. This skewness highlights economic disparities within the population.


Education Levels show a multimodal distribution, with peaks at certain levels likely representing significant milestones in the education system. This suggests the presence of standard education paths followed by the population.

Work Experience also exhibits a right-skewed distribution, with many individuals having lower levels of experience. This reflects the younger or less experienced portion of the workforce.

2. Education and Wage Relationship:

There is a positive correlation between education level and wages, suggesting that higher education levels are associated with higher earning potential. However, the wide range in wages at any given education level underscores the influence of other factors, such as field of study, industry, and geographic location, on wages.

3. Work Experience and Wage Relationship:

A positive trend indicates that individuals with more work experience tend to have higher wages, aligning with the expectation that experience contributes to skill development and higher pay. However, the increase in wages appears to plateau beyond a certain level of experience, hinting at a ceiling effect in certain jobs or industries.

4. Demographic Analysis:

Race: The data set reveals a larger number of non-black individuals compared to black individuals, providing a crucial perspective on the racial composition of the study population.

Region: The participants are spread across different regions, with variable counts, indicating regional diversity that could influence economic outcomes.

Living Situation at Age 14: A significant portion of the participants lived with both parents at age 14, suggesting a predominant family structure. The differences in living situations may have implications for educational and economic outcomes.

5. Correlation Matrix Insights:

The correlation matrix provides a nuanced understanding of how different variables interact. Notably, education level and work experience show positive correlations with wages, but the strength of these relationships varies, highlighting the multifaceted nature of wage determination.

6. Comprehensive Demographic Composition:

The combined demographic analysis elucidates the complex interplay between race, region, and early family environment, offering valuable insights for targeted interventions and policies aimed at improving educational and economic opportunities.

## Summary

The dataset paints a detailed picture of the socio-economic landscape in 1976, emphasizing the critical roles of education and work experience in determining wages, while also highlighting the influence of demographic factors. The insights suggest that while individual effort and achievement (as measured by education and experience) are important, background factors such as race, region, and early family environment also play significant roles in shaping economic outcomes.

## Statistical Modeling

This analysis includes the exploration of a composite score as a new response variable, the application of diverse cross-validation techniques and LASSO parameter adjustments, and alternative approaches to handling missing data. The methodology involves:

1. Integration of Multiple Response Variables
2. Cross-Validation in LASSO Regression

### Integration of Multiple Response Variables

The methodology involved standardizing key variables (`wage76`, `grade76`, `exp76`) and creating a composite score to serve as the response variable. This score aims to encapsulate the combined influence of education, experience, and wages on the subjects' outcomes.

To ensure the model's robustness and identify the most predictive features while mitigating overfitting, we performed cross-validation. Specifically, we utilized a range of lambda values to determine the optimal degree of regularization. This approach not only enhances model accuracy but also aids in discerning the most significant predictors among the variables considered.


```{r}
# Load necessary libraries
library(readr)
library(dplyr)
library(caret)
library(glmnet)

data_RetSchool <- read_csv("RetSchool.csv")
```

### Data Standardization and Composite Score Creation:
```{r}
data_standardized <- data_RetSchool %>%
  mutate_at(vars(wage76, grade76, exp76), ~scale(.) %>% as.vector()) %>%
  mutate(composite_score = rowMeans(select(., wage76, grade76, exp76), na.rm = TRUE))

```
This step involves standardizing the variables `wage76`, `grade76`, and `exp76` to have a mean of 0 and a standard deviation of 1. A composite score is then created as the average of these standardized scores to serve as a new response variable.

### Visualizing the Standardized Variables and Composite Score:
```{r}
# Visualizing the distribution of standardized variables
ggplot(data_standardized, aes(x = composite_score)) +
  geom_histogram(bins = 30, fill = 'green', alpha = 0.7) +
  ggtitle("Distribution of Composite Score")
```

### LASSO Regression Preparation and Execution:
```{r}
# Prepare the dataset for LASSO regression
X_composite <- select(data_standardized, -c(wage76, grade76, exp76, composite_score))
y_composite <- data_standardized$composite_score
y_composite[is.na(y_composite)] <- mean(y_composite, na.rm = TRUE)

# Data partitioning
set.seed(0)
trainIndex <- createDataPartition(y_composite, p = 0.8, list = FALSE)
X_train_composite <- X_composite[trainIndex, ]
X_test_composite <- X_composite[-trainIndex, ]
y_train_composite <- y_composite[trainIndex]
y_test_composite <- y_composite[-trainIndex]

# Imputation and scaling
preProcessImpute <- preProcess(X_train_composite, method = c("medianImpute"))
X_train_preprocessed_composite <- predict(preProcessImpute, X_train_composite)
X_test_preprocessed_composite <- predict(preProcessImpute, X_test_composite)
preProcessScale <- preProcess(X_train_preprocessed_composite, method = c("center", "scale"))
X_train_preprocessed_composite <- predict(preProcessScale, X_train_preprocessed_composite)
X_test_preprocessed_composite <- predict(preProcessScale, X_test_preprocessed_composite)

# Model training with LASSO
lambda_seq <- 10^seq(2, -2, length.out = 100)
set.seed(0)
cv_lasso <- cv.glmnet(as.matrix(X_train_preprocessed_composite), y_train_composite, alpha = 1, lambda = lambda_seq)
best_lambda <- cv_lasso$lambda.min
lasso_best <- glmnet(as.matrix(X_train_preprocessed_composite), y_train_composite, alpha = 1, lambda = best_lambda)

```
### Visualizing the cross-validation results:
```{r}
plot(cv_lasso)

```

### Model Performance Evaluation
```{r}
predictions <- predict(lasso_best, newx = as.matrix(X_test_preprocessed_composite), s = best_lambda)
mse_composite <- mean((y_test_composite - predictions)^2)
r2_composite <- summary(lm(predictions ~ y_test_composite))$r.squared
coefficients <- coef(lasso_best, s = best_lambda)
coefficient_values <- as.vector(coefficients[-1, ])

list(mse_composite = mse_composite, r2_composite = r2_composite, coefficients = coefficient_values)

```
### Predicted vs Actual Composite Scores Visualization:
```{r}
ggplot() +
  geom_point(aes(x = y_test_composite, y = predictions), color = 'red') +
  geom_line(aes(x = y_test_composite, y = y_test_composite), color = 'blue') +
  xlab("Actual Composite Score") + ylab("Predicted Composite Score") +
  ggtitle("Predicted vs Actual Composite Scores")

```
The LASSO regression model demonstrated a MSE of r mse_composite and an R² value of r r2_composite, suggesting a moderate level of predictive accuracy. The model is capable of explaining approximately 42.4% of the variance in the composite score from the standardized variables of wages, grades, and experience in 1976.

### Insights from Model Coefficients

The model's coefficients provide insights into the predictors' relative importance. Notable observations include:

Negative coefficients for certain predictors, indicating an inverse relationship with the composite score.
A significant positive coefficient for the 14th predictor, suggesting a strong positive influence on the composite score.
These insights highlight the model's capability to identify the most significant predictors while effectively managing dimensionality and potential overfitting through regularization.


### Enhanced Cross-Validation in LASSO Regression Section:

K-fold cross-validation emerged as a pivotal method in validating the LASSO model's performance. By systematically rotating the validation set across different subsets of the data, this technique provides a comprehensive assessment of the model's predictive capability. The iterative process of training and validating across K folds not only identifies the most effective lambda value but also underscores the model's reliability and robustness in various scenarios.


### Composite Score: A Holistic Approach to Modeling

Introducing a composite score as the response variable stands as a hallmark of this study's innovative approach. This decision underscores an intent to capture the collective impact of critical factors such as education, experience, and wages, rather than evaluating them in isolation. Such a multifaceted approach enables a more holistic understanding of these variables' interrelations and their cumulative effect on outcomes.


### Coefficients and Their Implications

The model's coefficients reveal the direction and magnitude of each predictor's influence on the composite score. Notably, the most substantial positive impact was observed for the coefficient corresponding to the 14th predictor, with a value of 0.280500409, suggesting a significant positive relationship with the composite outcome. Conversely, the third predictor exhibited a notably negative relationship, with a coefficient of -0.088595610. These findings highlight the diverse effects of individual predictors, underscoring the complexity of the underlying relationships.

### R Code for Analysis and Visualization

```{r}
# Load necessary libraries
library(dplyr)
library(caret)
library(readr)
library(glmnet)
library(ggplot2)

# Reading and preparing the data
data <- read_csv("RetSchool.csv")
data_standardized <- data %>%
  mutate_at(vars(wage76, grade76, exp76), ~scale(.) %>% as.vector()) %>%
  mutate(composite_score = rowMeans(select(., wage76, grade76, exp76), na.rm = TRUE))

X_composite <- select(data_standardized, -c(wage76, grade76, exp76, composite_score))
y_composite <- data_standardized$composite_score
y_composite[is.na(y_composite)] <- mean(y_composite, na.rm = TRUE)

# Data partitioning
set.seed(0)
trainIndex <- createDataPartition(y_composite, p = 0.8, list = FALSE)
X_train_composite <- X_composite[trainIndex, ]
X_test_composite <- X_composite[-trainIndex, ]
y_train_composite <- y_composite[trainIndex]
y_test_composite <- y_composite[-trainIndex]

# Preprocessing: Imputation and scaling
preProcessImpute <- preProcess(X_train_composite, method = c("medianImpute"))
X_train_preprocessed_composite <- predict(preProcessImpute, X_train_composite)
X_test_preprocessed_composite <- predict(preProcessImpute, X_test_composite)
preProcessScale <- preProcess(X_train_preprocessed_composite, method = c("center", "scale"))
X_train_preprocessed_composite <- predict(preProcessScale, X_train_preprocessed_composite)
X_test_preprocessed_composite <- predict(preProcessScale, X_test_preprocessed_composite)

# LASSO model training with cross-validation
lambda_seq <- 10^seq(2, -2, length.out = 100)
cv_lasso <- cv.glmnet(as.matrix(X_train_preprocessed_composite), y_train_composite, alpha = 1, lambda = lambda_seq)
best_lambda <- cv_lasso$lambda.min

# Model fitting with the optimal lambda
lasso_best <- glmnet(as.matrix(X_train_preprocessed_composite), y_train_composite, alpha = 1, lambda = best_lambda)

# Model prediction and evaluation
predictions <- predict(lasso_best, newx = as.matrix(X_test_preprocessed_composite), s = best_lambda)
mse_composite <- mean((y_test_composite - predictions)^2)
r2_composite <- summary(lm(predictions ~ y_test_composite))$r.squared
coefficients <- coef(lasso_best, s = best_lambda)
coefficient_values <- as.vector(coefficients[-1, ])

# Visualizing cross-validation results
plot(cv_lasso$lambda, cv_lasso$cvm, log = "x", type = 'l', xlab = "Lambda", ylab = "Mean Squared Error",
     main = "Cross-Validation Results for LASSO Regression")
abline(v = cv_lasso$lambda.min, col = "red", lwd = 2, lty = 2)
legend("topright", legend = c(paste("Optimal lambda:", round(cv_lasso$lambda.min, 5))), col = "red", lty = 2, lwd = 2)

```

### Model Performance and Insights from Cross-Validation
The LASSO regression model, tuned via rigorous K-fold cross-validation, achieved a Mean Squared Error (MSE) of 0.1575524 and an R² value of 0.4173462. These metrics reflect the model's moderate effectiveness in predicting the composite outcome, encapsulating the nuanced interplay among education, experience, and wages.

The selection of the optimal regularization parameter, λ (lambda), was critical in this context. The best lambda value identified was 0.0007330395, indicating a balance between minimizing model complexity and maximizing predictive accuracy. This fine-tuning process is vital for mitigating overfitting and ensuring the model's generalization across unseen data.

### WEEK 11

### 3) Subgroup Analysis: Assessing Variability Across Demographics

Exploring subgroup analysis and incorporating interaction terms offer avenues to assess variability across demographics and unveil complex relationships that a singular model approach might overlook.

This study also embarked on a subgroup analysis, focusing on the impact of racial background on outcomes. This involved conducting separate LASSO regressions for individuals identified as black and non-black, aiming to uncover distinct patterns and the model's predictive accuracy within these demographic cohorts.

```{r}
# Calculate 'composite_score'
data$composite_score <- rowMeans(data[, c('wage76', 'grade76', 'exp76')], na.rm = TRUE)

# Impute missing 'composite_score' values with the mean of the existing values
data$composite_score[is.na(data$composite_score)] <- mean(data$composite_score, na.rm = TRUE)

# Now, proceed with splitting the dataset into subgroups
subgroup_black <- data[data$black == 1, ]
subgroup_non_black <- data[data$black == 0, ]
```

```{r}
# Define a function to perform LASSO analysis
perform_lasso_analysis <- function(subgroup) {
  # Assuming 'subgroup' dataframe only contains numeric predictors after removing specified columns
  # Impute missing values in predictors if any (simple mean imputation)
  imputed_subgroup <- subgroup
  for(col_name in names(imputed_subgroup)) {
    if(col_name != "composite_score" && col_name != "black") { # Exclude non-predictor columns
      imputed_subgroup[[col_name]][is.na(imputed_subgroup[[col_name]])] <- mean(imputed_subgroup[[col_name]], na.rm = TRUE)
    }
  }
  
  # Prepare the dataset for LASSO regression
  X <- select(imputed_subgroup, -c(wage76, grade76, exp76, rownames, composite_score, black))
  y <- imputed_subgroup$composite_score
  if(any(is.na(y))) {
    stop("y contains missing values after imputation.")
  }
  
  # Splitting the data
  set.seed(0)
  trainIndex <- createDataPartition(y, p = 0.8, list = TRUE)[[1]]
  X_train <- X[trainIndex, ]
  X_test <- X[-trainIndex, ]
  y_train <- y[trainIndex]
  y_test <- y[-trainIndex]
  
  # LASSO regression with cross-validation
  set.seed(0)
  cv_lasso <- cv.glmnet(as.matrix(X_train), y_train, alpha = 1)
  best_lambda <- cv_lasso$lambda.min
  
  lasso_best <- glmnet(as.matrix(X_train), y_train, alpha = 1, lambda = best_lambda)
  
  # Prediction and evaluation
  predictions <- predict(lasso_best, newx = as.matrix(X_test), s = best_lambda)
  mse <- mean((y_test - predictions)^2)
  r2 <- cor(y_test, predictions)^2
  
  return(list(mse = mse, r2 = r2))
}

results_black <- perform_lasso_analysis(subgroup_black)
results_non_black <- perform_lasso_analysis(subgroup_non_black)

# Assuming results_black and results_non_black are correctly populated
results <- list(
  black = results_black,
  non_black = results_non_black
)

print(results)
```
###Results and Interpretation from the analysis of Subgroup Analysis (Assessing Variability Across Demographics)

Subgroup 1 (Black): The analysis for the black subgroup yielded an MSE of 1.30435 and an R² of 0.2659, indicating a moderate level of predictive accuracy. This suggests that the model explains approximately 26.59% of the variance in the composite score for this subgroup, highlighting a reasonable fit with room for improvement.

Subgroup 2 (Non-Black): For the non-black subgroup, the model showed an improved fit with an MSE of 1.69913 and an R² of 0.4007. This result suggests that nearly 40.07% of the variance in the composite score is accounted for, indicating a better predictive accuracy compared to the black subgroup.

These findings from subgroup analysis illuminate the importance of considering demographic variables in predictive models. By examining the influence of racial background, the study identifies varying levels of model performance across different groups, providing valuable insights into the factors driving these differences.


### 4) Interaction Terms: Unveiling Complex Relationships

The inclusion of interaction terms in regression models is a strategic method to capture the dependent and non-linear relationships between variables. By creating new predictors that represent the product of pairs of original predictors, the model gains the ability to assess the combined effects of two variables on the response.

This approach is grounded in the seminal work of Aiken, West, and Reno (1991), who highlighted the significance of interaction terms in enhancing the explanatory power of statistical models.

```{r}
data_standardized <- data %>%
  mutate_at(vars(wage76, grade76, exp76), ~scale(.) %>% as.vector())

# Create a composite score (average of standardized scores)
# Ensure NA values are handled appropriately by using rowMeans with na.rm = TRUE
data_standardized <- data_standardized %>%
  mutate(composite_score = rowMeans(select(., wage76, grade76, exp76), na.rm = TRUE))

# Prepare the dataset for LASSO regression
# Drop 'rownames' if it exists as a column; adjust based on actual data structure
X_composite <- select(data_standardized, -c(wage76, grade76, exp76, composite_score))
y_composite <- data_standardized$composite_score
y_composite[is.na(y_composite)] <- mean(y_composite, na.rm = TRUE)  # Impute missing values in 'y_composite'

# Splitting the data
set.seed(0)
trainIndex <- createDataPartition(y_composite, p = 0.8, list = FALSE)
X_train_composite <- X_composite[trainIndex, ]
X_test_composite <- X_composite[-trainIndex, ]
y_train_composite <- y_composite[trainIndex]
y_test_composite <- y_composite[-trainIndex]


X_train_df <- as.data.frame(X_train_preprocessed_composite)
X_test_df <- as.data.frame(X_test_preprocessed_composite)

# Create interaction terms for training data
# This automatically includes original terms and their interactions, similar to PolynomialFeatures with degree=2 and interaction_only=True in Python
X_train_interact <- model.matrix(~ .^2, data = X_train_df, response = y_train_composite)[,-1] # Removing intercept

# For LASSO, we need a matrix format
X_train_interact <- as.matrix(X_train_interact)

# Similar steps for test data
X_test_interact <- model.matrix(~ .^2, data = X_test_df, response = y_test_composite)[,-1] # Removing intercept
X_test_interact <- as.matrix(X_test_interact)
```

```{r}
set.seed(0) # For reproducibility
# Fit LASSO model with interaction terms
cv_lasso_interact <- cv.glmnet(X_train_interact, y_train_composite, alpha = 1, type.measure = "mse")

# Best lambda from cross-validation
best_lambda_interact <- cv_lasso_interact$lambda.min

# Fit the model using the best lambda
lasso_best_interact <- glmnet(X_train_interact, y_train_composite, alpha = 1, lambda = best_lambda_interact)


# Predict on the test set
predictions_interact <- predict(lasso_best_interact, newx = X_test_interact, s = best_lambda_interact)

# Calculate MSE and R2
mse_interact <- mean((y_test_composite - predictions_interact)^2)
r2_interact <- cor(y_test_composite, predictions_interact)^2 # Squared correlation as an R2 approximation

# Extracting non-zero coefficients
coef_interact <- as.numeric(coef(lasso_best_interact, s = best_lambda_interact)[-1]) # Excluding intercept

# Output the MSE, R^2, and coefficients
list(mse_interact = mse_interact, r2_interact = r2_interact, coefficients = coef_interact)


```
### Model Performance Insights from analysis of Interaction Terms(Unveiling Complex Relationships)

The investigation into interaction terms revealed their substantial influence on the composite score, which integrates the effects of education, experience, and wages. The analysis demonstrated that certain interactions notably enhanced the model's explanatory capacity, as evidenced by an improved R² of 0.5461776. This increase in R² underscores the value of considering interaction terms in predictive modeling, offering deeper insights into the complex interplay between predictors.

### Transitioning to Next Methodologies
Building on the foundational insights garnered from LASSO regression and its cross-validation, we proceed to explore additional methodologies. This progression is motivated by the intent to delve deeper into the data's structure, uncovering more complex relationships and enhancing our model's predictive accuracy.

### Handling Missing Data Differently
Given the notable impact of missing data on model performance, exploring alternative strategies for handling missing data can provide a more robust foundation for our analysis.

### Logistic Regression with LASSO for Categorical Outcomes
To complement our continuous outcome analysis with LASSO, employing logistic regression with LASSO for categorical outcome prediction will broaden our analytical scope, offering insights into different aspects of the data.



### Conclusion

## References

1.Chunli Zhoua, X. L. (2022). "Influencing factors of the high-quality economic development in China based on LASSO model." Energy Reports, 8, 1055–1065.

2.Yiming Lu, Y. Z. (2011). "A Lasso regression model for the construction of microRNA-target regulatory networks." Bioinformatics, pages 2406–2413.

3.Shengzheng Wang, B. J. (2017). "Predicting ship fuel consumption based on LASSO regression." Marine Pollution Bulletin, Elsevier, 1361-9209.

4.Archana J. McEligot, V. P. (2020). "Logistic LASSO Regression for Dietary Intakes and Breast Cancer." Nutrients, 12, 2652.

5.Muthukrishnan R, R. R. (2016). "LASSO: A Feature Selection Technique In Predictive Modeling For Machine Learning." IEEE International Conference on Advances in Computer Applications (ICACA), 978-1-5090-3770-4.

6.Efron, B., Hastie, T., Johnstone, I., & Tibshirani, R. (2004). "Least angle regression." Annals of Statistics, 32(2), 407-499.

7.Tibshirani, R. (1996). "Regression shrinkage and selection via the lasso." Journal of the Royal Statistical Society: Series B (Methodological), 58(1), 267-288.

8.Musoro, J.Z., et al. (2014). "Validation of prediction models based on lasso regression with multiply imputed data." BMC Medical Research Methodology.

9.Chintalapudi, N., et al. (2022). "LASSO Regression Modeling on Prediction of Medical Terms among Seafarers’ Health Documents Using Tidy Text Mining." Bioengineering.

10.Li, Y., et al. (2022). "Applying logistic LASSO regression for the diagnosis of atypical Crohn's disease." Scientific Reports.

11.Hastie, T., Tibshirani, R., Friedman, J. (2009). "The Elements of Statistical Learning: Data Mining, Inference, and Prediction." This book provides foundational knowledge on statistical learning techniques, including LASSO.

12.Simon, N., et al. (2013). "Regularization Paths for Cox’s Proportional Hazards Model via Coordinate Descent." Journal of Statistical Software. This paper discusses extensions of LASSO for survival analysis.

13.Friedman, J., Hastie, T., Tibshirani, R. (2010). "Regularization Paths for Generalized Linear Models via Coordinate Descent." Journal of Statistical Software. This work extends LASSO to a broader class of models.

14.Zou, H., Hastie, T. (2005). "Regularization and variable selection via the elastic net." Journal of the Royal Statistical Society: Series B. This paper introduces the Elastic Net, a significant extension of LASSO.

15.Meinshausen, N., Bühlmann, P. (2006). "High-dimensional graphs and variable selection with the Lasso." Annals of Statistics. This study explores the use of LASSO for variable selection in high-dimensional graphs.

16.Tibshirani, R., et al. (2012). "Strong rules for discarding predictors in lasso-type problems." Journal of the Royal Statistical Society: Series B. This paper discusses computational techniques to enhance LASSO's efficiency.

17.Park, M.Y., Hastie, T. (2008). "Penalized logistic regression for detecting gene interactions." Biostatistics. This study applies LASSO in a genetic context to detect interactions.

18.Belloni, A., Chernozhukov, V. (2013). "Least squares after model selection in high-dimensional sparse models." Bernoulli. This paper addresses post-model selection inference, relevant for LASSO applications.

19.Witten, Daniela M., and Robert Tibshirani. "A Framework for Feature Selection in Clustering." Journal of the American Statistical Association, vol. 105, no. 490, 2010, pp. 713-726.

20.Sauerbrei, Willi, et al. "On Stability Issues in Deriving Multivariable Regression Models." Biometrical Journal, vol. 57, no. 4, 2015, pp. 531-555.

21.Zhao, Peng, and Bin Yu. "On Model Selection Consistency of Lasso." Journal of Machine Learning Research, vol. 7, Nov. 2006, pp. 2541-2563.

22.Fan, Jianqing, and Jinchi Lv. "Nonconcave Penalized Likelihood with NP-Dimensionality." IEEE Transactions on Information Theory, vol. 57, no. 8, 2011, pp. 5467-5484.

23.Hastie, Trevor, et al. "Matrix Completion and Low-Rank SVD via Fast Alternating Least Squares." Journal of Machine Learning Research, vol. 16, 2015, pp. 3367-3402.

24.Kasper, G., Momen, M., Sorice, K.A. et al. Effect of neighbourhood and individual-level socioeconomic factors on breast cancer screening adherence in a multi-ethnic study. BMC Public Health 24, 63 (2024). 

25.Sohaee, N.; Bohluli, S. Nonlinear Analysis of the Effects of Socioeconomic, Demographic, and Technological Factors on the Number of Fatal Traffic Accidents. Safety 2024, 10, 11. https://doi.org/10.3390/safety10010011 

26.Bradley 	Efron. Trevor 	Hastie. Iain 	Johnstone. Robert Tibshirani. "Least angle regression." Ann. Statist. 32 (2) 407 - 499, April 2004. https://doi.org/10.1214/009053604000000067 

27.A. Jafar and M. Lee, "HypGB: High Accuracy GB Classifier for Predicting Heart Disease with HyperOpt HPO Framework and LASSO FS Method," in IEEE Access, vol. 11, pp. 138201-138214, 2023, doi: 10.1109/ACCESS.2023.3339225. 
28.RetSchool Dataset: https://vincentarelbundock.github.io/Rdatasets/doc/Ecdat/RetSchool.html,
https://vincentarelbundock.github.io/Rdatasets/articles/data.html

29.Little, R.J.A., & Rubin, D.B. (2002). Statistical Analysis with Missing Data. John Wiley & Sons, Inc. "This seminal text provides foundational insights into the techniques and implications of handling missing data in statistical analysis, informing our methodology's approach to imputation and data completeness".

30.Tibshirani, R. (1996). "Regression Shrinkage and Selection via the Lasso." Journal of the Royal Statistical Society, Series B, 58(1), 267-288. Tibshirani's groundbreaking work on LASSO regression underpins our strategy for model selection and complexity reduction, ensuring precise and interpretable predictive modeling.

31.Aiken, L. S., West, S. G., & Reno, R. R. (1991). Multiple regression: Testing and interpreting interactions. Newbury Park, CA: Sage.

32.James, G., Witten, D., Hastie, T., & Tibshirani, R. (2013). An Introduction to Statistical Learning: with Applications in R. Springer.

33.Hancock, Gregory R., and Ralph O. Mueller. "Rethinking Construct Reliability Within Latent Variable Systems." In Structural Equation Modeling: A Second Course, edited by Gregory R. Hancock and Ralph O. Mueller, 2nd ed., 303-324. Charlotte, NC: Information Age Publishing,2013.

34.James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. An Introduction to Statistical Learning: with Applications in R. New York:Springer,2013.

35.Sun, Xiaowei, Michael Borenstein, and Julian P.T. Higgins. "Are Subgroup Analyses Necessary? Examining the Controversy." Journal of Clinical Epidemiology 128 (2020):142-147.

36.Aiken, Leona S., Stephen G. West, and Raymond R. Reno. Multiple Regression: Testing and Interpreting Interactions. Newbury Park,CA:Sage,1991.

37.Little, Roderick J. A., and Donald B. Rubin. Statistical Analysis with Missing Data. Wiley, 2002.

38. Hastie, Trevor, et al. The Elements of Statistical Learning: Data Mining, Inference, and Prediction. 2nd ed.,Springer,2009.

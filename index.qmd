---
title: "Lasso Regression"
author: "Nikhitha Amireddy, Ishrath Jahan, Sai Kumar Miryala, Muhammad Usman Aslam"
date: '`r Sys.Date()`'
format:
  html:
    code-fold: true
course: STA 6257 - Advanced Statistical Modeling
bibliography: references.bib # file contains bibtex for references
#always_allow_html: true # this allows to get PDF with HTML features
self-contained: true
execute: 
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
---

## Introduction

In 1996, Robert Tibshirani introduced LASSO regression to further the discussion about variable selection [@tibshirani1996]. This provided a new and sophisticated solution for the problems in the course of predictive modeling, especially when it came to big and complex datasets. It is therefore important that the model remains parsimonious in nature, explains easily interpretable phenomena, and reduces the risks associated with overfitting by applying a penalty to the absolute values of regression coefficients [@zhao2006; @fan2011]. This innovation was therefore taken up by many scientific disciplines, characterizing the beginning of an era where high-dimensional datasets could be probed with more depth for complexities with better clarity [@meinshausen2006; @hastie2015]. The popularity of LASSO regression has risen with the birth of Least Angle Regression by Efron et al. in 2004 [@efron2004]. LASSO regression has particularly become a key instrument with which to insight the complex dynamics underlying socioeconomic trends, public health outcomes, and environmental studies—hence, significantly enriching the toolset available to the data scientist [@belloni2013; @park2008]. Logistic regression is further extended by integration with the capabilities of LASSO, offering a refined method in prediction for the categorical outcomes [@li2022]. This synergy provides a response to long-term barriers to predictive modeling such as overfitting and multicollinearity with a scope of applications in, in particular, economic forecasting and health diagnostics, amongst many more [@sauerbrei2015].
The 21st century continues, and the data revolution heightens. A dataset in relation to the augmented domain complexity continues to pervade. Here, LASSO regression does not exist just as a statistical tool but as an innovation beacon assisting researchers with precision and simplicity within the dynamic landscape [@chintalapudi2022]. The introduction of logistic regression into the area of LASSO has been especially dynamic, providing a nuanced approach to modeling of categorical outcomes [@simon2013]. This is necessary for areas from epidemiology to finance [@friedman2010]. This thus places this combination as a great leap forward in the direction of data-driven insights aimed at action, where traditional analytical methods might possibly face challenges of interpretation, firmly situating logistic regression with LASSO among the basic constitutions of modern statistical analysis and predictive modeling [@hastie2009]. Indeed, the way of LASSO development has been marked by a series of important developments that pinpoint the historical trajectory, including the emergence of Least Angle Regression by Efron and colleagues in pointing out the potential, influence, and continued utility in research [@zou2005; @witten2010]. This cutting-edge technology is used throughout a wide spectrum of industries with the goal of dismantling complex data structures and, eventually, extracting meaningful patterns from trends as socioeconomic, public health, and environmental studies. LASSO regression adapted for logistic models was one of the best steps to ward off the impediments towards modeling categorical data with more precision [@tibshirani2012].




## Literature Review

The versatility of LASSO regression is reflected in the extensive literature discussing its various applications. Notably, LASSO regression has made significant contributions to economics. For example, Zhou et al. (2022) used the LASSO model to investigate what determines the growth of quality of life in China and found that the LASSO model is a good tool for selecting important economic predictors out of a large number of regressors [@zhou2022]. Thus, it makes important improvements on economic models useful for economic policy stakeholders and researchers in how to promote economic growth. The technique has also been critical for important developments in the area of bioinformatics. Lu, et al. (2011) showed how LASSO regression can be applied to constructing predictive models — based on gene expressions and other high-throughput data — to understand the relationships between gene expressions and multiple phenotypes, which are critical for understanding genetic pathways and cell systems [@lu2011]. In this example, the LASSO regression was used to build a microRNA-target regulatory network, which provides insight into gene regulation mechanisms, as well as applications in personalized medicine and in the understanding of genetic diseases [@musoro2014].
LASSO regression has expedited the processing and analysis of large datasets in the environmental and energy sectors. Wang et al. (2017) detailed its use in predicting ship fuel consumption for diminished environmental impact and increased fuel efficiency in maritime operations [@wang2017]. LASSO regression aids in selecting the most informative variables that influence fuel usage and empowers the maritime industry's sustainability and operational performance efforts. LASSO regression has been a meaningful addition to the analysis of multi-factorial diseases and health outcomes in public health research. In an analysis of dietary intakes for breast cancer, McEligot et al. (2020) showed that logistic LASSO can handle high-dimensional data to uncover the key drivers of health, and this has broad applicability in public health for unraveling and prioritizing the risk factors for a variety of illnesses [@mcEligot2020]. His role as a facilitator for the development of machine learning models (especially in feature selection) has been pivotal to improving the accuracy of those predictions as LASSO was proving its worthiness time and time again. Muthukrishnan and Rohini (2016) showcased this by demonstrating the ability of LASSO regression “to select the relevant feature attributes of predictive modelling for machine learning” [@muthukrishnan2016].
After gaining prominence within the traditional domain, the applications of LASSO regression have extended into new spheres such as financial modeling, where it assists analysts in singling out influential market predictors from large datasets characterized by volatility and noise, eventually translating into forecasts and investment strategies [@friedman2010; @hastie2015]. The adaptability and precision of LASSO regression within financial analysis underline its growing promise and popularity. Another field where LASSO regression's ability to parse through voluminous climate variable datasets to extract critical indicators of environmental change has been crucial is climate research and sustainability studies. The power of this application is not only a testament to the adaptability but also reflects the urgency in deploying LASSO regression in the face of challenges such as global warming and biodiversity loss, where parsing through complex environmental patterns becomes critical to devising strategies to mitigate its devastating impacts [@chintalapudi2022; @li2022]. Digital health technologies have opened a trove of opportunities for LASSO regression to analyze the complex interplay between individual lifestyle factors, genetic predispositions, and health outcomes. The inclusion of LASSO in this field of research is an acknowledgement that it is uniquely suited to handle complex, multidimensional data that is needed to understand and generate personalized medicine or public health interventions—a frontier of healthcare that holds promise for applications where LASSO's valuable data-driven insights can be channeled to address bespoke health solutions [@simon2013; @park2008].
Indeed, these and many more diverse applications are a testament to the potency of LASSO regression as a powerful tool in the arsenal of researchers and practitioners across disciplines, as they grapple with increasingly high-resolution and large datasets. By simplifying complex models and identifying important predictors from large datasets, LASSO regression has cemented itself as an essential part of modern statistical analysis and predictive modeling [@belloni2013; @zou2005; @witten2010; @tibshirani2012].



## Methods

The common non-parametric regression model is
$Y_i = m(X_i) + \varepsilon_i$, where $Y_i$ can be defined as the sum of
the regression function value $m(x)$ for $X_i$. Here $m(x)$ is unknown
and $\varepsilon_i$ some errors. With the help of this definition, we
can create the estimation for local averaging i.e. $m(x)$ can be
estimated with the product of $Y_i$ average and $X_i$ is near to $x$. In
other words, this means that we are discovering the line through the
data points with the help of surrounding data points. The estimation
formula is printed below [@R-base]:

$$
M_n(x) = \sum_{i=1}^{n} W_n (X_i) Y_i  \tag{1}
$$ $W_n(x)$ is the sum of weights that belongs to all real numbers.
Weights are positive numbers and small if $X_i$ is far from $x$.

Another equation:

$$
y_i = \beta_0 + \beta_1 X_1 +\varepsilon_i
$$

## Analysis and Results

### Data and Visualization

A study was conducted to determine how...

```{r, warning=FALSE, echo=T, message=FALSE}
# loading packages 
library(tidyverse)
library(knitr)
library(ggthemes)
library(ggrepel)
library(dslabs)
```

```{r, warning=FALSE, echo=TRUE}
# Load Data
kable(head(murders))

ggplot1 = murders %>% ggplot(mapping = aes(x=population/10^6, y=total)) 

  ggplot1 + geom_point(aes(col=region), size = 4) +
  geom_text_repel(aes(label=abb)) +
  scale_x_log10() +
  scale_y_log10() +
  geom_smooth(formula = "y~x", method=lm,se = F)+
  xlab("Populations in millions (log10 scale)") + 
  ylab("Total number of murders (log10 scale)") +
  ggtitle("US Gun Murders in 2010") +
  scale_color_discrete(name = "Region")+
      theme_bw()
  

```

### Statistical Modeling

```{r}

```

### Conclusion

## References

1.	1.	Chunli Zhoua, X. L. (2022). "Influencing factors of the high-quality economic development in China based on LASSO model." Energy Reports, 8, 1055–1065.
2.	Yiming Lu, Y. Z. (2011). "A Lasso regression model for the construction of microRNA-target regulatory networks." Bioinformatics, pages 2406–2413.
3.	Shengzheng Wang, B. J. (2017). "Predicting ship fuel consumption based on LASSO regression." Marine Pollution Bulletin, Elsevier, 1361-9209.
4.	Archana J. McEligot, V. P. (2020). "Logistic LASSO Regression for Dietary Intakes and Breast Cancer." Nutrients, 12, 2652.
5.	Muthukrishnan R, R. R. (2016). "LASSO: A Feature Selection Technique In Predictive Modeling For Machine Learning." IEEE International Conference on Advances in Computer Applications (ICACA), 978-1-5090-3770-4.
6.	Efron, B., Hastie, T., Johnstone, I., & Tibshirani, R. (2004). "Least angle regression." Annals of Statistics, 32(2), 407-499.
7.	Tibshirani, R. (1996). "Regression shrinkage and selection via the lasso." Journal of the Royal Statistical Society: Series B (Methodological), 58(1), 267-288.
8.	Musoro, J.Z., et al. (2014). "Validation of prediction models based on lasso regression with multiply imputed data." BMC Medical Research Methodology.
9.	Chintalapudi, N., et al. (2022). "LASSO Regression Modeling on Prediction of Medical Terms among Seafarers’ Health Documents Using Tidy Text Mining." Bioengineering.
10.	Li, Y., et al. (2022). "Applying logistic LASSO regression for the diagnosis of atypical Crohn's disease." Scientific Reports.
11.	Hastie, T., Tibshirani, R., Friedman, J. (2009). "The Elements of Statistical Learning: Data Mining, Inference, and Prediction." This book provides foundational knowledge on statistical learning techniques, including LASSO.
12.	Simon, N., et al. (2013). "Regularization Paths for Cox’s Proportional Hazards Model via Coordinate Descent." Journal of Statistical Software. This paper discusses extensions of LASSO for survival analysis.
13.	Friedman, J., Hastie, T., Tibshirani, R. (2010). "Regularization Paths for Generalized Linear Models via Coordinate Descent." Journal of Statistical Software. This work extends LASSO to a broader class of models.
14.	Zou, H., Hastie, T. (2005). "Regularization and variable selection via the elastic net." Journal of the Royal Statistical Society: Series B. This paper introduces the Elastic Net, a significant extension of LASSO.
15.	Meinshausen, N., Bühlmann, P. (2006). "High-dimensional graphs and variable selection with the Lasso." Annals of Statistics. This study explores the use of LASSO for variable selection in high-dimensional graphs.
16.	Tibshirani, R., et al. (2012). "Strong rules for discarding predictors in lasso-type problems." Journal of the Royal Statistical Society: Series B. This paper discusses computational techniques to enhance LASSO's efficiency.
17.	Park, M.Y., Hastie, T. (2008). "Penalized logistic regression for detecting gene interactions." Biostatistics. This study applies LASSO in a genetic context to detect interactions.
18.	Belloni, A., Chernozhukov, V. (2013). "Least squares after model selection in high-dimensional sparse models." Bernoulli. This paper addresses post-model selection inference, relevant for LASSO applications.
19.	Witten, Daniela M., and Robert Tibshirani. "A Framework for Feature Selection in Clustering." Journal of the American Statistical Association, vol. 105, no. 490, 2010, pp. 713-726.
20.	Sauerbrei, Willi, et al. "On Stability Issues in Deriving Multivariable Regression Models." Biometrical Journal, vol. 57, no. 4, 2015, pp. 531-555.
21.	Zhao, Peng, and Bin Yu. "On Model Selection Consistency of Lasso." Journal of Machine Learning Research, vol. 7, Nov. 2006, pp. 2541-2563.
22.	Fan, Jianqing, and Jinchi Lv. "Nonconcave Penalized Likelihood with NP-Dimensionality." IEEE Transactions on Information Theory, vol. 57, no. 8, 2011, pp. 5467-5484.
23.	Hastie, Trevor, et al. "Matrix Completion and Low-Rank SVD via Fast Alternating Least Squares." Journal of Machine Learning Research, vol. 16, 2015, pp. 3367-3402.


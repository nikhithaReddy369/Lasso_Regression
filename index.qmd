---
title: "Innovative Data Exploration with LASSO: Unveiling Patterns in Earnings and Education"
author: "Nikhitha Amireddy, Ishrath Jahan, Sai Kumar Miryala, Muhammad Usman Aslam"
date: '`r Sys.Date()`'
format:
  html:
    code-fold: true
course: STA 6257 - Advanced Statistical Modeling
bibliography: references.bib # file contains bibtex for references
always_allow_html: true # this allows to get PDF with HTML features
self-contained: true
execute: 
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
---
## Introduction

LASSO, Least Absolute Shrinkage and Selection Operator, is a penalized linear regression method that reduces the number of variables in the model, useful for avoiding overfitting. It works by minimizing the residual sum of squares subject to the sum of the absolute value of the coefficients being less than a constant, simplifying the model and potentially improving prediction accuracy.

In 1996, Robert Tibshirani [@Tibshirani1996] introduced LASSO regression to further the discussion about variable selection. This provided a new and sophisticated solution for the problems in the course of predictive modeling, especially when it came to big and complex datasets. It is therefore important that the model remains parsimonious in nature, explains easily interpretable phenomena, and reduces the risks associated with overfitting by applying a penalty to the absolute values of regression coefficients [@Zhao2006], [@Fan2011]. This innovation was therefore taken up by many scientific disciplines, characterizing the beginning of an era where high-dimensional datasets could be probed with more depth for complexities with better clarity [@Meinshausen2006] @Hastie2009\]. The popularity of LASSO regression has risen with the birth of Least Angle Regression by Efron et al. [@Efron2004] in 2004.

LASSO regression has particularly become a key instrument with which to insight the complex dynamics underlying socioeconomic trends, public health outcomes, and environmental studies—hence, significantly enriching the toolset available to the data scientist [@Belloni2013], [@Park2008]. Logistic regression is further extended by integration with the capabilities of LASSO, offering a refined method in prediction for the categorical outcomes [@Li2022]. This synergy provides a response to long-term barriers to predictive modeling such as overfitting and multicollinearity with a scope of applications in, in particular, economic forecasting and health diagnostics, amongst many more [@Sauerbrei2015].

The 21st century continues, and the data revolution heightens. A dataset in relation to the augmented domain complexity continues to pervade. Here, LASSO regression does not exist just as a statistical tool but as an innovation beacon assisting researchers with precision and simplicity within the dynamic landscape [@Chintalapudi2022]. The introduction of logistic regression into the area of LASSO has been especially dynamic, providing a nuanced approach to modeling of categorical outcomes [@Simon2013]. This is necessary for areas from epidemiology to finance [@Friedman2010]. This thus places this combination as a great leap forward in the direction of data-driven insights aimed at action, where traditional analytical methods might possibly face challenges of interpretation, firmly situating logistic regression with LASSO among the basic constitutions of modern statistical analysis and predictive modeling [@Hastie2009].

Indeed, the way of LASSO development has been marked by a series of important developments that pinpoint the historical trajectory, including the emergence of Least Angle Regression by Efron and colleagues in pointing out the potential, influence, and continued utility in research [@Zhou2022]. This cutting-edge technology is used throughout a wide spectrum of industries with the goal of dismantling complex data structures and, eventually, extracting meaningful patterns from trends as socioeconomic, public health, and environmental studies. LASSO regression adapted for logistic models was one of the best steps to ward off the impediments towards modeling categorical data with more precision [@Tibshirani2012].

## Literature Review

The versatility of LASSO regression is reflected in the extensive literature discussing its various applications. Notably, LASSO regression has made significant contributions to economics. For example, Zhou et al. (2022)[@Zhou2022] used the LASSO model to investigate what determines the growth of quality of life in China and found that the LASSO model is a good tool for selecting important economic predictors out of a large number of regressors. Thus, it makes important improvements on economic models useful for economic policy stakeholders and researchers in how to promote economic growth. The technique has also been critical for important developments in the area of bioinformatics.

Lu, et al. (2011) [@Lu2011] showed how LASSO regression can be applied to constructing predictive models — based on gene expressions and other high-throughput data — to understand the relationships between gene expressions and multiple phenotypes, which are critical for understanding genetic pathways and cell systems. In this example, the LASSO regression was used to build a microRNA-target regulatory network, which provides insight into gene regulation mechanisms, as well as applications in personalized medicine and in the understanding of genetic diseases [@Musoro2014]. LASSO regression has expedited the processing and analysis of large datasets in the environmental and energy sectors. Wang et al. (2017) [@Wang2017] detailed its use in predicting ship fuel consumption for diminished environmental impact and increased fuel efficiency in maritime operations.

LASSO regression aids in selecting the most informative variables that influence fuel usage and empowers the maritime industry's sustainability and operational performance efforts. LASSO regression has been a meaningful addition to the analysis of multi-factorial diseases and health outcomes in public health research. In an analysis of dietary intakes for breast cancer, McEligot et al. (2020) [@McEligot2020] showed that logistic LASSO can handle high-dimensional data to uncover the key drivers of health, and this has broad applicability in public health for unraveling and prioritizing the risk factors for a variety of illnesses. His role as a facilitator for the development of machine learning models (especially in feature selection) has been pivotal to improving the accuracy of those predictions as LASSO was proving its worthiness time and time again.

Muthukrishnan and Rohini (2016) [@Muthukrishnan2016] showcased this by demonstrating the ability of LASSO regression “to select the relevant feature attributes of predictive modelling for machine learning”. After gaining prominence within the traditional domain, the applications of LASSO regression have extended into new spheres such as financial modeling, where it assists analysts in singling out influential market predictors from large datasets characterized by volatility and noise, eventually translating into forecasts and investment strategies [@Friedman2010]. The adaptability and precision of LASSO regression within financial analysis underline its growing promise and popularity. Another field where LASSO regression's ability to parse through voluminous climate variable datasets to extract critical indicators of environmental change has been crucial is climate research and sustainability studies. The power of this application is not only a testament to the adaptability but also reflects the urgency in deploying LASSO regression in the face of challenges such as global warming and biodiversity loss, where parsing through complex environmental patterns becomes critical to devising strategies to mitigate its devastating impacts [@Chintalapudi2022].

Digital health technologies have opened a trove of opportunities for LASSO regression to analyze the complex interplay between individual lifestyle factors, genetic predispositions, and health outcomes. The inclusion of LASSO in this field of research is an acknowledgement that it is uniquely suited to handle complex, multidimensional data that is needed to understand and generate personalized medicine or public health interventions—a frontier of healthcare that holds promise for applications where LASSO's valuable data-driven insights can be channeled to address bespoke health solutions [@Simon2013] and Sohaee and Bohluli (2024b)[@2024b] employ Lasso polynomial regression to analyze the intricate relationship between socioeconomic, demographic, and technological factors and fatal traffic accidents, demonstrating its effectiveness in handling categorical variables and preventing overfitting. In their study on breast cancer screening adherence (Sohaee and Bohluli 2024a)[@2024a], they utilized Lasso regression to identify significant predictors from various socioeconomic factors, highlighting health disparities and advocating for targeted healthcare policies. Both studies underscore the importance of advanced statistical techniques, such as Lasso regression, in understanding complex datasets to inform policy decisions and improve outcomes in transportation safety and healthcare.

Efron et al. (2004)[@annalsofstatistics] introduced Least Angle Regression (LARS) as adept for high-dimensional data analysis, emphasizing its suitability for large datasets with thousands of predictors. LARS enables efficient feature selection and model building, particularly when adapted for Lasso regression. By combining LARS with Lasso, researchers can select relevant features, manage high-dimensional data, and improve prediction accuracy while mitigating multicollinearity and overfitting concerns. [@10341221] research introduced a novel approach to predicting heart disease by leveraging a sophisticated computer model. By integrating LASSO, a technique akin to a smart sieve for crucial information, with other methods, the accuracy of predictions is enhanced. It showcases how advanced computer methods can assist doctors in foreseeing health issues preemptively, potentially leading to improved prevention and treatment strategies for heart disease. Researchers analyzed extensive health data, identifying key factors using LASSO, and developed a highly effective predictive model. This could translate to earlier and personalized care for individuals at risk of heart disease, promising significant advancements in healthcare.

Indeed, these and many more diverse applications are a testament to the potency of LASSO regression as a powerful tool in the arsenal of researchers and practitioners across disciplines, as they grapple with increasingly high-resolution and large datasets. By simplifying complex models and identifying important predictors from large datasets, LASSO regression has cemented itself as an essential part of modern statistical analysis and predictive modeling [@Belloni2013; @Zou2005; @Witten2010; @Tibshirani2012].

## Methodology

### LASSO Regression

LASSO (Least Absolute Shrinkage and Selection Operator) regression, introduced by Robert Tibshirani in 1996, extends traditional linear regression by introducing a penalty on the size of coefficients. This regularization technique is invaluable in scenarios with numerous potential predictors, some of which might not significantly impact the outcome variable. [@Tibshirani1996]

#### Mathematical Formulation

The objective function of LASSO regression is described as follows:

$$
\min_{\beta} \left\{ \frac{1}{2n} \sum_{i=1}^{n} (y_i - \beta_0 - \sum_{j=1}^{p} \beta_j x_{ij})^2 + \lambda \sum_{j=1}^{p} |\beta_j| \right\}
$$

This formulation cleverly balances the fidelity to the data, represented by the residual sum of squares (RSS), and model complexity, controlled by the LASSO penalty term. The regularization parameter, $\lambda$, is pivotal in this balance, dictating the extent of shrinkage applied to the coefficients [@Zhao2006; @Friedman2010].

##### The Shrinkage Effect and Its Merits

The essence of LASSO's power lies in its ability to perform coefficient shrinkage and feature selection simultaneously. This dual capability not only aids in model simplification but also in addressing multicollinearity and enhancing predictive performance. - **Feature Selection**: Critical in high-dimensional datasets, where irrelevant predictors can obscure true relationships [@Meinshausen2006]. - **Regularization**: Shields against overfitting, a common pitfall in model fitting, especially in the realm of complex datasets [@Hastie2009].

##### Choosing $\lambda$

The choice of $\lambda$ plays a pivotal role in the behavior of the LASSO model. It balances the trade-off between fitting the data well and keeping the model simple. Cross-validation is commonly used to select an optimal $\lambda$ that minimizes prediction error.

##### Role of the Regularization Parameter ($\lambda$)

The selection of $\lambda$ is critical in LASSO regression. When $\lambda = 0$, LASSO regression becomes ordinary least squares regression, with no penalty on coefficient size. As $\lambda$ increases, the penalty's impact grows, driving more coefficients to zero and simplifying the model. This attribute makes LASSO particularly effective for variable selection, as it can identify and eliminate irrelevant predictors by setting their coefficients to zero[@Zhao2006].

-   **Bias-Variance Trade-off**: A small $\lambda$ results in low bias but high variance, as the model fits closely to the training data, which may lead to overfitting. Conversely, a larger $\lambda$ increases bias but reduces variance by simplifying the model, potentially causing underfitting.
-   **Model Selection and Cross-Validation**: The optimal $\lambda$ value is usually determined through cross-validation. K-fold cross-validation is a common method to assess the model's predictive performance across a range of $\lambda$ values, selecting the $\lambda$ that minimizes cross-validation error [@Friedman2010].

##### Beyond Mathematical Formulation: LASSO in Practice

While LASSO's theoretical appeal is undeniable, its real-world applications underscore its practical significance. From enhancing predictive models in healthcare to enabling financial risk assessment, LASSO's utility spans across domains. Its ability to discern the signal from the noise makes it a go-to method in the toolkit of data scientists and researchers.

###### Advantages Over Alternative Methods

Compared to other regularization techniques like Ridge regression or the Elastic Net, LASSO stands out for its feature selection capability. Unlike Ridge, which only shrinks coefficients, LASSO can set them to zero, effectively choosing a simpler model. This aspect is particularly useful in scenarios where interpretability is as crucial as predictive accuracy [@Zou2005].

##### LASSO: Bridging Theory and Application

The discussion of LASSO would be incomplete without acknowledging its foundational role in modern data analysis. By offering a robust framework for dealing with high-dimensional data, LASSO facilitates the extraction of meaningful insights, ensuring that the models we build are not just statistical artifacts but true reflections of the underlying phenomena.

In essence, LASSO regression embodies the confluence of statistical elegance and practical applicability. Its continued relevance in statistical modeling and predictive analysis is a testament to its foundational importance in navigating the complexities of modern data landscapes.

##### Computational Considerations

LASSO regression's implementation computationally hinges on numerical optimization techniques, as the penalty term's absolute value renders the optimization problem non-differentiable at zero. Algorithms such as coordinate descent are commonly used to solve the LASSO optimization challenge, particularly with high-dimensional data.

##### Advantages of LASSO Regression

The amalgamation of feature selection and regularization underpins LASSO's utility for models prioritizing prediction accuracy and simplicity. Its applications span various fields, including biomedical research, financial modeling, and social sciences, attesting to its versatility and efficacy in predictive analytics [@Hastie2009; @Meinshausen2006].

-   **Feature Selection**: By reducing some coefficients to exactly zero, LASSO achieves automatic feature selection, highlighting significant predictors [@Park2008].
-   **Model Interpretability**: Simplifying the model by removing irrelevant variables improves interpretability, making LASSO-regressed models more comprehensible [@Belloni2013].
-   **Mitigation of Multicollinearity**: LASSO effectively addresses multicollinearity among predictors by selecting a single variable from a group of highly correlated variables, thereby alleviating this concern [@Efron2004].

## Exploring the Dataset: Key Variables

Before delving into data cleaning and modeling processes, it is essential to understand the key variables within the `RetSchool` dataset. These variables are pivotal for our analysis, offering insights into the socio-economic and educational landscape of 1976. This section outlines the significance of each variable, setting the stage for a nuanced exploration of factors influencing wages and educational outcomes.

**Economic and Demographic Indicators**

-   **`wage76`**: Represents the wages of individuals in 1976, providing a direct measure of economic status. Analysis of this variable will allow us to uncover wage disparities and explore their relationship with other socio-economic factors.

-   **`age76`**: Indicates the age of individuals in 1976, a fundamental demographic variable for analyzing the workforce's age distribution and its implications on wages.

**Educational Variables**

-   **`grade76`**: Denotes the highest grade completed by individuals as of 1976, offering insight into the level of educational attainment and its correlation with economic success.

-   **`col4`**: An indicator variable pertaining to college education. The precise meaning of this variable would require further clarification for meaningful analysis.

**Work Experience**

-   **`exp76`**: Measures the years of work experience by 1976, shedding light on the impact of labor market participation on wage levels.

**Family Background and Structure**

-   **`momdad14`, `sinmom14`**: Provide information on the living situation of individuals at age 14, essential for understanding the influence of early family environment on future educational and economic outcomes.

-   **`daded`, `momed`**: Reflect the education levels of the father and mother, respectively, key to exploring parental influence on the educational achievements and economic opportunities of their offspring.

**Racial and Regional Characteristics**

-   **`black`**: A binary variable identifying individuals as black, enabling the analysis of racial disparities within the dataset.

-   **`south76`**: Indicates whether individuals lived in the South in 1976, important for regional analyses and their socio-economic implications.

-   **`region`**: Categorizes individuals based on geographic regions, useful for understanding how regional factors influence economic and educational outcomes.

-   **`smsa76`**: Identifies residence within a Standard Metropolitan Statistical Area (SMSA) in 1976, relevant for examining urban vs. rural disparities.

This comprehensive overview of the dataset's key variables provides the groundwork for our in-depth analysis, aiming to illuminate the complex interplay between education, work experience, and socio-economic outcomes in 1976.

### Data Cleaning and Exploration

Initial exploration of the `RetSchool` dataset revealed a diverse range of variables, necessitating meticulous data cleaning to ensure accuracy in our subsequent analyses. Missing values were addressed through imputation or removal, refining our dataset for a comprehensive examination.

```{r setup, include=FALSE}
library(tidyverse)
library(ggplot2)
library(corrplot)
library(gridExtra)
df <- read_csv("https://vincentarelbundock.github.io/Rdatasets/csv/Ecdat/RetSchool.csv", show_col_types = FALSE)
df_clean <- df %>%
  mutate(
    grade76 = ifelse(is.na(grade76), median(grade76, na.rm = TRUE), grade76),
    exp76 = ifelse(is.na(exp76), median(exp76, na.rm = TRUE), exp76)
  ) %>%
  na.omit()
cat("Rows after cleaning and imputation:", nrow(df_clean), "\n")

```

Distribution of Key Variables Histograms and density plots provide a nuanced view of the distributions of wages, education, and experience.

```{r wage-distribution, fig.cap="Figure 1: Wage Distribution in 1976"}
library(ggplot2)
library(patchwork)

# Wage Distribution in 1976
p_wage <- ggplot(df_clean, aes(x = wage76)) +
  geom_histogram(aes(y = ..density..), bins = 30, fill = "skyblue", color = "black") +
  geom_density(color = "red", fill = "red", alpha = 0.2) +
  labs(title = "Wage Distribution in 1976", x = "Wage", y = "Density") +
  annotate("text", x = Inf, y = Inf, label = "Right-skewed distribution", hjust = 1.1, vjust = 2, size = 3, color = "darkred")
p_wage

```

**• Wage in 1976 (wage76):** 

The histogram for wage76 shows a right-skewed distribution, indicating that most individuals earned lower wages, with a few outliers earning significantly more. This pattern is typical for income data. 

The box plot reveals a wide range of wages with several outliers on the higher end. The median wage is relatively low compared to the mean, further indicating the skewness towards lower wage values.

```{r education-distribution, fig.cap="Figure 2: Education Level Distribution in 1976"}

# Education Level Distribution in 1976
p_education <- ggplot(df_clean, aes(x = grade76)) +
  geom_histogram(aes(y = ..density..), bins = 30, fill = "green", color = "black") +
  geom_density(color = "blue", fill = "blue", alpha = 0.2) +
  labs(title = "Education Level Distribution in 1976", x = "Highest Grade Completed", y = "Density") +
  annotate("text", x = Inf, y = Inf, label = "Multi-modal distribution", hjust = 1.1, vjust = 2, size = 3, color = "darkblue")
p_education
```

**• Education Level in 1976 (grade76):**

The distribution of grade76 shows a multi-modal pattern, suggesting concentrations of individuals at specific education levels. The peaks likely represent completion of certain education milestones (e.g., high school, some college). 

The box plot indicates a relatively normal distribution with a few outliers, suggesting most individuals had a similar level of education, with a median around high school completion.

```{r work-distribution, fig.cap="Figure 3: Work Experience Distribution in 1976"}

# Work Experience Distribution in 1976
p_experience <- ggplot(df_clean, aes(x = exp76)) +
  geom_histogram(aes(y = ..density..), bins = 30, fill = "orange", color = "black") +
  geom_density(color = "purple", fill = "purple", alpha = 0.2) +
  labs(title = "Work Experience Distribution in 1976", x = "Years of Experience", y = "Density") +
  annotate("text", x = Inf, y = Inf, label = "Right-skewed distribution", hjust = 1.1, vjust = 2, size = 3, color = "darkmagenta")
p_experience

```

**• Work Experience in 1976 (exp76):**

The histogram for exp76 also exhibits a right-skewed distribution, suggesting that a larger number of individuals had lower work experience, with fewer individuals having high levels of experience. 

The box plot for exp76 shows the presence of outliers with exceptionally high experience levels, and a median experience level that indicates the majority had moderate to low work experience.

### Correlation Matrix

```{r correlation-matrix, fig.cap="Figure 4: Correlation Matrix of Selected Variables"}
library(corrplot)

# Calculate the correlation matrix on selected variables, omitting NAs
cor_matrix <- cor(select(df_clean, wage76, grade76, exp76, age76) %>% na.omit())

# Generate the correlation matrix plot
corrplot(cor_matrix, method = "color", 
         addCoef.col = "black",
         type = "upper",
         tl.col = "black",
         tl.srt = 45,
         tl.cex = 0.6,
         number.cex = 0.6,
         mar = c(0, 0, 2, 0))


```

The correlation matrix provides a nuanced understanding of how different variables interact. Notably, education level and work experience show positive correlations with wages, but the strength of these relationships varies, highlighting the multifaceted nature of wage determination.

### Demographic Distributions

```{r demographic-distributions, fig.cap="Figure 5: Demographic Distributions"}
# Create the 'LivingSituationAt14' column based on existing data

df_clean <- df_clean %>%
  mutate(LivingSituationAt14 = case_when(
    momdad14 == 1 ~ "Both Parents",
    sinmom14 == 1 ~ "Single Mom",
    TRUE ~ "Other"
  ))

# Convert the new column to a factor for better plotting
df_clean$LivingSituationAt14 <- factor(df_clean$LivingSituationAt14)

# Race Distribution
p_race <- ggplot(df_clean, aes(x = as.factor(black))) +
  geom_bar(fill = "coral") +
  labs(title = "Race Distribution", x = "Race (1 = Black, 0 = Other)", y = "Count")

# Region Distribution
p_region <- ggplot(df_clean, aes(x = as.factor(region))) +
  geom_bar(fill = "lightgreen") +
  labs(title = "Region Distribution", x = "Region", y = "Count")

# Living Situation at Age 14
p_living <- ggplot(df_clean, aes(x = LivingSituationAt14)) +
  geom_bar(fill = "lightblue") +
  labs(title = "Living Situation at Age 14", x = "Living Situation", y = "Count")

# Combine the plots with patchwork and assign a collective title
demographic_plots <- p_race + p_region + p_living + 
  plot_layout(ncol = 3)

print(demographic_plots)


```

The combined demographic analysis elucidates the complex interplay between race, region, and early family environment, offering valuable insights for targeted interventions and policies aimed at improving educational and economic opportunities.

### Comprehensive Data Set Insights

Our exploration of the RetSchool dataset through advanced visualizations reveals significant insights into the socio-economic landscape of 1976. Notably, the wage distribution underscores economic disparities, while the educational attainment distribution highlights the societal valuation of specific educational milestones. Work experience distribution and demographic analyses further enrich our understanding of the workforce's characteristics and the socio-economic factors influencing educational and economic outcomes.

## Data Preparation and Model Fitting

The dataset of interest captures various potential predictors of wages in 1976, encompassing education levels, work experience, and demographic variables such as race and geographic location. Effective modeling requires meticulous data preparation to ensure the quality and comparability of the input data. \#### Data Cleaning To ensure the integrity and accuracy of our predictive model, we first tackled the issue of missing data—a factor that can significantly skew predictions if left unaddressed. Our approach to managing missing values was multifaceted, tailored to the nature and extent of the missingness within our dataset. This included strategies such as imputation for numerical columns, where missing entries were replaced with the median value of the respective column, and outright removal of rows where data remained incomplete. Through these careful cleaning and imputation steps, we refined our dataset down to 3,078 observations , each fully equipped with the necessary information for a robust analysis.

```{r data-cleaning, message=FALSE, warning=FALSE}
library(dplyr)
library(ggplot2)
library(readr)

# Load the dataset
df <- read_csv("RetSchool.csv", show_col_types = FALSE)

# Initial dataset size
initial_count <- nrow(df)

# Simple imputation for numerical columns with missing values replaced by median
df_clean <- df %>%
  mutate(
    grade76 = ifelse(is.na(grade76), median(grade76, na.rm = TRUE), grade76),
    exp76 = ifelse(is.na(exp76), median(exp76, na.rm = TRUE), exp76)
  ) %>%
  na.omit() # Remove rows with any remaining missing values

# Count after cleaning
cleaned_count <- nrow(df_clean)

# Print the initial and cleaned dataset sizes
cat("Initial rows:", initial_count, "\n")
cat("Rows after cleaning and imputation:", cleaned_count, "\n")
# Define the response variable y
y <- df_clean$wage76
```

After the cleaning process, our dataset was reduced from 5225 to 3078 observations, illustrating the substantial impact of addressing missing data. \#### Visualizing the Impact of Data Cleaning To visualize how data cleaning influenced the dataset, we compare the distribution of wages (wage76) before and after the cleaning process. This comparison aids in understanding the implications of our data cleaning strategy, especially the effect of median imputation on the distribution.

```{r data-cleaning-visualization, fig.cap="Comparison of Wage Distribution Before and After Data Cleaning"}
# Plotting the distribution of wage76 before cleaning
p_before <- ggplot(df, aes(x = wage76)) +
  geom_histogram(bins = 30, fill = "skyblue", alpha = 0.7) +
  labs(title = "Wage Distribution Before Cleaning", x = "Wage in 1976", y = "Frequency")

# Plotting the distribution of wage76 after cleaning
p_after <- ggplot(df_clean, aes(x = wage76)) +
  geom_histogram(bins = 30, fill = "lightgreen", alpha = 0.7) +
  labs(title = "Wage Distribution After Cleaning", x = "Wage in 1976", y = "Frequency")

# Arrange the plots side by side for comparison
library(patchwork)
p_before + p_after + plot_layout(ncol = 2)

```

This side-by-side comparison offers a visual insight into the changes in the wage distribution due to our data cleaning efforts, specifically highlighting the effects of imputing missing values with the median. The visualization emphasizes the importance of careful data preprocessing to ensure that subsequent analyses are based on reliable and robust data.

#### Feature Scaling

Scaling ensures that all variables contribute equally to the model, preventing features with larger scales from dominating the model's fit. glmnet expects input features to be centered and scaled:

```{r feature-scaling, message=FALSE, warning=FALSE}
library(caret)

# Selecting only numeric features and excluding the target variable 'wage76'
numeric_features <- select(df_clean, where(is.numeric), -wage76)

# Converting the selected features into a matrix, as required by glmnet
features <- data.matrix(numeric_features)

# Scaling the features using the preProcess function from the caret package
preProcValues <- preProcess(features, method = c("center", "scale"))
features_scaled <- predict(preProcValues, features)

# Displaying the effect of scaling on the first few rows of the features
head(features_scaled)


```

#### Visualization of Feature Scaling

To visualize the impact of feature scaling, we compare the distributions of a select variable before and after scaling. This comparison highlights the importance of scaling in bringing all features to a common scale, ensuring fairness in the model's consideration of each feature's contribution.

```{r}
# Selecting a feature for comparison
feature_name <- "exp76" # Example feature

# Plotting the distribution before scaling
p_before_scaling <- ggplot(df_clean, aes_string(x = feature_name)) +
  geom_histogram(binwidth = 1, fill = "#F8766D", alpha = 0.7) +
  labs(title = paste("Distribution of", feature_name, "Before Scaling"), x = feature_name, y = "Frequency")

# Extracting the scaled values for the selected feature
scaled_feature <- features_scaled[,feature_name]

# Plotting the distribution after scaling
p_after_scaling <- ggplot(data.frame(scaled_feature), aes(x = scaled_feature)) +
  geom_histogram(binwidth = 0.5, fill = "#00BFC4", alpha = 0.7) +
  labs(title = paste("Distribution of", feature_name, "After Scaling"), x = paste(feature_name, "(scaled)"), y = "Frequency")

# Arranging the plots side by side for comparison
p_before_scaling + p_after_scaling + plot_layout(ncol = 2)


```

The before-and-after comparison of the feature scaling process underscores the transformation that occurs when features are normalized. This transformation is pivotal for models like LASSO regression, which rely on regularization techniques sensitive to the scale of the input variables.

## Cross-Validation in LASSO Regression

Cross-validation is a pivotal technique in LASSO regression for identifying the optimal regularization parameter, $\lambda$, which moderates the coefficient shrinkage intensity. This mechanism is essential to achieve a model that is appropriately complex to capture the underlying data patterns without succumbing to overfitting or underfitting.

### The Cross-Validation Process

In mathematical terms, cross-validation involves dividing the dataset into $k$ equally sized segments, or "folds." The model is trained on $k-1$ of these folds, with the remaining fold serving as the validation set to gauge model performance. This cycle is executed $k$ times, each time with a different fold as the validation set. The cross-validation error (CVE) for each $\lambda$ is aggregated across all folds, and the $\lambda$ that minimizes this cumulative error is selected as the optimal choice.

The cross-validation error for a given $\lambda$ is expressed as:

$$
CVE(\lambda) = \frac{1}{k} \sum_{i=1}^{k} MSE_i(\lambda)
$$

Here, $MSE_i(\lambda)$ represents the mean squared error on the $i^{th}$ fold, calculated as the average of the squared differences between the observed values and the model predictions, parameterized by $\lambda$.

### Visualizing the Impact of $\lambda$ on Cross-Validation Error

Let's explore the cross-validation outcomes to comprehend how different $\lambda$ values impact model efficacy.

```{r, echo=TRUE, fig.cap="Figure: Cross-Validation Curve"}
library(glmnet)

# Perform 10-fold cross-validation
set.seed(123) # For reproducibility
cv_outcome <- cv.glmnet(features_scaled, y, alpha = 1, nfolds = 10)

# Display the optimal lambda values
cat("Optimal lambda (lambda.min):", cv_outcome$lambda.min, "\\n\\n")
cat("Lambda.1se (within one standard error):", cv_outcome$lambda.1se, "\\n\\n")

# Plot the cross-validation curve
plot(cv_outcome)
title("Cross-Validation Curve")
```

The plot delineates how the prediction error evolves with varying $\lambda$ values, highlighting two critical points:

-   **Optimal** $\lambda$ (lambda.min): `{r} cv_outcome$lambda.min`. This value minimizes cross-validation error, suggesting an optimal balance between model simplicity and predictive accuracy.

-   $\lambda$ within one standard error (Lambda.1se): `{r} cv_outcome$lambda.1se`. Represents a more cautious $\lambda$ choice, potentially yielding a simpler model with a slight increase in error, favoring robustness.

#### Model Coefficients and Interpretation

After identifying the optimal $\lambda$, we can examine the model's coefficients to understand the influence of each predictor:

```{r coefficients-table, echo=FALSE}
library(knitr)

coef_optimal <- coef(cv_outcome, s = "lambda.min")
coef_matrix <- as.matrix(coef_optimal)

# Convert the matrix to a data frame
coef_df <- as.data.frame(coef_matrix, stringsAsFactors = FALSE)

# Add variable names as a new column
coef_df$Variable <- rownames(coef_df)

# Rename the coefficient column for clarity
names(coef_df)[1] <- "Coefficient"

# Use knitr::kable() to create a Markdown table
kable(coef_df, caption = "Optimal Coefficients from LASSO Regression", align = 'l', format = "markdown")
```

Upon determining the optimal $\lambda$ value, a detailed examination of the model's coefficients provides us with valuable insights into the predictors' contributions towards estimating wages in 1976. The coefficients, represented in a sparse matrix, shed light on the relative importance and direction of each variable's effect:

-   **Intercept (1.658013288)**: Sets the baseline wage prediction when all other predictors are at their mean values.
-   **grade76 (0.083832188)**: Each additional year of education in 1976 is associated with an increase in wage, underscoring the value of education.
-   **black (-0.071115786)**: Being black is associated with a decrease in wage compared to non-black individuals, highlighting potential racial disparities.
-   **south76 (-0.059039330)**: Living in the South in 1976 is associated with a decrease in wage, possibly reflecting regional economic differences.
-   **smsa76 (0.064270413)**: Living in a Standard Metropolitan Statistical Area (SMSA) in 1976 is associated with an increase in wage, indicating urban wage premiums.
-   **smsa66 (0.006040491)**: Similarly, living in an SMSA in 1966 shows a smaller positive association with wages, suggesting long-term benefits of urban living.
-   **momdad14 (0.008504452)**: Living with both parents at age 14 is mildly associated with higher wages, possibly reflecting benefits of a stable early home environment.
-   **momed (0.010829506)**: Higher education levels of the mother are associated with higher wages, pointing to the influence of parental education.
-   **age76 (0.117310381)**: Age in 1976 shows a strong positive correlation with wages, reflecting the typical accumulation of experience and skills over time.
-   **col4 (0.007218061)**: Some indication that attending college is associated with higher wages, albeit with a smaller effect size. Variables such as **exp76**, **region**, **sinmom14**, **nodaded**, **nomomed**, **daded**, and **famed** have their coefficients shrunk to zero, indicating that within the context of this model and the data, these predictors do not have a significant impact on wage predictions. This outcome exemplifies the LASSO model's capacity for feature selection, focusing on predictors with meaningful contributions and simplifying the model by excluding those deemed less relevant.

## Results

The analysis conducted through LASSO regression, underpinned by meticulous data preparation and feature scaling, yielded compelling insights into the determinants of wages in 1976.

### Optimal Regularization Parameter

The cross-validation process, pivotal for the model's integrity and predictiveness, identified an optimal regularization parameter ($\lambda$) of approximately 0.00488. This value signifies a balance, minimizing prediction error while avoiding overfitting, thereby ensuring the model's generalization capability.

### Significant Predictors

The model's coefficients at this optimal $\lambda$ highlight significant predictors influencing wage levels:

-   **Education (grade76)**: Demonstrating a positive relationship, indicating that each additional year of education is associated with a wage increase, underscoring the premium on educational attainment.
-   **Racial Background (black)**: Showing a negative association, reflecting wage disparities possibly tied to racial discrimination.
-   **Geographical Factors**: Both living in the South (`south76`) and urban areas (`smsa76`) in 1976 influenced wages, with urban dwelling associated with higher wages, suggesting location-based economic opportunities.
-   **Family Background and Age**: Variables like living with both parents at age 14 (`momdad14`) and the mother's education level (`momed`) were positively correlated with wages, along with `age76`, highlighting the role of familial support and the accrual of experience over time.

Notably, variables such as `exp76`, `region`, and `famed`, among others, saw their coefficients reduced to zero, indicating their limited impact within the predictive framework of this model.

## Recommendations

### Future studies should consider:

-   **Exploring Additional Variables:** Investigate other potential predictors not included in the current model to uncover more determinants of wage disparities.
-   **Temporal Comparisons:** Conduct similar analyses for different time periods to observe changes in wage determinants over time, providing historical wage trend insights.
-   **Employing Other Analytical Techniques:** Utilize complementary regression models or machine learning algorithms to validate findings and reveal additional insights.

## Conclusion

This study's application of LASSO regression has elucidated the multifaceted nature of wage determinants in 1976, revealing the significant roles played by education, racial background, geographical location, and family background. The optimal $\lambda$ identified through cross-validation underscores the importance of balancing model complexity with predictive accuracy, a testament to the LASSO method's robustness.

The findings not only contribute to our understanding of wage dynamics during the period but also underscore the value of regularization techniques like LASSO in distilling complex datasets to their most informative elements. Future research may build on this foundation, exploring additional variables, different time periods, or employing complementary analytical techniques to deepen our understanding of wage determinants.














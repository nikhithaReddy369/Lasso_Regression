---
title: "Innovative Data Exploration with LASSO: Unveiling Patterns in Earnings and Education"
author: "Nikhitha Amireddy, Ishrath Jahan, Sai Kumar Miryala, Muhammad Usman Aslam"
date: '`r Sys.Date()`'
format:
  html:
    code-fold: true
course: STA 6257 - Advanced Statistical Modeling
bibliography: references.bib # file contains bibtex for references
#always_allow_html: true # this allows to get PDF with HTML features
self-contained: true
execute: 
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
---

## Introduction

LASSO, Least Absolute Shrinkage and Selection Operator, is a penalized linear regression method that reduces the number of variables in the model, useful for avoiding overfitting. It works by minimizing the residual sum of squares subject to the sum of the absolute value of the coefficients being less than a constant, simplifying the model and potentially improving prediction accuracy.

In 1996, Robert Tibshirani [@Tibshirani1996] introduced LASSO regression to further the discussion about variable selection. This provided a new and sophisticated solution for the problems in the course of predictive modeling, especially when it came to big and complex datasets. It is therefore important that the model remains parsimonious in nature, explains easily interpretable phenomena, and reduces the risks associated with overfitting by applying a penalty to the absolute values of regression coefficients [@Zhao2006], [@Fan2011]. This innovation was therefore taken up by many scientific disciplines, characterizing the beginning of an era where high-dimensional datasets could be probed with more depth for complexities with better clarity [@Meinshausen2006] @Hastie2009]. The popularity of LASSO regression has risen with the birth of Least Angle Regression by Efron et al. [@Efron2004]  in 2004. 

LASSO regression has particularly become a key instrument with which to insight the complex dynamics underlying socioeconomic trends, public health outcomes, and environmental studies—hence, significantly enriching the toolset available to the data scientist [@Belloni2013], [@Park2008]. Logistic regression is further extended by integration with the capabilities of LASSO, offering a refined method in prediction for the categorical outcomes [@Li2022]. This synergy provides a response to long-term barriers to predictive modeling such as overfitting and multicollinearity with a scope of applications in, in particular, economic forecasting and health diagnostics, amongst many more [@Sauerbrei2015].

The 21st century continues, and the data revolution heightens. A dataset in relation to the augmented domain complexity continues to pervade. Here, LASSO regression does not exist just as a statistical tool but as an innovation beacon assisting researchers with precision and simplicity within the dynamic landscape [@Chintalapudi2022]. The introduction of logistic regression into the area of LASSO has been especially dynamic, providing a nuanced approach to modeling of categorical outcomes [@Simon2013]. This is necessary for areas from epidemiology to finance [@Friedman2010]. This thus places this combination as a great leap forward in the direction of data-driven insights aimed at action, where traditional analytical methods might possibly face challenges of interpretation, firmly situating logistic regression with LASSO among the basic constitutions of modern statistical analysis and predictive modeling [@Hastie2009]. 

Indeed, the way of LASSO development has been marked by a series of important developments that pinpoint the historical trajectory, including the emergence of Least Angle Regression by Efron and colleagues in pointing out the potential, influence, and continued utility in research [@Zhou2022; @Witten2010]. This cutting-edge technology is used throughout a wide spectrum of industries with the goal of dismantling complex data structures and, eventually, extracting meaningful patterns from trends as socioeconomic, public health, and environmental studies. LASSO regression adapted for logistic models was one of the best steps to ward off the impediments towards modeling categorical data with more precision [@Tibshirani2012].




## Literature Review

The versatility of LASSO regression is reflected in the extensive literature discussing its various applications. Notably, LASSO regression has made significant contributions to economics. For example, Zhou et al. (2022)[@Zhou2022] used the LASSO model to investigate what determines the growth of quality of life in China and found that the LASSO model is a good tool for selecting important economic predictors out of a large number of regressors. Thus, it makes important improvements on economic models useful for economic policy stakeholders and researchers in how to promote economic growth. The technique has also been critical for important developments in the area of bioinformatics. 

Lu, et al. (2011) [@Lu2011] showed how LASSO regression can be applied to constructing predictive models — based on gene expressions and other high-throughput data — to understand the relationships between gene expressions and multiple phenotypes, which are critical for understanding genetic pathways and cell systems. In this example, the LASSO regression was used to build a microRNA-target regulatory network, which provides insight into gene regulation mechanisms, as well as applications in personalized medicine and in the understanding of genetic diseases [@Musoro2014].
LASSO regression has expedited the processing and analysis of large datasets in the environmental and energy sectors. Wang et al. (2017) [@Wang2017] detailed its use in predicting ship fuel consumption for diminished environmental impact and increased fuel efficiency in maritime operations.

LASSO regression aids in selecting the most informative variables that influence fuel usage and empowers the maritime industry's sustainability and operational performance efforts. LASSO regression has been a meaningful addition to the analysis of multi-factorial diseases and health outcomes in public health research. In an analysis of dietary intakes for breast cancer, McEligot et al. (2020) [@McEligot2020] showed that logistic LASSO can handle high-dimensional data to uncover the key drivers of health, and this has broad applicability in public health for unraveling and prioritizing the risk factors for a variety of illnesses. His role as a facilitator for the development of machine learning models (especially in feature selection) has been pivotal to improving the accuracy of those predictions as LASSO was proving its worthiness time and time again.

Muthukrishnan and Rohini (2016) [@Muthukrishnan2016] showcased this by demonstrating the ability of LASSO regression “to select the relevant feature attributes of predictive modelling for machine learning”.
After gaining prominence within the traditional domain, the applications of LASSO regression have extended into new spheres such as financial modeling, where it assists analysts in singling out influential market predictors from large datasets characterized by volatility and noise, eventually translating into forecasts and investment strategies [@Friedman2010; @Hastie2015]. The adaptability and precision of LASSO regression within financial analysis underline its growing promise and popularity. Another field where LASSO regression's ability to parse through voluminous climate variable datasets to extract critical indicators of environmental change has been crucial is climate research and sustainability studies. The power of this application is not only a testament to the adaptability but also reflects the urgency in deploying LASSO regression in the face of challenges such as global warming and biodiversity loss, where parsing through complex environmental patterns becomes critical to devising strategies to mitigate its devastating impacts [@Chintalapudi2022; @Li2022]. 

Digital health technologies have opened a trove of opportunities for LASSO regression to analyze the complex interplay between individual lifestyle factors, genetic predispositions, and health outcomes. The inclusion of LASSO in this field of research is an acknowledgement that it is uniquely suited to handle complex, multidimensional data that is needed to understand and generate personalized medicine or public health interventions—a frontier of healthcare that holds promise for applications where LASSO's valuable data-driven insights can be channeled to address bespoke health solutions [@Simon2013; @Park2008] and Sohaee and Bohluli (2024b)[@2024b] employ Lasso polynomial regression to analyze the intricate relationship between socioeconomic, demographic, and technological factors and fatal traffic accidents, demonstrating its effectiveness in handling categorical variables and preventing overfitting. In their study on breast cancer screening adherence (Sohaee and Bohluli 2024a)[@2024a], they utilized Lasso regression to identify significant predictors from various socioeconomic factors, highlighting health disparities and advocating for targeted healthcare policies. Both studies underscore the importance of advanced statistical techniques, such as Lasso regression, in understanding complex datasets to inform policy decisions and improve outcomes in transportation safety and healthcare.

Efron et al. (2004)[@annalsofstatistics] introduced Least Angle Regression (LARS) as adept for high-dimensional data analysis, emphasizing its suitability for large datasets with thousands of predictors. LARS enables efficient feature selection and model building, particularly when adapted for Lasso regression. By combining LARS with Lasso, researchers can select relevant features, manage high-dimensional data, and improve prediction accuracy while mitigating multicollinearity and overfitting concerns. [@10341221] research introduced a novel approach to predicting heart disease by leveraging a sophisticated computer model. By integrating LASSO, a technique akin to a smart sieve for crucial information, with other methods, the accuracy of predictions is enhanced. It showcases how advanced computer methods can assist doctors in foreseeing health issues preemptively, potentially leading to improved prevention and treatment strategies for heart disease. Researchers analyzed extensive health data, identifying key factors using LASSO, and developed a highly effective predictive model. This could translate to earlier and personalized care for individuals at risk of heart disease, promising significant advancements in healthcare.

Indeed, these and many more diverse applications are a testament to the potency of LASSO regression as a powerful tool in the arsenal of researchers and practitioners across disciplines, as they grapple with increasingly high-resolution and large datasets. By simplifying complex models and identifying important predictors from large datasets, LASSO regression has cemented itself as an essential part of modern statistical analysis and predictive modeling [@Belloni2013; @Zou2005; @Witten2010; @Tibshirani2012].



## Methods

## Analysis of the dataset and Results

## Dataset RetSchool 

RetSchool from R datasets:
[https://vincentarelbundock.github.io/Rdatasets/doc/Ecdat/RetSchool.html](https://vincentarelbundock.github.io/Rdatasets/doc/Ecdat/RetSchool.html)

[https://vincentarelbundock.github.io/Rdatasets/articles/data.html](https://vincentarelbundock.github.io/Rdatasets/articles/data.html) 

## Data Description

The dataset contains 5,225 entries and 18 columns, indicating a fairly large size in terms of the number of records. Here are some key points observed from the initial examination:

•	Multiple Features: 

There are 18 columns in total, which include a variety of variables such as education level (grade76), experience (exp76), race (black), region indicators (south76, smsa76, region, smsa66), family background (momdad14, sinmom14, nodaded, nomomed, daded, momed, famed), age (age76), and college education indicator (col4). 

This variety suggests a potentially high-dimensional feature space, especially considering interactions or polynomial features could be explored.

•	Missing Values: Some columns have missing values (wage76, grade76, exp76, south76), which LASSO regression can handle well if preprocessing steps include imputation or if the analysis focuses on available data.

•	Binary and Continuous Variables: The dataset comprises both binary (e.g., black, momdad14, sinmom14, nodaded, nomomed, col4) and continuous variables (e.g., wage76, grade76, exp76, daded, momed, age76), suggesting that LASSO could be beneficial in handling a mix of variable types by identifying which variables contribute most to the prediction.
Given these characteristics, LASSO regression is a strong candidate for several reasons:

1.	Feature Selection: LASSO's ability to shrink coefficients of less relevant features to zero will help in identifying which variables are most predictive of the outcome, likely simplifying the model by excluding variables that do not contribute significantly.

2.	Handling Multicollinearity: If variables such as education level and experience are highly correlated, LASSO can help in mitigating multicollinearity by selecting one or a few variables from a group of correlated variables, which enhances model stability and interpretability.

3.	Model Interpretability: With many variables in play, reducing the model to only include significant predictors can make it easier to understand and explain, which is often crucial in research and policy analysis contexts.

4.	Robustness to Overfitting: The regularization aspect of LASSO helps in preventing overfitting, making the model more generalizable to unseen data, which is particularly valuable in predictive modeling.

###The choice of a dataset like "RetSchool.csv":

This dataset provides a thorough overview of the socioeconomic environment in 1976, emphasizing the critical roles of education and work experience in determining wages, while also highlighting the influence of demographic factors. The insights suggest that while individual effort and achievement (as measured by education and experience) are important, background factors such as race, region, and early family environment also play significant roles in shaping economic outcomes.


```{r}
library(readr)
data_RetSchool <- read_csv("RetSchool.csv")
print(data_RetSchool, n = 10)
summary(data_RetSchool)
```

This dataset provides a rich collection of socio-economic and demographic information about individuals, focusing on their wages, education levels, work experience, and other personal and family background characteristics.Here's an overview of the dataset and the key variables it includes:

## Key Variables:

•	wage76: The wage of individuals in 1976, providing insight into the economic status and disparities within the population.

•	grade76: The highest grade completed by 1976, indicating the educational attainment and its distribution among individuals.

•	exp76: Work experience in years by 1976, shedding light on the labor market engagement and its impact on wages.

•	black: A binary variable indicating whether an individual is black, offering a lens into the racial composition of the dataset.

•	south76: Indicates whether an individual lived in the South in 1976, allowing for regional analysis and its impact on socio-economic outcomes.

•	smsa76: A binary variable indicating residence in a Standard Metropolitan Statistical Area (SMSA) in 1976, relevant for urban versus rural disparities.

•	region: Categorical variable representing different regions, useful for understanding geographic influences on economic and educational outcomes.

•	momdad14, sinmom14: Variables indicating living situations at age 14, which can be crucial for studies on family structure and its effects on individual development.

•	daded, momed: Education levels of dad and mom, respectively, providing data on family background and its potential influence on educational attainment.

•	age76: Age of individuals in 1976, necessary for demographic analyses and understanding the age distribution of the workforce.

•	col4: An unspecified indicator variable, the context of which would need clarification for meaningful analysis.

## Insights and Implications from the dataset

The RetSchool dataset offers a comprehensive view of the interplay between education, work experience, and wages, highlighting the positive correlation between educational attainment and economic outcomes. It also underscores the significant impact of demographic factors such as race, region, and family background on these outcomes.
The diversity in educational attainment, work experience, and wages across different demographic groups suggests the presence of underlying socio-economic disparities. The dataset allows for an exploration of how early-life conditions, including family structure and parental education, affect later-life economic success.


### Data Visualization: Week 7

```{r}
library(ggplot2)
library(patchwork)
data_RetSchool <- read.csv("RetSchool.csv")
data_RetSchool <- na.omit(data_RetSchool)
```


```{r}
library(ggplot2)
library(patchwork)

# Summary Statistics Plots
# Histograms and density plots for wage76, grade76, and exp76
p_wage_hist <- ggplot(data_RetSchool, aes(x = wage76)) +
  geom_histogram(bins = 30, fill = "skyblue", color = "black") +
  geom_density(alpha = .2, fill = "#FF6666") +
  labs(title = "Distribution of wage76")

p_grade_hist <- ggplot(data_RetSchool, aes(x = grade76)) +
  geom_histogram(bins = 30, fill = "skyblue", color = "black") +
  geom_density(alpha = .2, fill = "#FF6666") +
  labs(title = "Distribution of grade76")

p_exp_hist <- ggplot(data_RetSchool, aes(x = exp76)) +
  geom_histogram(bins = 30, fill = "skyblue", color = "black") +
  geom_density(alpha = .2, fill = "#FF6666") +
  labs(title = "Distribution of exp76")

# Box plots for wage76, grade76, and exp76
p_wage_box <- ggplot(data_RetSchool, aes(y = wage76)) +
  geom_boxplot(fill = "lightgreen") +
  labs(title = "Box Plot of wage76")

p_grade_box <- ggplot(data_RetSchool, aes(y = grade76)) +
  geom_boxplot(fill = "lightgreen") +
  labs(title = "Box Plot of grade76")

p_exp_box <- ggplot(data_RetSchool, aes(y = exp76)) +
  geom_boxplot(fill = "lightgreen") +
  labs(title = "Box Plot of exp76")

# Combine the plots into a single layout
summary_stats_layout <- (p_wage_hist | p_grade_hist | p_exp_hist) / 
                        (p_wage_box | p_grade_box | p_exp_box) + 
                        plot_annotation(title = "Summary Statistics of Key Numeric Variables")
print(summary_stats_layout)
```


### Summary Statistics of Key Numeric Variables
### Distribution and Box Plot Analysis:

•	Wage in 1976 (wage76):
o	The histogram for wage76 shows a right-skewed distribution, indicating that most individuals earned lower wages, with a few outliers earning significantly more. This pattern is typical for income data.
o	The box plot reveals a wide range of wages with several outliers on the higher end. The median wage is relatively low compared to the mean, further indicating the skewness towards lower wage values.

•	Education Level in 1976 (grade76):
o	The distribution of grade76 shows a multi-modal pattern, suggesting concentrations of individuals at specific education levels. The peaks likely represent completion of certain education milestones (e.g., high school, some college).
o	The box plot indicates a relatively normal distribution with a few outliers, suggesting most individuals had a similar level of education, with a median around high school completion.

•	Work Experience in 1976 (exp76):
o	The histogram for exp76 also exhibits a right-skewed distribution, suggesting that a larger number of individuals had lower work experience, with fewer individuals having high levels of experience.
o	The box plot for exp76 shows the presence of outliers with exceptionally high experience levels, and a median experience level that indicates the majority had moderate to low work experience.

These visualizations help in understanding the basic characteristics of the dataset, including the central tendency and variability of wages, education levels, and work experience among the individuals in 1976. The presence of skewness and outliers in these distributions indicates the diversity in the economic and educational background of the population studied.

```{r}
# Create the 'LivingSituationAt14' column based on existing data

data_RetSchool$LivingSituationAt14 <- ifelse(data_RetSchool$momdad14 == 1, 'Both Parents', 
                                   ifelse(data_RetSchool$sinmom14 == 1, 'Single Mom', 'Other'))
data_RetSchool$LivingSituationAt14 <- factor(data_RetSchool$LivingSituationAt14)

```

### Wage Distribution by Living Situation at Age 14:

```{r}
#Wage Distribution by Living Situation at Age 14
p_wage_distribution <- ggplot(data_RetSchool, aes(x = wage76)) + geom_histogram(aes(y = after_stat(density)), bins = 20, fill = "skyblue", color = "black") + geom_density(alpha = 0.2, fill = "#FF6666") + facet_wrap(~ LivingSituationAt14, scales = "free_y") + labs(title = "Wage Distribution by Living Situation at Age 14", x = "Wage in 1976 (wage76)", y = "Density") + theme_minimal()
#ggsave("wage_distribution_by_living_situation.png", p_wage_distribution, width = 12, height = 4)

print(p_wage_distribution)
```

The faceted charts provide a comparison of wage distribution in 1976 (wage76) across different living situations at age 14, namely "Both Parents", "Single Mom", and "Other". Each facet represents a histogram of wage distribution for a specific living situation, allowing for direct comparison. Key insights:

•	General Distribution: Across all living situations, the wage distribution is right-skewed, indicating a majority of individuals earning lower wages with a tail of higher earners.

•	Differences in Distribution: While the overall shape of the distribution appears similar across different living situations, there are subtle differences in the spread and peaks of the distributions. This suggests that living situation at age 14 may have some influence on wage outcomes, but the effect is not starkly different when visualized this way.

•	Variability: The presence of variability within each living situation category underscores the complexity of factors influencing wage, beyond just early life living conditions.


### Correlation Matrix Plot

```{r}

# Correlation Matrix Plot
library(corrplot)
cor_matrix <- cor(na.omit(data_RetSchool[sapply(data_RetSchool, is.numeric)]))

corrplot(cor_matrix, method = "color",
addCoef.col = "black",  # Color of the correlation coefficients
tl.col = "black",       # Color of the text labels
tl.srt = 45,            # Rotation of text labels
tl.cex = 0.7,           # Font size of text labels
number.cex = 0.5,       # Font size of the correlation coefficients
cl.cex = 0.7,           # Font size of the color legend
addgrid.col = "white",  # Color of the grid lines
tl.pos = "lt",          # Position of text labels (left and top sides)
diag = FALSE)           # Remove the diagonal
# Save the plot as a PNG file
# Plot the correlation matrix with adjusted parameters
```

Demographic Analysis: 

```{r}

#Demographic Analysis Plots
p_race <- ggplot(data_RetSchool, aes(x = as.factor(black))) + geom_bar(fill = "skyblue") + labs(title = "Distribution by Race", x = "Race (1=Black, 0=Not Black)", y = "Count") + theme_minimal()

p_region <- ggplot(data_RetSchool, aes(x = as.factor(region))) + geom_bar(fill = "lightgreen") + labs(title = "Distribution by Region", x = "Region", y = "Count") + theme_minimal()

p_living_situation <- ggplot(data_RetSchool, aes(x = LivingSituationAt14)) + geom_bar(fill = "lightcoral") + labs(title = "Living Situation at Age 14", x = "Living Situation", y = "Count") + theme_minimal()
#Combine the demographic plots into one figure

demographic_plot <- (p_race | p_region | p_living_situation) + plot_layout(guides = 'collect') & theme(plot.title = element_text(hjust = 0.5))

#ggsave("demographic_analysis.png", demographic_plot, width = 18, height = 6)

print(demographic_plot)

```
 
###Demographic Analysis:

 The bar charts provide insights into the demographic composition of the dataset based on race, region, and living situation at age 14.
 
Distribution by Race:
•	The dataset includes a larger number of individuals who are not black compared to those who are black. This distribution is crucial for understanding the racial composition of the study population and assessing the representativeness of the dataset across different racial groups.

Distribution by Region:
•	The participants are spread across different regions, with varying counts in each. The diversity in region distribution can help analyze regional influences on wages, education, and other factors of interest.

Living Situation at Age 14:
•	A significant portion of the participants lived with both parents at age 14, with fewer living with a single mom, and a small number categorized under "Other" living situations. This demographic information can be valuable for studies examining the impact of early family environment on education and economic outcomes.

Comprehensive Demographic Composition:
•	The combined demographic analysis elucidates the complex interplay between race, region, and early family environment, offering valuable insights for targeted interventions and policies aimed at improving educational and economic opportunities.

###Dataset Summary:

The dataset and its visualizations offer a wealth of insights into the socio-economic characteristics of individuals, focusing on aspects such as wages, education, work experience, race, regional distribution, and living situations at age 14. 

Here's a comprehensive summary of the insights derived from the data and visual analyses:
1. Wage, Education, and Experience Distribution:
•	Wages in 1976 are right-skewed, indicating that most individuals earned lower wages, with a few outliers earning significantly more. This skewness highlights economic disparities within the population.

•	Education Levels show a multimodal distribution, with peaks at certain levels likely representing significant milestones in the education system. This suggests the presence of standard education paths followed by the population.

•	Work Experience also exhibits a right-skewed distribution, with many individuals having lower levels of experience. This reflects the younger or less experienced portion of the workforce.

2. Education and Wage Relationship:
•	There is a positive correlation between education level and wages, suggesting that higher education levels are associated with higher earning potential. However, the wide range in wages at any given education level underscores the influence of other factors, such as field of study, industry, and geographic location, on wages.

3. Work Experience and Wage Relationship:
•	A positive trend indicates that individuals with more work experience tend to have higher wages, aligning with the expectation that experience contributes to skill development and higher pay. However, the increase in wages appears to plateau beyond a certain level of experience, hinting at a ceiling effect in certain jobs or industries.

4. Demographic Analysis:
•	Race: The data set reveals a larger number of non-black individuals compared to black individuals, providing a crucial perspective on the racial composition of the study population.
•	Region: The participants are spread across different regions, with variable counts, indicating regional diversity that could influence economic outcomes.
•	Living Situation at Age 14: A significant portion of the participants lived with both parents at age 14, suggesting a predominant family structure. The differences in living situations may have implications for educational and economic outcomes.

5. Correlation Matrix Insights:
•	The correlation matrix provides a nuanced understanding of how different variables interact. Notably, education level and work experience show positive correlations with wages, but the strength of these relationships varies, highlighting the multifaceted nature of wage determination.

6. Comprehensive Demographic Composition:
•	The combined demographic analysis elucidates the complex interplay between race, region, and early family environment, offering valuable insights for targeted interventions and policies aimed at improving educational and economic opportunities.

### Summary:
The dataset paints a detailed picture of the socio-economic landscape in 1976, emphasizing the critical roles of education and work experience in determining wages, while also highlighting the influence of demographic factors. The insights suggest that while individual effort and achievement (as measured by education and experience) are important, background factors such as race, region, and early family environment also play significant roles in shaping economic outcomes.


### WEEK 8

### Methodology

Data Source and Preparation:

This study adopts a rigorous approach to dissect the intricate relationships between a set of predictors and response variables—namely wage76, grade76, and exp76—within our dataset. The essence of our analysis pivots around meticulous data handling, the strategic application of LASSO regression for diverse outcomes, and a thorough exploration of interaction terms, subgroup analyses, and innovative techniques for managing missing data.

A cornerstone of our methodology is the acknowledgement and mitigation of the challenges posed by missing data on the reliability of predictive modeling. In light of this, we have implemented two pivotal strategies:

1.Dropping Rows with Missing Response Variables: To safeguard the robustness of our analysis, we ensured that all instances involving the key response variables wage76, grade76, and exp76 were complete. This selective exclusion is fundamental to precluding biased estimations that could compromise the integrity of our findings.


2.Imputation of Missing Predictor Variables: In cases of absent predictor variables, we employed a nuanced imputation strategy tailored to the nature of the missing data. Specifically, for continuous variables, we imputed missing values with their median, while categorical variables saw imputation via the mode. This approach is designed to maintain the dataset's structural coherence and optimize the utilization of available information for modeling purposes.

###Predictive Modeling and Feature Selection

###LASSO Regression

LASSO Regression, standing for Least Absolute Shrinkage and Selection Operator, serves a dual purpose in this study: prediction and feature selection. The model is elegantly formulated to balance predictive accuracy with model simplicity, automatically selecting a subset of predictors that contribute most significantly to the outcome.

###Model Formulation

The essence of the LASSO model is captured in its objective function:

minβ{2N1∑i=1N(yi−β0−∑j=1pβjxij)2+λ∑j=1p∣βj∣}

Here, yi represents the response variables, xij the predictor variables, βj the coefficients, N the number of observations, p the number of predictors, and λ the regularization parameter. The choice of λ plays a critical role in the model's ability to select relevant features and predict outcomes effectively.


###Model Performance and Feature Selection

The study's findings reveal that LASSO regression's predictive accuracy varies across different scenarios. For instance, while the model shows moderate accuracy in predicting wage76, it excels in predicting grade76 and exp76. This indicates a strong predictive relationship for certain variables, showcasing the model's effectiveness in feature selection and prediction.

These measures are essential for the robustness of the study's findings, enabling a comprehensive analysis of the relationships between predictors and response variables.

```{r}
data <- read.csv("RetSchool.csv")

library(tidyverse) # For data manipulation and visualization
library(caret) # For modeling and machine learning
library(recipes) # For preprocessing
library(glmnet) # For LASSO regression

# Define the function to run scenarios
run_scenario <- function(data, target, scenario_name) {
  # Ensure the target variable is not in the set of predictors
  predictors <- setdiff(names(data), target)
  
  # Dropping rows where the target is missing
  data_scenario <- na.omit(data[, c(predictors, target)])
  
  # Preprocessing steps
  preprocess <- recipe(as.formula(paste(target, "~ .")), data = data_scenario) %>%
    step_impute_mean(all_numeric_predictors(), -all_outcomes()) %>%
    step_impute_mode(all_nominal_predictors(), -all_outcomes()) %>%
    step_dummy(all_nominal_predictors(), -all_outcomes()) %>%
    step_scale(all_numeric_predictors(), -all_outcomes())
  
  # Splitting the data into training and testing sets
  set.seed(0)
training_indices <- createDataPartition(data_scenario[[target]], p = 0.8, list = FALSE)
  train_data <- data_scenario[training_indices, ]
  test_data <- data_scenario[-training_indices, ]
  
  # Fit the preprocessing steps
  prepped_data <- prep(preprocess, training = train_data)
  
  # Preprocessing training and testing data
  train_processed <- bake(prepped_data, new_data = train_data)
  test_processed <- bake(prepped_data, new_data = test_data)
  
  # Model training with LASSO
  lasso_model <- train(
    as.formula(paste(target, "~ .")), data = train_processed,
    method = "glmnet",
    trControl = trainControl("cv", number = 5),
    tuneGrid = expand.grid(alpha = 1, lambda = seq(0.001, 0.1, length = 10))
  )
  
  # Predicting and evaluating the model
  predictions <- predict(lasso_model, newdata = test_processed)
  mse <- mean((predictions - test_processed[[target]])^2)
  r2 <- cor(predictions, test_processed[[target]], use = "complete.obs")^2
  
  list(Scenario = scenario_name, MSE = mse, R2 = r2, Coefficients = coef(lasso_model$finalModel, lasso_model$bestTune$lambda))
}
# Assuming 'data' has been properly loaded
results <- list(
  run_scenario(data, "wage76", "Predicting wage76"),
  run_scenario(data, "grade76", "Predicting grade76"),
  run_scenario(data, "exp76", "Predicting exp76")
)

print(results)
```


###Model Performance Evaluation:

Our analytical approach employed the LASSO (Least Absolute Shrinkage and Selection Operator) regression technique, renowned for its efficacy in handling multicollinearity and its prowess in selecting relevant features by diminishing the coefficients of lesser variables to zero, as elucidated by Tibshirani (1996)[@Tibshirani1996]. The performance of the LASSO regression models was meticulously evaluated using the Mean Squared Error (MSE) and R-squared (R²) metrics to assess predictive accuracy and the variance explained by the models, respectively. The model's performance varied across different response variables: it exhibited moderate accuracy in predicting wage76 (MSE = 0.1491, R² = 0.2742), exceptional precision in forecasting grade76 (MSE ≈ 3.09e-05, R² ≈ 0.99999552), and similarly high accuracy for exp76 (MSE ≈ 1.86e-05, R² ≈ 0.99999888).

###Implications and Recommendations

The study's findings emphasize the critical importance of careful variable selection and the pivotal role of data quality in the success of predictive modeling efforts. While LASSO regression demonstrated high accuracy in certain instances, its performance was notably variable, contingent on the response variable in question and the dataset's characteristics. Based on these insights, we propose several avenues for further analysis, including the exploration of a composite score as a new response variable, the application of diverse cross-validation techniques and LASSO parameter adjustments, the inclusion of interaction terms, subgroup analyses, alternative approaches to handling missing data, and the extension of LASSO regression to classification tasks for predicting categorical outcomes, such as college attendance as follows


###Integration of Multiple Response Variables: 

By crafting a composite score to serve as a novel response variable, researchers can gain deeper insights into the dynamics between education, professional experience, and wage levels.

###Enhanced Cross-Validation and Parameter Optimization: 

Implementing a variety of cross-validation methods and fine-tuning the regularization parameter specific to LASSO could significantly improve both the accuracy of the model and the precision in selecting relevant features.

###Incorporation of Interaction Terms: 

The introduction of interaction terms among predictors may reveal intricate relationships that simple linear models fail to capture, offering a more nuanced understanding of the data.

###Focused Subgroup Analysis: 

Dissecting the dataset into specific subgroups and analyzing these segments can uncover unique patterns and influential factors that differ across various demographics.

###Predictive Modeling for Categorical Outcomes: 

In cases dealing with categorical outcomes, like predicting whether an individual attends college, adapting LASSO regression for classification tasks could broaden its utility. This adaptation allows for the efficient handling of binary or multinomial outcomes, thus expanding the scope of LASSO regression to encompass a wider range of predictive scenarios.


### WEEK 9


Week 9 analysis includes the exploration of a composite score as a new response variable, the application of diverse cross-validation techniques and LASSO parameter adjustments, the inclusion of interaction terms, subgroup analyses, alternative approaches to handling missing data, and the extension of LASSO regression to classification tasks for predicting categorical outcomes, such as college attendance as follows

1)Integration of Multiple Response Variables
2)Cross-Validation in LASSO Regression
3)Subgroup Analysis: Assessing Variability Across Demographics
4)Interaction Terms: Unveiling Complex Relationships
5)Different Handling of Missing Data
6)Logistic Regression with LASSO for Categorical Outcome Prediction

### 1. Integration of Multiple Response Variables

In complex research scenarios, single response variables often fall short of capturing the multifaceted nature of the phenomena under investigation. The integration of multiple response variables from [@Hancock2013] into a unified composite score addresses this limitation by aggregating varied dimensions of the subject matter into a single, comprehensive metric. This approach is particularly beneficial in fields such as psychology, education, and health sciences, where outcomes are inherently multidimensional. For instance, in evaluating educational interventions, a composite score might combine measures of academic achievement, student engagement, and social skills to provide a holistic view of student development.

Constructing a meaningful composite score requires careful consideration of the variables to be included and the method of aggregation. Variables should be selected based on their relevance to the research question and their ability to collectively represent the construct of interest. The aggregation method, whether it involves simple averaging or a more complex weighting scheme, should reflect the relative importance of each variable to the overall score. This process not only enhances the validity of the composite measure but also ensures that the score meaningfully reflects the underlying construct.

The use of composite scores in statistical analysis allows researchers to address complex questions with greater clarity and efficiency. By reducing the dimensionality of the data, this approach can simplify the analysis and interpretation of results, making it easier to identify patterns and draw conclusions. Furthermore, composite scores can facilitate comparisons across studies or populations, providing a standardized metric for evaluating and synthesizing research findings.

### 2. Cross-Validation in LASSO Regression

Cross-validation serves as a cornerstone in the application of LASSO regression[@James2013], ensuring that the model is neither overfitted nor underfitted. LASSO regression, by its design, reduces model complexity and prevents overfitting through the introduction of a regularization parameter, lambda, which penalizes the magnitude of coefficients. The cross-validation process involves systematically dividing the dataset into complementary subsets, training the model on one subset, and validating the model on the other subset to determine the lambda value that minimizes the prediction error.

The choice of cross-validation technique, such as k-fold or leave-one-out, depends on the size and nature of the dataset. K-fold cross-validation, where the data is divided into k equally sized segments, strikes a balance between computational efficiency and the reliability of the validation process. In contrast, leave-one-out cross-validation, though more computationally intensive, offers a more thorough assessment by using all but one data point for training and the excluded point for validation, iteratively.

This meticulous process of model validation through cross-validation not only enhances the predictive accuracy of LASSO regression but also contributes to the robustness of the model. By selecting the optimal lambda value, LASSO effectively identifies the most relevant predictors, reducing the risk of overfitting and ensuring the model's generalization to new, unseen data. This approach underscores the importance of a disciplined methodology in model selection, safeguarding against common pitfalls in predictive modeling.


### 3.Subgroup Analysis: Assessing Variability Across Demographics

Subgroup analysis[@Sun2020] is a vital tool for unveiling the differential effects of predictors across various segments of a population. This methodological approach is critical in fields such as medicine, public health, and social sciences, where understanding the heterogeneity of treatment effects or predictors across diverse groups can inform tailored interventions and policies. By stratifying the sample based on key demographic or categorical variables, researchers can evaluate whether and how the relationships between independent and dependent variables differ among distinct subgroups.

The execution of subgroup analysis requires rigorous methodological considerations to avoid spurious findings or over interpretation of results. It is essential to pre-specify the subgroups of interest based on theoretical or empirical justifications and to adjust for multiple comparisons when testing hypotheses across multiple subgroups. Moreover, the interpretation of subgroup analyses must consider the potential for interaction effects, which may suggest that the effect of a predictor on the outcome is contingent upon the level of another variable.

Despite its challenges, subgroup analysis is indispensable for advancing our understanding of the nuanced dynamics within complex data. By identifying subgroup-specific patterns, researchers can contribute to the development of more nuanced theories and models that reflect the complexity of human behavior and societal phenomena. This approach not only enhances the explanatory power of statistical analyses but also supports the development of more personalized and effective interventions.

### 4.Interaction Terms: Unveiling Complex Relationships

Incorporating interaction terms[@Aiken1991] into regression models is crucial for understanding the complex interplay between variables. This approach is instrumental in identifying situations where the effect of one independent variable on the dependent variable is modified by the presence or level of another variable. For example, in the context of labor economics, the impact of education on income might differ based on gender, suggesting an interaction between education and gender.

The identification and inclusion of meaningful interaction terms require a deep understanding of the theoretical framework guiding the research, as well as a careful statistical testing of the hypothesized interactions. This involves not only the statistical modeling of interactions but also a thoughtful consideration of the scale and coding of variables, as these can significantly influence the interpretation of interaction effects.

The analysis and interpretation of interaction terms demand a nuanced approach, as the presence of significant interactions indicates that the main effects of the involved variables cannot be interpreted in isolation. Instead, the effect of one variable is contingent upon the level of another, highlighting the conditional nature of relationships within the data. This complexity underscores the richness of interaction effects in capturing the multifaceted relationships that exist in real-world phenomena.

### 5.Different Handling of Missing Data

Missing data significantly challenges statistical analysis, potentially leading to biased estimates and diminishing the robustness of analytical insights. 
Approaches to tackle this issue range from the straightforward method of complete case analysis, where rows containing any missing values are dropped, to more sophisticated imputation techniques. The straightforward nature of complete case analysis may seem appealing, yet it can lead to substantial data loss and introduce bias, especially if the missing data is not randomly distributed. Conversely, imputation methods, like KNNImputer, infer missing values based on the similarity of observations within the dataset, thereby retaining a larger portion of the data and potentially minimizing bias.

The decision between these methodologies hinges on the nature of the missing data and the specific goals of the analysis. While complete case analysis might suffice for datasets with minimal and randomly missing data, imputation generally becomes the preferred choice in situations where maintaining the sample size and integrity of the dataset is paramount. The KNN Imputer method [@Little2002], in particular, capitalizes on the principle that observations with similar characteristics are likely to yield similar responses, positioning it as a robust tool for imputation in datasets where variables exhibit complex interrelationships.

Engaging in a comparative analysis between these methods sheds light on their respective impacts on analytical conclusions. By examining how different approaches to handling missing data influence the results, researchers can better gauge the sensitivity of their analyses to missingness, thereby informing the selection of the most suitable method for their context.


### 6.Logistic Regression with LASSO for Categorical Outcome Prediction

Logistic regression, when augmented with LASSO regularization[@Hastie2009], emerges as a formidable tool for modeling relationships between a constellation of predictors and a categorical outcome. This enhancement is particularly advantageous in scenarios characterized by high-dimensional data or when the objective is to bolster model interpretability through judicious feature selection. LASSO regularization applies a penalty to the coefficients' magnitude, compelling some to shrink to zero, which facilitates the selection of a simpler model that mitigates the risk of overfitting.

The synergy between logistic regression and LASSO regularization proves invaluable for predicting binary or categorical outcomes, such as estimating the probability of college attendance based on socioeconomic factors. This methodology not only enables the computation of outcome probabilities but also assists in pinpointing the predictors most instrumental in shaping those outcomes. A crucial aspect of this process is the determination of the optimal penalty parameter via cross-validation, ensuring the model attains an equilibrium between complexity and generalization.

The efficacy and clarity logistic regression with LASSO brings to predicting categorical outcomes render it a favored approach among researchers and practitioners alike, particularly in disciplines eager to decipher the determinants influencing categorical phenomena. Concentrating on pivotal predictors while discarding superfluous ones enhances the model's interpretability, facilitating deeper insights into the driving forces behind observed outcomes.

### Statistical Modeling

WEEK 10-12

```{r}

```

### Conclusion

## References

1.Chunli Zhoua, X. L. (2022). "Influencing factors of the high-quality economic development in China based on LASSO model." Energy Reports, 8, 1055–1065.

2.Yiming Lu, Y. Z. (2011). "A Lasso regression model for the construction of microRNA-target regulatory networks." Bioinformatics, pages 2406–2413.

3.Shengzheng Wang, B. J. (2017). "Predicting ship fuel consumption based on LASSO regression." Marine Pollution Bulletin, Elsevier, 1361-9209.

4.Archana J. McEligot, V. P. (2020). "Logistic LASSO Regression for Dietary Intakes and Breast Cancer." Nutrients, 12, 2652.

5.Muthukrishnan R, R. R. (2016). "LASSO: A Feature Selection Technique In Predictive Modeling For Machine Learning." IEEE International Conference on Advances in Computer Applications (ICACA), 978-1-5090-3770-4.

6.Efron, B., Hastie, T., Johnstone, I., & Tibshirani, R. (2004). "Least angle regression." Annals of Statistics, 32(2), 407-499.

7.Tibshirani, R. (1996). "Regression shrinkage and selection via the lasso." Journal of the Royal Statistical Society: Series B (Methodological), 58(1), 267-288.

8.Musoro, J.Z., et al. (2014). "Validation of prediction models based on lasso regression with multiply imputed data." BMC Medical Research Methodology.

9.Chintalapudi, N., et al. (2022). "LASSO Regression Modeling on Prediction of Medical Terms among Seafarers’ Health Documents Using Tidy Text Mining." Bioengineering.

10.Li, Y., et al. (2022). "Applying logistic LASSO regression for the diagnosis of atypical Crohn's disease." Scientific Reports.

11.Hastie, T., Tibshirani, R., Friedman, J. (2009). "The Elements of Statistical Learning: Data Mining, Inference, and Prediction." This book provides foundational knowledge on statistical learning techniques, including LASSO.

12.Simon, N., et al. (2013). "Regularization Paths for Cox’s Proportional Hazards Model via Coordinate Descent." Journal of Statistical Software. This paper discusses extensions of LASSO for survival analysis.

13.Friedman, J., Hastie, T., Tibshirani, R. (2010). "Regularization Paths for Generalized Linear Models via Coordinate Descent." Journal of Statistical Software. This work extends LASSO to a broader class of models.

14.Zou, H., Hastie, T. (2005). "Regularization and variable selection via the elastic net." Journal of the Royal Statistical Society: Series B. This paper introduces the Elastic Net, a significant extension of LASSO.

15.Meinshausen, N., Bühlmann, P. (2006). "High-dimensional graphs and variable selection with the Lasso." Annals of Statistics. This study explores the use of LASSO for variable selection in high-dimensional graphs.

16.Tibshirani, R., et al. (2012). "Strong rules for discarding predictors in lasso-type problems." Journal of the Royal Statistical Society: Series B. This paper discusses computational techniques to enhance LASSO's efficiency.

17.Park, M.Y., Hastie, T. (2008). "Penalized logistic regression for detecting gene interactions." Biostatistics. This study applies LASSO in a genetic context to detect interactions.

18.Belloni, A., Chernozhukov, V. (2013). "Least squares after model selection in high-dimensional sparse models." Bernoulli. This paper addresses post-model selection inference, relevant for LASSO applications.

19.Witten, Daniela M., and Robert Tibshirani. "A Framework for Feature Selection in Clustering." Journal of the American Statistical Association, vol. 105, no. 490, 2010, pp. 713-726.

20.Sauerbrei, Willi, et al. "On Stability Issues in Deriving Multivariable Regression Models." Biometrical Journal, vol. 57, no. 4, 2015, pp. 531-555.

21.Zhao, Peng, and Bin Yu. "On Model Selection Consistency of Lasso." Journal of Machine Learning Research, vol. 7, Nov. 2006, pp. 2541-2563.

22.Fan, Jianqing, and Jinchi Lv. "Nonconcave Penalized Likelihood with NP-Dimensionality." IEEE Transactions on Information Theory, vol. 57, no. 8, 2011, pp. 5467-5484.

23.Hastie, Trevor, et al. "Matrix Completion and Low-Rank SVD via Fast Alternating Least Squares." Journal of Machine Learning Research, vol. 16, 2015, pp. 3367-3402.

24.Kasper, G., Momen, M., Sorice, K.A. et al. Effect of neighbourhood and individual-level socioeconomic factors on breast cancer screening adherence in a multi-ethnic study. BMC Public Health 24, 63 (2024). 

25.Sohaee, N.; Bohluli, S. Nonlinear Analysis of the Effects of Socioeconomic, Demographic, and Technological Factors on the Number of Fatal Traffic Accidents. Safety 2024, 10, 11. https://doi.org/10.3390/safety10010011 

26.Bradley 	Efron. Trevor 	Hastie. Iain 	Johnstone. Robert Tibshirani. "Least angle regression." Ann. Statist. 32 (2) 407 - 499, April 2004. https://doi.org/10.1214/009053604000000067 

27.A. Jafar and M. Lee, "HypGB: High Accuracy GB Classifier for Predicting Heart Disease with HyperOpt HPO Framework and LASSO FS Method," in IEEE Access, vol. 11, pp. 138201-138214, 2023, doi: 10.1109/ACCESS.2023.3339225. 
28.RetSchool Dataset: https://vincentarelbundock.github.io/Rdatasets/doc/Ecdat/RetSchool.html,
https://vincentarelbundock.github.io/Rdatasets/articles/data.html

29.Little, R.J.A., & Rubin, D.B. (2002). Statistical Analysis with Missing Data. John Wiley & Sons, Inc. "This seminal text provides foundational insights into the techniques and implications of handling missing data in statistical analysis, informing our methodology's approach to imputation and data completeness".

30.Tibshirani, R. (1996). "Regression Shrinkage and Selection via the Lasso." Journal of the Royal Statistical Society, Series B, 58(1), 267-288. Tibshirani's groundbreaking work on LASSO regression underpins our strategy for model selection and complexity reduction, ensuring precise and interpretable predictive modeling.

31.Aiken, L. S., West, S. G., & Reno, R. R. (1991). Multiple regression: Testing and interpreting interactions. Newbury Park, CA: Sage.

32.James, G., Witten, D., Hastie, T., & Tibshirani, R. (2013). An Introduction to Statistical Learning: with Applications in R. Springer.

33.Hancock, Gregory R., and Ralph O. Mueller. "Rethinking Construct Reliability Within Latent Variable Systems." In Structural Equation Modeling: A Second Course, edited by Gregory R. Hancock and Ralph O. Mueller, 2nd ed., 303-324. Charlotte, NC: Information Age Publishing,2013.

34.James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. An Introduction to Statistical Learning: with Applications in R. New York:Springer,2013.

35.Sun, Xiaowei, Michael Borenstein, and Julian P.T. Higgins. "Are Subgroup Analyses Necessary? Examining the Controversy." Journal of Clinical Epidemiology 128 (2020):142-147.

36.Aiken, Leona S., Stephen G. West, and Raymond R. Reno. Multiple Regression: Testing and Interpreting Interactions. Newbury Park,CA:Sage,1991.

37.Little, Roderick J. A., and Donald B. Rubin. Statistical Analysis with Missing Data. Wiley, 2002.

38. Hastie, Trevor, et al. The Elements of Statistical Learning: Data Mining, Inference, and Prediction. 2nd ed.,Springer,2009.

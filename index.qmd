---
title: "Innovative Data Exploration with LASSO: Unveiling Patterns in Earnings and Education"
author: "Nikhitha Amireddy, Ishrath Jahan, Sai Kumar Miryala, Muhammad Usman Aslam"
date: '`r Sys.Date()`'
format:
  html:
    code-fold: true
course: STA 6257 - Advanced Statistical Modeling
#bibliography: references.bib # file contains bibtex for references
always_allow_html: true # this allows to get PDF with HTML features
self-contained: true
execute: 
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
---

## Introduction

LASSO, Least Absolute Shrinkage and Selection Operator, is a penalized linear regression method that reduces the number of variables in the model, useful for avoiding overfitting. It works by minimizing the residual sum of squares subject to the sum of the absolute value of the coefficients being less than a constant, simplifying the model and potentially improving prediction accuracy.

In 1996, Robert Tibshirani [@Tibshirani1996] introduced LASSO regression to further the discussion about variable selection. This provided a new and sophisticated solution for the problems in the course of predictive modeling, especially when it came to big and complex datasets. It is therefore important that the model remains parsimonious in nature, explains easily interpretable phenomena, and reduces the risks associated with overfitting by applying a penalty to the absolute values of regression coefficients [@Zhao2006], [@Fan2011]. This innovation was therefore taken up by many scientific disciplines, characterizing the beginning of an era where high-dimensional datasets could be probed with more depth for complexities with better clarity [@Meinshausen2006] @Hastie2009\]. The popularity of LASSO regression has risen with the birth of Least Angle Regression by Efron et al. [@Efron2004] in 2004.

LASSO regression has particularly become a key instrument with which to insight the complex dynamics underlying socioeconomic trends, public health outcomes, and environmental studies—hence, significantly enriching the toolset available to the data scientist [@Belloni2013], [@Park2008]. Logistic regression is further extended by integration with the capabilities of LASSO, offering a refined method in prediction for the categorical outcomes [@Li2022]. This synergy provides a response to long-term barriers to predictive modeling such as overfitting and multicollinearity with a scope of applications in, in particular, economic forecasting and health diagnostics, amongst many more [@Sauerbrei2015].

The 21st century continues, and the data revolution heightens. A dataset in relation to the augmented domain complexity continues to pervade. Here, LASSO regression does not exist just as a statistical tool but as an innovation beacon assisting researchers with precision and simplicity within the dynamic landscape [@Chintalapudi2022]. The introduction of logistic regression into the area of LASSO has been especially dynamic, providing a nuanced approach to modeling of categorical outcomes [@Simon2013]. This is necessary for areas from epidemiology to finance [@Friedman2010]. This thus places this combination as a great leap forward in the direction of data-driven insights aimed at action, where traditional analytical methods might possibly face challenges of interpretation, firmly situating logistic regression with LASSO among the basic constitutions of modern statistical analysis and predictive modeling [@Hastie2009].

Indeed, the way of LASSO development has been marked by a series of important developments that pinpoint the historical trajectory, including the emergence of Least Angle Regression by Efron and colleagues in pointing out the potential, influence, and continued utility in research [@Zhou2022]. This cutting-edge technology is used throughout a wide spectrum of industries with the goal of dismantling complex data structures and, eventually, extracting meaningful patterns from trends as socioeconomic, public health, and environmental studies. LASSO regression adapted for logistic models was one of the best steps to ward off the impediments towards modeling categorical data with more precision [@Tibshirani2012].

## Literature Review

The versatility of LASSO regression is reflected in the extensive literature discussing its various applications. Notably, LASSO regression has made significant contributions to economics. For example, Zhou et al. (2022)[@Zhou2022] used the LASSO model to investigate what determines the growth of quality of life in China and found that the LASSO model is a good tool for selecting important economic predictors out of a large number of regressors. Thus, it makes important improvements on economic models useful for economic policy stakeholders and researchers in how to promote economic growth. The technique has also been critical for important developments in the area of bioinformatics.

Lu, et al. (2011) [@Lu2011] showed how LASSO regression can be applied to constructing predictive models — based on gene expressions and other high-throughput data — to understand the relationships between gene expressions and multiple phenotypes, which are critical for understanding genetic pathways and cell systems. In this example, the LASSO regression was used to build a microRNA-target regulatory network, which provides insight into gene regulation mechanisms, as well as applications in personalized medicine and in the understanding of genetic diseases [@Musoro2014]. LASSO regression has expedited the processing and analysis of large datasets in the environmental and energy sectors. Wang et al. (2017) [@Wang2017] detailed its use in predicting ship fuel consumption for diminished environmental impact and increased fuel efficiency in maritime operations.

LASSO regression aids in selecting the most informative variables that influence fuel usage and empowers the maritime industry's sustainability and operational performance efforts. LASSO regression has been a meaningful addition to the analysis of multi-factorial diseases and health outcomes in public health research. In an analysis of dietary intakes for breast cancer, McEligot et al. (2020) [@McEligot2020] showed that logistic LASSO can handle high-dimensional data to uncover the key drivers of health, and this has broad applicability in public health for unraveling and prioritizing the risk factors for a variety of illnesses. His role as a facilitator for the development of machine learning models (especially in feature selection) has been pivotal to improving the accuracy of those predictions as LASSO was proving its worthiness time and time again.

Muthukrishnan and Rohini (2016) [@Muthukrishnan2016] showcased this by demonstrating the ability of LASSO regression “to select the relevant feature attributes of predictive modelling for machine learning”. After gaining prominence within the traditional domain, the applications of LASSO regression have extended into new spheres such as financial modeling, where it assists analysts in singling out influential market predictors from large datasets characterized by volatility and noise, eventually translating into forecasts and investment strategies [@Friedman2010]. The adaptability and precision of LASSO regression within financial analysis underline its growing promise and popularity. Another field where LASSO regression's ability to parse through voluminous climate variable datasets to extract critical indicators of environmental change has been crucial is climate research and sustainability studies. The power of this application is not only a testament to the adaptability but also reflects the urgency in deploying LASSO regression in the face of challenges such as global warming and biodiversity loss, where parsing through complex environmental patterns becomes critical to devising strategies to mitigate its devastating impacts [@Chintalapudi2022].

Digital health technologies have opened a trove of opportunities for LASSO regression to analyze the complex interplay between individual lifestyle factors, genetic predispositions, and health outcomes. The inclusion of LASSO in this field of research is an acknowledgement that it is uniquely suited to handle complex, multidimensional data that is needed to understand and generate personalized medicine or public health interventions—a frontier of healthcare that holds promise for applications where LASSO's valuable data-driven insights can be channeled to address bespoke health solutions [@Simon2013] and Sohaee and Bohluli (2024b)[@2024b] employ Lasso polynomial regression to analyze the intricate relationship between socioeconomic, demographic, and technological factors and fatal traffic accidents, demonstrating its effectiveness in handling categorical variables and preventing overfitting. In their study on breast cancer screening adherence (Sohaee and Bohluli 2024a)[@2024a], they utilized Lasso regression to identify significant predictors from various socioeconomic factors, highlighting health disparities and advocating for targeted healthcare policies. Both studies underscore the importance of advanced statistical techniques, such as Lasso regression, in understanding complex datasets to inform policy decisions and improve outcomes in transportation safety and healthcare.

Efron et al. (2004)[@annalsofstatistics] introduced Least Angle Regression (LARS) as adept for high-dimensional data analysis, emphasizing its suitability for large datasets with thousands of predictors. LARS enables efficient feature selection and model building, particularly when adapted for Lasso regression. By combining LARS with Lasso, researchers can select relevant features, manage high-dimensional data, and improve prediction accuracy while mitigating multicollinearity and overfitting concerns. [@10341221] research introduced a novel approach to predicting heart disease by leveraging a sophisticated computer model. By integrating LASSO, a technique akin to a smart sieve for crucial information, with other methods, the accuracy of predictions is enhanced. It showcases how advanced computer methods can assist doctors in foreseeing health issues preemptively, potentially leading to improved prevention and treatment strategies for heart disease. Researchers analyzed extensive health data, identifying key factors using LASSO, and developed a highly effective predictive model. This could translate to earlier and personalized care for individuals at risk of heart disease, promising significant advancements in healthcare.

Indeed, these and many more diverse applications are a testament to the potency of LASSO regression as a powerful tool in the arsenal of researchers and practitioners across disciplines, as they grapple with increasingly high-resolution and large datasets. By simplifying complex models and identifying important predictors from large datasets, LASSO regression has cemented itself as an essential part of modern statistical analysis and predictive modeling [@Belloni2013; @Zou2005; @Witten2010; @Tibshirani2012].

## Methodology

### LASSO Regression

LASSO (Least Absolute Shrinkage and Selection Operator) regression, introduced by Robert Tibshirani in 1996, extends traditional linear regression by introducing a penalty on the size of coefficients. This regularization technique is invaluable in scenarios with numerous potential predictors, some of which might not significantly impact the outcome variable. [@Tibshirani1996]

#### Mathematical Formulation

The objective function of LASSO regression is described as follows:

$$
\min_{\beta} \left\{ \frac{1}{2n} \sum_{i=1}^{n} (y_i - \beta_0 - \sum_{j=1}^{p} \beta_j x_{ij})^2 + \lambda \sum_{j=1}^{p} |\beta_j| \right\}
$$

where $y_i$ represents the observed outcomes, $x_{ij}$ denotes the predictor variables, $\beta_j$ are the coefficients, $n$ is the number of observations, $p$ represents the number of predictors, and $\lambda$ is the regularization parameter controlling the degree of shrinkage applied to the coefficients.

The first term of the objective function is the residual sum of squares (RSS), measuring the model's fit to the data. The second term, the LASSO penalty, imposes a cost on model complexity by penalizing the absolute value of the coefficients. The regularization parameter, $\lambda$, crucially balances fitting the data well (minimizing RSS) and maintaining model simplicity by penalizing large coefficients[@Tibshirani1996; @Zhao2006].

##### Role of the Regularization Parameter ($\lambda$)

The selection of $\lambda$ is critical in LASSO regression. When $\lambda = 0$, LASSO regression becomes ordinary least squares regression, with no penalty on coefficient size. As $\lambda$ increases, the penalty's impact grows, driving more coefficients to zero and simplifying the model. This attribute makes LASSO particularly effective for variable selection, as it can identify and eliminate irrelevant predictors by setting their coefficients to zero[@Zhao2006].

-   **Bias-Variance Trade-off**: A small $\lambda$ results in low bias but high variance, as the model fits closely to the training data, which may lead to overfitting. Conversely, a larger $\lambda$ increases bias but reduces variance by simplifying the model, potentially causing underfitting.
-   **Model Selection and Cross-Validation**: The optimal $\lambda$ value is usually determined through cross-validation. K-fold cross-validation is a common method to assess the model's predictive performance across a range of $\lambda$ values, selecting the $\lambda$ that minimizes cross-validation error [@Friedman2010].

##### Computational Considerations

LASSO regression's implementation computationally hinges on numerical optimization techniques, as the penalty term's absolute value renders the optimization problem non-differentiable at zero. Algorithms such as coordinate descent are commonly used to solve the LASSO optimization challenge, particularly with high-dimensional data.

##### Advantages of LASSO Regression

The amalgamation of feature selection and regularization underpins LASSO's utility for models prioritizing prediction accuracy and simplicity. Its applications span various fields, including biomedical research, financial modeling, and social sciences, attesting to its versatility and efficacy in predictive analytics [@Hastie2009; @Meinshausen2006].

-   **Feature Selection**: By reducing some coefficients to exactly zero, LASSO achieves automatic feature selection, highlighting significant predictors [@Park2008].
-   **Model Interpretability**: Simplifying the model by removing irrelevant variables improves interpretability, making LASSO-regressed models more comprehensible [@Belloni2013].
-   **Mitigation of Multicollinearity**: LASSO effectively addresses multicollinearity among predictors by selecting a single variable from a group of highly correlated variables, thereby alleviating this concern [@Efron2004].

## Data Set Description

The `RetSchool` dataset, accessible from [R datasets](https://vincentarelbundock.github.io/Rdatasets/doc/Ecdat/RetSchool.html), encompasses a rich array of variables aimed at exploring the determinants of wages in 1976. This section delves into the dataset through sophisticated visualizations and best practices in data analysis, focusing exclusively on dataset characteristics without discussing specific modeling techniques like LASSO regression.

### Initial Data Exploration and Cleaning

Initial exploration of the `RetSchool` dataset revealed a diverse range of variables, necessitating meticulous data cleaning to ensure accuracy in our subsequent analyses. Missing values were addressed through imputation or removal, refining our dataset for a comprehensive examination.

```{r setup, include=FALSE}
library(tidyverse)
library(ggplot2)
library(corrplot)
library(gridExtra)
df <- read_csv("https://vincentarelbundock.github.io/Rdatasets/csv/Ecdat/RetSchool.csv", show_col_types = FALSE)
df_clean <- df %>%
  mutate(
    grade76 = ifelse(is.na(grade76), median(grade76, na.rm = TRUE), grade76),
    exp76 = ifelse(is.na(exp76), median(exp76, na.rm = TRUE), exp76)
  ) %>%
  na.omit()
cat("Rows after cleaning and imputation:", nrow(df_clean), "\n")

```

Distribution of Key Variables Histograms and density plots provide a nuanced view of the distributions of wages, education, and experience.

```{r wage-distribution, fig.cap="Figure 1: Wage Distribution in 1976"}
library(ggplot2)
library(patchwork)

# Wage Distribution in 1976
p_wage <- ggplot(df_clean, aes(x = wage76)) +
  geom_histogram(aes(y = ..density..), bins = 30, fill = "skyblue", color = "black") +
  geom_density(color = "red", fill = "red", alpha = 0.2) +
  labs(title = "Wage Distribution in 1976", x = "Wage", y = "Density") +
  annotate("text", x = Inf, y = Inf, label = "Right-skewed distribution", hjust = 1.1, vjust = 2, size = 3, color = "darkred")
p_wage

```

• Wage in 1976 (wage76): o The histogram for wage76 shows a right-skewed distribution, indicating that most individuals earned lower wages, with a few outliers earning significantly more. This pattern is typical for income data. o The box plot reveals a wide range of wages with several outliers on the higher end. The median wage is relatively low compared to the mean, further indicating the skewness towards lower wage values.

```{r education-distribution, fig.cap="Figure 2: Education Level Distribution in 1976"}

# Education Level Distribution in 1976
p_education <- ggplot(df_clean, aes(x = grade76)) +
  geom_histogram(aes(y = ..density..), bins = 30, fill = "green", color = "black") +
  geom_density(color = "blue", fill = "blue", alpha = 0.2) +
  labs(title = "Education Level Distribution in 1976", x = "Highest Grade Completed", y = "Density") +
  annotate("text", x = Inf, y = Inf, label = "Multi-modal distribution", hjust = 1.1, vjust = 2, size = 3, color = "darkblue")
p_education
```

• Education Level in 1976 (grade76): o The distribution of grade76 shows a multi-modal pattern, suggesting concentrations of individuals at specific education levels. The peaks likely represent completion of certain education milestones (e.g., high school, some college). o The box plot indicates a relatively normal distribution with a few outliers, suggesting most individuals had a similar level of education, with a median around high school completion.

```{r work-distribution, fig.cap="Figure 3: Work Experience Distribution in 1976"}

# Work Experience Distribution in 1976
p_experience <- ggplot(df_clean, aes(x = exp76)) +
  geom_histogram(aes(y = ..density..), bins = 30, fill = "orange", color = "black") +
  geom_density(color = "purple", fill = "purple", alpha = 0.2) +
  labs(title = "Work Experience Distribution in 1976", x = "Years of Experience", y = "Density") +
  annotate("text", x = Inf, y = Inf, label = "Right-skewed distribution", hjust = 1.1, vjust = 2, size = 3, color = "darkmagenta")
p_experience

```

• Work Experience in 1976 (exp76): o The histogram for exp76 also exhibits a right-skewed distribution, suggesting that a larger number of individuals had lower work experience, with fewer individuals having high levels of experience. o The box plot for exp76 shows the presence of outliers with exceptionally high experience levels, and a median experience level that indicates the majority had moderate to low work experience.

### Correlation Matrix

```{r correlation-matrix, fig.cap="Figure 4: Correlation Matrix of Selected Variables"}
library(corrplot)

# Calculate the correlation matrix on selected variables, omitting NAs
cor_matrix <- cor(select(df_clean, wage76, grade76, exp76, age76) %>% na.omit())

# Generate the correlation matrix plot
corrplot(cor_matrix, method = "color", 
         addCoef.col = "black",
         type = "upper",
         tl.col = "black",
         tl.srt = 45,
         tl.cex = 0.6,
         number.cex = 0.6,
         mar = c(0, 0, 2, 0))


```

The correlation matrix provides a nuanced understanding of how different variables interact. Notably, education level and work experience show positive correlations with wages, but the strength of these relationships varies, highlighting the multifaceted nature of wage determination.

### Demographic Distributions

```{r demographic-distributions, fig.cap="Figure 5: Demographic Distributions"}
# Create the 'LivingSituationAt14' column based on existing data

df_clean <- df_clean %>%
  mutate(LivingSituationAt14 = case_when(
    momdad14 == 1 ~ "Both Parents",
    sinmom14 == 1 ~ "Single Mom",
    TRUE ~ "Other"
  ))

# Convert the new column to a factor for better plotting
df_clean$LivingSituationAt14 <- factor(df_clean$LivingSituationAt14)

# Race Distribution
p_race <- ggplot(df_clean, aes(x = as.factor(black))) +
  geom_bar(fill = "coral") +
  labs(title = "Race Distribution", x = "Race (1 = Black, 0 = Other)", y = "Count")

# Region Distribution
p_region <- ggplot(df_clean, aes(x = as.factor(region))) +
  geom_bar(fill = "lightgreen") +
  labs(title = "Region Distribution", x = "Region", y = "Count")

# Living Situation at Age 14
p_living <- ggplot(df_clean, aes(x = LivingSituationAt14)) +
  geom_bar(fill = "lightblue") +
  labs(title = "Living Situation at Age 14", x = "Living Situation", y = "Count")

# Combine the plots with patchwork and assign a collective title
demographic_plots <- p_race + p_region + p_living + 
  plot_layout(ncol = 3)

print(demographic_plots)


```

The combined demographic analysis elucidates the complex interplay between race, region, and early family environment, offering valuable insights for targeted interventions and policies aimed at improving educational and economic opportunities.

### Comprehensive Data Set Insights

Our exploration of the RetSchool dataset through advanced visualizations reveals significant insights into the socio-economic landscape of 1976. Notably, the wage distribution underscores economic disparities, while the educational attainment distribution highlights the societal valuation of specific educational milestones. Work experience distribution and demographic analyses further enrich our understanding of the workforce's characteristics and the socio-economic factors influencing educational and economic outcomes.

## Data Preparation and Model Fitting

The dataset of interest captures various potential predictors of wages in 1976, encompassing education levels, work experience, and demographic variables such as race and geographic location. Effective modeling requires meticulous data preparation to ensure the quality and comparability of the input data. \### Data Cleaning To ensure the integrity and accuracy of our predictive model, we first tackled the issue of missing data—a factor that can significantly skew predictions if left unaddressed. Our approach to managing missing values was multifaceted, tailored to the nature and extent of the missingness within our dataset. This included strategies such as imputation for numerical columns, where missing entries were replaced with the median value of the respective column, and outright removal of rows where data remained incomplete. Through these careful cleaning and imputation steps, we refined our dataset down to 3,078 observations , each fully equipped with the necessary information for a robust analysis.

```{r data-cleaning}
library(dplyr)
df <- read_csv("RetSchool.csv", show_col_types = FALSE)

# Simple imputation for numerical columns
df_clean <- df %>%
  mutate_at(vars(grade76, exp76), ~ifelse(is.na(.), median(., na.rm = TRUE), .))

df_filtered <- na.omit(df_clean)

print(paste("Rows after cleaning and imputation:", nrow(df_filtered)))


```

### Feature Scaling

Scaling ensures that all variables contribute equally to the model, preventing features with larger scales from dominating the model's fit. glmnet expects input features to be centered and scaled:

```{r}
library(caret)

numeric_features <- select(df_filtered, where(is.numeric), -wage76)

features <- data.matrix(numeric_features)

# Scaling the features
preProcValues <- preProcess(features, method = c("center", "scale"))
features_scaled <- predict(preProcValues, features)


```

### Model Fitting with Cross-Validation

Leveraging the glmnet package, we embarked on fitting a LASSO regression model to our dataset. A pivotal step in this process involved employing cross-validation techniques to meticulously identify the most suitable value for the regularization parameter, $\lambda$. This parameter is instrumental in fine-tuning the balance between model complexity and prediction accuracy, thereby mitigating the risk of overfitting. Through this methodical approach, we determined the optimal $\lambda$ to be approximately 0.00488, a value that significantly enhances the model's predictive reliability while ensuring simplicity.

```{r lasso-model, echo=FALSE, message=FALSE, warning=FALSE, results='asis'}
library(glmnet)

# Assuming df_filtered and features_scaled are already defined
y <- df_filtered$wage76

set.seed(123)
cv_fit <- cv.glmnet(features_scaled, y, alpha = 1)

# Plotting the cross-validation result
plot(cv_fit)
```

### Explanation of the Cross-Validation Plot

The cross-validation plot serves as a crucial tool for visualizing how the choice of $\lambda$ influences our model's predictive accuracy. By plotting the mean squared error against the logarithm of $\lambda$ values, we gain insights into the model's performance dynamics across a spectrum of regularization strengths. Notably, the plot features two significant vertical lines:

-   A dotted line highlights the $\lambda$ value achieving the lowest cross-validation error within one standard error margin, suggesting an optimal balance between model complexity and accuracy. This "one standard error rule" helps in selecting a simpler model that still performs comparably to the best model found.
-   Another line, often more solid, directly points to the $\lambda$ that corresponds to the absolute minimum cross-validation error, indicating the point of highest accuracy without regard to model simplicity.

In our analysis, this graphical representation was instrumental in pinpointing a $\lambda$ value of approximately 0.00488 as the sweet spot for our model. It ensures a judicious blend of complexity and accuracy, steering clear of overfitting while maintaining robust predictive performance.

### Model Coefficients and Interpretation

After identifying the optimal $\lambda$, we can examine the model's coefficients to understand the influence of each predictor:

```{r}
coef_optimal <- coef(cv_fit, s = "lambda.min")
coef_dense <- as.matrix(coef_optimal)
coef_df <- data.frame(coef_dense)

# Displaying coefficients
print(coef_df)

```

Upon determining the optimal $\lambda$ value, a detailed examination of the model's coefficients provides us with valuable insights into the predictors' contributions towards estimating wages in 1976. The coefficients, represented in a sparse matrix, shed light on the relative importance and direction of each variable's effect:

-   **Intercept (1.658013288)**: Sets the baseline wage prediction when all other predictors are at their mean values.
-   **grade76 (0.083832188)**: Each additional year of education in 1976 is associated with an increase in wage, underscoring the value of education.
-   **black (-0.071115786)**: Being black is associated with a decrease in wage compared to non-black individuals, highlighting potential racial disparities.
-   **south76 (-0.059039330)**: Living in the South in 1976 is associated with a decrease in wage, possibly reflecting regional economic differences.
-   **smsa76 (0.064270413)**: Living in a Standard Metropolitan Statistical Area (SMSA) in 1976 is associated with an increase in wage, indicating urban wage premiums.
-   **smsa66 (0.006040491)**: Similarly, living in an SMSA in 1966 shows a smaller positive association with wages, suggesting long-term benefits of urban living.
-   **momdad14 (0.008504452)**: Living with both parents at age 14 is mildly associated with higher wages, possibly reflecting benefits of a stable early home environment.
-   **momed (0.010829506)**: Higher education levels of the mother are associated with higher wages, pointing to the influence of parental education.
-   **age76 (0.117310381)**: Age in 1976 shows a strong positive correlation with wages, reflecting the typical accumulation of experience and skills over time.
-   **col4 (0.007218061)**: Some indication that attending college is associated with higher wages, albeit with a smaller effect size. Variables such as **exp76**, **region**, **sinmom14**, **nodaded**, **nomomed**, **daded**, and **famed** have their coefficients shrunk to zero, indicating that within the context of this model and the data, these predictors do not have a significant impact on wage predictions. This outcome exemplifies the LASSO model's capacity for feature selection, focusing on predictors with meaningful contributions and simplifying the model by excluding those deemed less relevant.

### Visualizing Coefficient Paths

A pivotal aspect of understanding our LASSO regression model's intricacies involves examining how the predictor coefficients evolve across a range of $\lambda$ values. This exploration is facilitated through the visualization of coefficient paths:

```{r}
glmnet_fit <- glmnet(features_scaled, y, alpha = 1)
plot(glmnet_fit, xvar = "lambda", label = TRUE)

```

This graph delineates each predictor's coefficient trajectory as we incrementally adjust $\lambda$, the regularization strength. Notably, the visualization reveals several key dynamics:

-   As $\lambda$ escalates, the model increasingly penalizes coefficient magnitude, compelling the paths of numerous coefficients towards zero. This trend underscores LASSO's mechanism for enforcing sparsity and simplification within the model.
-   Variables deemed peripheral or less impactful in predicting wages experience a swift contraction to zero. This rapid decline illustrates LASSO's effective discernment in distinguishing between essential and non-essential predictors, based on their contribution to model accuracy.
-   In contrast, coefficients for variables with a robust association to wages—evident in their sustained deviation from zero—even at higher $\lambda$ values, affirm their fundamental role within the model. These paths not only highlight predictors integral to wage determination but also showcase the model's capacity to retain critical information despite the overarching pressure to simplify.

## Results

The analysis conducted through LASSO regression, underpinned by meticulous data preparation and feature scaling, yielded compelling insights into the determinants of wages in 1976.

### Optimal Regularization Parameter

The cross-validation process, pivotal for the model's integrity and predictiveness, identified an optimal regularization parameter ($\lambda$) of approximately 0.00488. This value signifies a balance, minimizing prediction error while avoiding overfitting, thereby ensuring the model's generalization capability.

### Significant Predictors

The model's coefficients at this optimal $\lambda$ highlight significant predictors influencing wage levels:

-   **Education (grade76)**: Demonstrating a positive relationship, indicating that each additional year of education is associated with a wage increase, underscoring the premium on educational attainment.
-   **Racial Background (black)**: Showing a negative association, reflecting wage disparities possibly tied to racial discrimination.
-   **Geographical Factors**: Both living in the South (`south76`) and urban areas (`smsa76`) in 1976 influenced wages, with urban dwelling associated with higher wages, suggesting location-based economic opportunities.
-   **Family Background and Age**: Variables like living with both parents at age 14 (`momdad14`) and the mother's education level (`momed`) were positively correlated with wages, along with `age76`, highlighting the role of familial support and the accrual of experience over time.

Notably, variables such as `exp76`, `region`, and `famed`, among others, saw their coefficients reduced to zero, indicating their limited impact within the predictive framework of this model.

## Conclusion

This study's application of LASSO regression has elucidated the multifaceted nature of wage determinants in 1976, revealing the significant roles played by education, racial background, geographical location, and family background. The optimal $\lambda$ identified through cross-validation underscores the importance of balancing model complexity with predictive accuracy, a testament to the LASSO method's robustness.

The findings not only contribute to our understanding of wage dynamics during the period but also underscore the value of regularization techniques like LASSO in distilling complex datasets to their most informative elements. Future research may build on this foundation, exploring additional variables, different time periods, or employing complementary analytical techniques to deepen our understanding of wage determinants.

## Recommendations

### Future studies should consider:

-   **Exploring Additional Variables:** Investigate other potential predictors not included in the current model to uncover more determinants of wage disparities.
-   **Temporal Comparisons:** Conduct similar analyses for different time periods to observe changes in wage determinants over time, providing historical wage trend insights.
-   **Employing Other Analytical Techniques:** Utilize complementary regression models or machine learning algorithms to validate findings and reveal additional insights.









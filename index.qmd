---
title: "Innovative Data Exploration with LASSO: Unveiling Patterns in Earnings and Education"
author: "Nikhitha Amireddy, Ishrath Jahan, Sai Kumar Miryala, Muhammad Usman Aslam"
date: '`r Sys.Date()`'
format:
  html:
    code-fold: true
course: STA 6257 - Advanced Statistical Modeling
bibliography: references.bib
knitr:
  opts_chunk:
    echo: true
    warning: false
    message: false
editor-options:
  markdown:
    wrap: 72
output: html_document
---

[Slides](Presentation.html)

## Introduction

LASSO, or Least Absolute Shrinkage and Selection Operator, is a refined
linear regression method introduced by Robert Tibshirani in 1996
[@Tibshirani1996]. This technique incorporates a penalty on the absolute
values of regression coefficients, effectively reducing the risk of
overfitting by eliminating less significant variables. LASSO's ability
to maintain model parsimony while enhancing interpretability makes it
particularly valuable in handling large and complex datasets across
various fields, including socioeconomic studies, public health, and
environmental research [@Zhao2006; @Fan2011].

The development of LASSO has been influenced significantly by
advancements such as Least Angle Regression (LARS) by Efron et al.
[@Efron2004], which further highlights the method's capability in
feature selection and regularization. This innovation has led to LASSO's
widespread adoption in disciplines requiring a clear interpretation of
intricate data dynamics, such as economic forecasting and health
diagnostics [@Hastie2009; @Belloni2013].

Moreover, the integration of logistic regression with LASSO has opened
new avenues for modeling categorical outcomes, enhancing the predictive
accuracy and interpretability in various applications from epidemiology
to finance [@Simon2013; @Li2022]. As the complexity of datasets
increases in the 21st century, LASSO remains a cornerstone in the
toolkit of data scientists, aiding in the precise and simplified
exploration of multifaceted data landscapes [@Chintalapudi2022;
@Friedman2010].

## Literature Review

LASSO regression's versatility across multiple fields illustrates its capability to manage complex datasets effectively, particularly with continuous outcomes. In the field of economics, Zhou et al. (2022)[@Zhou2022] demonstrated LASSO's effectiveness in isolating key economic predictors that are crucial for strategic decision-making. This application highlights its utility in economic analysis, where identifying factors that directly influence continuous outcomes like wages or economic growth is essential.

### Applications Across Fields

- **Economics**: Zhou et al. (2022)[@Zhou2022] highlighted LASSO’s ability to identify key economic predictors that assist in strategic decision-making. This example underscores its utility in economic analysis, where it helps to isolate factors that directly influence continuous economic outcomes like wages, prices, or economic growth.

- **Bioinformatics**: Lu et al. (2011)[@Lu2011; @Musoro2014] used LASSO regression to develop models based on gene expression data, advancing our understanding of genetic influences on continuous traits and diseases. Their work illustrates how LASSO can handle vast amounts of biological data to pinpoint critical genetic pathways.

- **Environmental Science**: Wang et al. (2017)[@Wang2017] applied LASSO to predict fuel consumption—a continuous variable—in maritime operations. This research supports sustainability efforts by providing precise predictions that help reduce environmental impact.

- **Public Health**: McEligot et al. (2020)[@McEligot2020] employed logistic LASSO to explore how dietary factors, which vary continuously, affect the risk of developing breast cancer. Their findings highlight LASSO's strength in dealing with complex, high-dimensional datasets in health sciences.

### Continuous Data Analysis

The consistent evolution of LASSO regression [@Muthukrishnan2016; @Friedman2010] reflects its increased adoption in fields that require robust statistical tools to analyze extensive and intricate data sets. Its effectiveness in managing continuous variables makes it particularly valuable in predictive modeling and data analysis.

### Model Optimization and Comparison

To ensure the accuracy and reliability of our LASSO regression model in predicting continuous wage outcomes, we implemented k-fold cross-validation[@James2013]. This technique is crucial for evaluating model performance on unseen data, helping to fine-tune the regularization parameter (lambda) which balances model complexity against predictive accuracy [@Hastie2009].

After refining our model, we compared its performance to that of Multiple Linear Regression (MLR). This comparison was essential to illustrate how each approach handles multicollinearity and overfitting, particularly in the context of continuous data [@Friedman2010]. The analysis of model coefficients showed that LASSO effectively simplifies the model while maintaining excellent predictive capability, even with many predictors or potential for overfitting [@Tibshirani1996].


Our comprehensive analysis underscores LASSO's utility in producing models that are more parsimonious compared to MLR, particularly in scenarios with numerous predictors and potential overfitting [@Tibshirani1996]. The results affirm the value of LASSO in modern statistical analysis and predictive modeling, as it enhances model interpretability and ensures robustness against the complexities inherent in large datasets.


## Methodology

### LASSO Regression

LASSO (Least Absolute Shrinkage and Selection Operator) regression, introduced by Robert Tibshirani in 1996, enhances traditional linear regression by adding a penalty for the size of the coefficients. This approach is particularly valuable in complex datasets with many potential predictors, some of which may not significantly influence the outcome. By penalizing the magnitude of the coefficients, LASSO helps in selecting only the most significant variables, simplifying the model and improving interpretability. [@Tibshirani1996]

### Mathematical Formulation

The core of LASSO regression is captured by the following objective function:

$$
\min_{\beta} \left\{ \frac{1}{2n} \sum_{i=1}^{n} (y_i - \beta_0 - \sum_{j=1}^{p} \beta_j x_{ij})^2 + \lambda \sum_{j=1}^{p} |\beta_j| \right\}
$$

This formula aims to minimize the residual sum of squares (RSS) while imposing a penalty proportional to the sum of the absolute values of the coefficients, regulated by the parameter $\lambda$. This balancing act helps manage model complexity and prevents overfitting, especially important in datasets with many predictors. [@Zhao2006; @Friedman2010]

### Understanding the Outcome Variable

A crucial aspect of our methodology was determining the nature of our outcome variable, `wage76`, which we identified as continuous. This was based on statistical analysis and visualization of the data, showing a range of wage values without distinct groupings or categories. This understanding is vital as it dictates the choice of LASSO regression, which is well-suited for models with continuous outcomes due to its ability to perform regularization and feature selection effectively.

### The Shrinkage Effect and Its Benefits

LASSO's dual capability of coefficient shrinkage and feature selection is instrumental in our analysis:

- **Feature Selection**: In high-dimensional datasets, irrelevant predictors can obscure true relationships. LASSO counters this by reducing the coefficients of non-essential predictors to zero, thus highlighting the variables that genuinely impact the outcome. [@Meinshausen2006]
- **Regularization**: This process helps in avoiding overfitting, making the model more generalizable and reliable for predicting new data. It is especially critical when dealing with complex models derived from large datasets. [@Hastie2009]

### Role of the Regularization Parameter ($\lambda$)

Selecting an appropriate $\lambda$ is key:

- At $\lambda = 0$, LASSO equals an ordinary least squares regression, offering no coefficient shrinkage.
- Increasing $\lambda$ enhances the penalty, pushing more coefficients to zero, which simplifies the model and focuses on the most impactful variables.
- The optimal $\lambda$ is usually determined through cross-validation, ensuring the model strikes the right balance between bias and variance. [@Friedman2010]

### Practical Application and Model Comparison

In practice, LASSO's ability to discern key predictors makes it invaluable across various fields, including finance and healthcare, where precise models can significantly impact decision-making.

Additionally, comparing LASSO with Multiple Linear Regression (MLR) illustrates its superiority in handling datasets with numerous predictors. While MLR provides a baseline by fitting data without regularization, LASSO advances this by selectively including only the most relevant variables, thereby preventing overfitting and enhancing the model's predictive accuracy. [@Park2008; @Belloni2013]


### Advantages of LASSO Regression

LASSO regression is highly valued in fields ranging from healthcare to finance due to its ability to simplify complex models without sacrificing accuracy. The method's key strengths include:

- **Feature Selection**: LASSO can set some coefficients exactly to zero, effectively choosing the most relevant variables from many possibilities. This automatic feature selection helps focus the model on the truly impactful factors. [@Park2008]
- **Model Interpretability**: By eliminating irrelevant variables, LASSO makes the resulting models easier to understand and communicate, enhancing their practical use. [@Belloni2013]
- **Mitigation of Multicollinearity**: LASSO addresses issues that arise when predictor variables are highly correlated. It selects one variable from a group of closely related variables, which simplifies the model and avoids redundancy. [@Efron2004]

### Cross-Validation for Model Tuning

To ensure that our LASSO model performs well on new, unseen data, we used a technique called k-fold cross-validation. This approach tests the model's effectiveness across different subsets of the dataset:

- **Methodology**: We divide the data into 'k' equal parts, or folds. The model is trained on 'k-1' of these folds, with the remaining part used as a test set. This process repeats so that each fold serves as a test set once, allowing us to use all data for both training and validation.
- **Optimization**: We calculate the cross-validation error for each value of the regularization parameter $\lambda$. This error is the average of the errors obtained from each fold and helps us find the best $\lambda$ that minimizes these errors:

$$
CVE(\lambda) = \frac{1}{k} \sum_{i=1}^{k} MSE_i(\lambda)
$$

Where $MSE_i(\lambda)$ represents the mean squared error on the ith fold. [@Hastie2015; @Tibshirani2021]

### Comparing LASSO Regression to Multiple Linear Regression

To highlight LASSO's effectiveness, we compare it with Multiple Linear Regression (MLR), which models the relationship between a dependent variable and multiple predictors without regularization:

$$
y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \ldots + \beta_n x_n + \epsilon
$$

MLR provides a baseline by estimating how each predictor affects the dependent variable, assuming all other variables are held constant. Here's how MLR breaks down:

- **Intercept ($\beta_0$)**: Represents the expected value of $y$ when all predictors are zero.
- **Coefficients ($\beta_1, \beta_2, ..., \beta_n$)**: Each coefficient indicates the change in $y$ associated with a one-unit change in the respective predictor.
- **Error Term ($\epsilon$)**: Captures the variability in $y$ not explained by the predictors.

In scenarios with high-dimensional data, LASSO's ability to reduce some coefficients to zero provides a clear advantage over MLR. It simplifies the model, which can improve both interpretability and prediction accuracy, avoiding the common pitfalls of overfitting that MLR might suffer from.


## Analysis and Results

### Data Description

Understanding the variables in the `RetSchool` dataset is crucial for our analysis. These variables give us insights into the socio-economic and educational environment of 1976, helping us explore factors that influence wages and educational outcomes. Below is a table summarizing the key variables and their roles:

| Variable   | Description   | Type    | Relevance |
|------------|---------------|---------|-----------|
| `wage76`   | Wages of individuals in 1976 | Continuous | Primary measure of economic status, used to explore wage disparities. |
| `age76`    | Age of individuals in 1976 | Continuous | Helps analyze the age distribution's impact on wages. |
| `grade76`  | Highest grade completed by 1976 | Continuous | Indicates educational attainment and its correlation with economic success. |
| `col4`     | Whether individuals received college education by 1976 | Binary | Differentiates the impact of higher education on wages. |
| `exp76`    | Years of work experience by 1976 | Continuous | Examines how work experience influences wages. |
| `momdad14` | Whether lived with both parents at age 14 | Binary | Assesses the impact of family structure on early life outcomes. |
| `sinmom14` | Whether lived with a single mother at age 14 | Binary | Similar to `momdad14`, focuses on single-mother households. |
| `daded`    | Education level of father | Continuous | Explores how paternal education affects offspring's outcomes. |
| `momed`    | Education level of mother | Continuous | Similar to `daded`, but focuses on maternal education. |
| `black`    | Whether individuals are identified as black | Binary | Used to analyze racial disparities within the dataset. |
| `south76`  | Whether individuals resided in the South in 1976 | Binary | Important for regional economic analysis. |
| `region`   | Geographic region classification | Categorical | Enhances the analysis of regional influences on outcomes. |
| `smsa76`   | Residence within a Standard Metropolitan Statistical Area in 1976 | Binary | Relevant for examining urban versus rural disparities. |

This table categorizes the variables based on their type and relevance to our study, setting the stage for a detailed exploration of how these factors interplay to shape economic outcomes during the mid-1970s.

**Data Cleaning and Exploration**


Initial exploration of the `RetSchool` dataset revealed a diverse range of variables, necessitating meticulous data cleaning to ensure accuracy in our subsequent analyses. Missing values were addressed through imputation or removal, refining our dataset for a comprehensive examination.


```{r setup, include=TRUE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE, results = 'hide')
library(tidyverse)
library(ggplot2)
library(corrplot)
library(gridExtra)
df <- read_csv("https://vincentarelbundock.github.io/Rdatasets/csv/Ecdat/RetSchool.csv", show_col_types = FALSE)
df_clean <- df %>%
  mutate(
    grade76 = ifelse(is.na(grade76), median(grade76, na.rm = TRUE), grade76),
    exp76 = ifelse(is.na(exp76), median(exp76, na.rm = TRUE), exp76)
  ) %>%
  na.omit()
cat("Rows after cleaning and imputation:", nrow(df_clean), "\n")
# Define the response variable y
y <- df_clean$wage76
```

### Distribution of Key Variables

### Work Experience Distribution

```{r work-distribution, fig.cap="Figure 1: Work Experience Distribution in 1976"}
library(ggplot2)
# Generate the plot
p_experience <- ggplot(df_clean, aes(x = exp76)) +
  geom_histogram(aes(y = ..density..), bins = 30, fill = "orange", color = "black") +
  geom_density(color = "purple", fill = "purple", alpha = 0.2) +
  labs(title = "Work Experience Distribution in 1976", x = "Years of Experience", y = "Density")
p_experience
# Save the plot as an image
ggsave("p_experience.png", plot = p_experience, width = 10, height = 5, dpi = 300)
```



The right-skewed distribution of `exp76` in the `RetSchool` dataset indicates a predominantly young and less experienced workforce in 1976. This skewness suggests a workforce that was entering or early in their careers. For more details, see [Figure 1: Work Experience Distribution in 1976](/p_experience.png) the distribution of work experience in 1976 indicates a predominantly young workforce:

- **Younger Workforce**: A significant entry of younger individuals into the job market, possibly influenced by demographic shifts or growth in new industries.
- **Impact on Wages**: Lower experience levels correspond with lower wages, contributing to observed wage disparities.
- **Economic Context**: The skew provides insights into the economic environment of the era, reflecting labor market conditions and potential impacts of educational and economic policies.

#### Wage Distribution Analysis


The histogram and density plot for `wage76` reveal the distribution of wages among workers in 1976, showing a typical pattern that indicates economic inequalities [Figure 2: Wage Distribution in 1976](p_wage.png):

- **General Trend**: The distribution suggests that most workers earned lower wages, while a smaller group had significantly higher earnings. This spread highlights the income disparities among different economic groups.
  
- **Skewness**: A right-skewed distribution indicates that while the majority of the workforce earned below the median wage level, there was a smaller segment with much higher wages. This skewness is critical for understanding the extent of wage disparities.
  
- **Economic Well-Being Insights**: By examining where most wages lie, we gain insights into the economic well-being of the population. This analysis provides a clearer picture of the economic conditions in 1976, reflecting the standard of living and financial stability of the workers during that time.


#### Correlation Matrix

```{r correlation-matrix,  echo=FALSE, message=FALSE, warning=FALSE, fig.cap="Figure 3: Correlation Matrix of Selected Variables"}
library(corrplot)
library(dplyr)

# Calculate the correlation matrix for selected variables, omitting missing values
df_selected <- df_clean %>% select(wage76, grade76, exp76, age76) %>% na.omit()
cor_matrix <- cor(df_selected)

# Open a PNG device
png(filename = "corrplot_plot.png", width = 1000, height = 800, res = 300)

# Generate and display the correlation matrix plot
corrplot(cor_matrix, method = "color", 
         addCoef.col = "black",  
         type = "upper",         
         tl.col = "black",       
         tl.srt = 45,            
         tl.cex = 0.6,           
         number.cex = 0.6,       
         mar = c(0, 0, 2, 0))

# Close the device to save the plot
dev.off()
corrplot(cor_matrix, method = "color", 
         addCoef.col = "black",  
         type = "upper",         
         tl.col = "black",       
         tl.srt = 45,            
         tl.cex = 0.6,           
         number.cex = 0.6,       
         mar = c(0, 0, 2, 0))
```
The correlation matrix is a powerful tool used to visualize and quantify the relationships between key variables such as `wage76`, `grade76`, `exp76`, and `age76` in the `RetSchool` dataset. Here’s what we learn from it [Figure 3: Correlation Matrix of Selected Variables](corrplot_plot.png):

- **Visual and Statistical Insights**: The matrix uses color intensity to indicate the strength of the relationships between variables. Correlation coefficients displayed in the matrix range from -1 to 1, where values near ±1 indicate strong positive or negative correlations, and values near 0 indicate weak or no linear relationship.

- **Influential Factors on Wages**: The correlations help identify significant predictors of wages:
  - A strong positive correlation between `wage76` and `grade76` suggests that higher educational attainment is likely linked to higher wages.
  - The relationship between `wage76` and `exp76` indicates how accumulated work experience correlates with wage levels, potentially showing that more experience leads to higher earnings.

- **Economic Analysis Applications**: By analyzing these correlations, we gain insights into the economic dynamics affecting the workforce in 1976. This helps in understanding how education, experience, and age were interacting to shape the economic landscape and influence wage disparities at that time.

**Why LASSO for the RetSchool Dataset?**


LASSO regression is particularly well-suited for our study on wage disparities due to its robust features that simplify complex data analysis:

- **Feature Selection**: Our initial look at the data showed many variables that could affect wages. LASSO helps by automatically selecting the most important factors, such as education level and region. This selection makes the model easier to understand and focuses on what really affects wages. [@Zhao2006]

- **Handling Multicollinearity**: Our analysis indicated that some variables, like education and work experience, might be overlapping in their effects on wages. LASSO tackles this issue by reducing the influence of less critical variables to zero. This is key to making sure our model remains stable and reliable. [@Tibshirani1996]

- **Simplicity and Clarity**: We want our model not just to be accurate, but also easy to interpret. LASSO strikes a good balance by simplifying the model, which helps in formulating clear and actionable insights for wage-related policies. [@Fan2011]

- **Predictive Accuracy**: Beyond just analyzing historical data, we aim to predict future trends accurately. LASSO improves the model’s ability to perform well with new, unseen data by avoiding overfitting. We use a method called k-fold cross-validation to fine-tune the model, ensuring it predicts accurately outside our sample. [@James2013]

- **Comparison with Traditional Regression**: We compared LASSO with regular linear regression to test its effectiveness. The results showed that LASSO was better at managing complex issues like multicollinearity and selecting key features, which are essential for robust analysis of wage factors. 

### Understanding Wages as a Continuous Variable

Through our LASSO analysis, we also confirmed that `wage76`, our main variable of interest, is continuous. This was evident as LASSO helped highlight the relationships between wages and various predictors without breaking them into arbitrary categories, preserving the natural continuous scale of wage data. This continuous perspective is crucial for accurate modeling and meaningful conclusions about how various factors influence wages.

Given these strengths, LASSO regression is an ideal choice for navigating the complexities of the RetSchool dataset, providing a clear, robust model that is well-suited for making informed decisions on wage policy and understanding the economic landscape of 1976.


## Statistical Modeling

Given the robust nature of LASSO (Least Absolute Shrinkage and Selection Operator) regression for handling complex datasets, it becomes an indispensable tool in our analysis of the RetSchool dataset. Here, we focus on refining our approach through meticulous data preparation and precise model fitting.

### Understanding Key Variables

The main goal of our analysis is to figure out what drives differences in wages in 1976. To do this, we look at various socio-economic factors that might impact wages.

#### Predictor Variables:

- **Educational Background (`grade76`, `col4`)**: Education level, which we expect to significantly affect wages. Higher education often correlates with higher earnings.
- **Work Experience (`exp76`)**: The number of years a person has worked, which should relate directly to their skills and salary potential.
- **Demographic and Regional Factors (`age76`, `black`, `south76`, `region`, `smsa76`)**: Includes age, race, geographical location, and whether a person lived in an urban area, all of which can influence wages.

#### Target Variable:

- **Wage (`wage76`)**: This is the main variable we are studying—it represents the income of individuals in 1976 and varies continuously across the dataset.

### Visualizing Key Variables

Understanding how these variables interact is easier when we visualize their distributions and relationships [Figure 4:Visualizations of Key Variables](d_plot.png).

```{r visualizations-key-variables, fig.cap="Figure 4: Visualizations of Key Variables", fig.width=10, fig.height=5}
library(ggplot2)
library(patchwork)

# Wage Distribution
p_wage <- ggplot(df_clean, aes(x = wage76)) +
  geom_histogram(bins = 30, fill = "blue", color = "black", alpha = 0.7) +
  labs(title = "Distribution of Wages in 1976", x = "Wage", y = "Frequency")

# Education Distribution
p_education <- ggplot(df_clean, aes(x = grade76)) +
  geom_histogram(bins = 12, fill = "green", color = "black", alpha = 0.7) +
  labs(title = "Distribution of Education Levels in 1976", x = "Education Level", y = "Frequency")

# Experience Distribution
p_experience <- ggplot(df_clean, aes(x = exp76)) +
  geom_histogram(bins = 30, fill = "red", color = "black", alpha = 0.7) +
  labs(title = "Distribution of Work Experience in 1976", x = "Years of Experience", y = "Frequency")

# Combine plots using patchwork
d_plot <- p_wage + p_education + p_experience + plot_layout(ncol = 1)

# Display the combined plot
d_plot

# Save the combined plot
ggsave("d_plot.png", plot = d_plot, width = 10, height = 5, dpi = 300)
```



### Data Preparation and Feature Scaling

Feature scaling is a critical preparatory step in our analysis, essential for the application of LASSO regression. LASSO's approach to regularization is sensitive to the scale of variables, as it penalizes the absolute size of the regression coefficients. This sensitivity can lead to disproportionate influence from variables with larger numeric ranges, skewing the model and potentially leading to misleading conclusions about the factors influencing wages.

```{r feature-scaling-setup, include=FALSE}
library(caret)
library(glmnet)

# Selecting only numeric features and excluding the target variable 'wage76'
numeric_features <- select(df_clean, where(is.numeric), -wage76)

# Converting the selected features into a matrix, as required by glmnet
features <- data.matrix(numeric_features)
preProcValues <- preProcess(features, method = c("center", "scale"))
features_scaled <- predict(preProcValues, features)

```

**Demonstrating the Impact of Feature Scaling**
We focus specifically on scaling `exp76`—years of experience—because it is a key predictor of wages, which is our continuous outcome variable. Proper scaling is vital to fairly assess how work experience impacts wages, alongside other factors such as education and demographic variables.

To illustrate the importance of this process, consider the variable `exp76`. Before scaling, `exp76` might display a wide range of values that reflect the diverse work experiences in our dataset. However, without scaling, this range could disproportionately affect the LASSO model’s ability to apply regularization fairly across all predictors [Figure 5: Distribution of Feature 'exp76' Before and After Scaling](combined_plot.png).

```{r data-feature-visualization, fig.cap="Figure 5: Distribution of Feature 'exp76' Before and After Scaling",fig.width=10, fig.height=5, lable="fig:data-feature-visualization"}
library(ggplot2)
library(gridExtra)  # Ensure this package is installed for plot_layout

# Selecting a feature for comparison
feature_name <- "exp76"  # Example feature

# Plotting the distribution before scaling
p_before_scaling <- ggplot(df_clean, aes(x = get(feature_name))) +  # Using get() for dynamic variable name
  geom_histogram(binwidth = 1, fill = "#F8766D", color = "#FFFFFF", alpha = 0.8) +
  labs(title = paste("Distribution of", feature_name, "Before Scaling"),
       x = "Years of Experience",
       y = "Frequency") +
  theme_minimal() +
  theme(plot.title = element_text(size = 14, face = "bold", hjust = 0.5),
        axis.title.x = element_text(size = 12),
        axis.title.y = element_text(size = 12),
        axis.text.x = element_text(size = 11),
        axis.text.y = element_text(size = 11),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        plot.margin = unit(c(1, 1, 1, 1), "lines"))  # Increase margins around the plot

# Extracting the scaled values for the selected feature
scaled_feature <- features_scaled[, feature_name]

# Plotting the distribution after scaling
p_after_scaling <- ggplot(data.frame(scaled_feature = scaled_feature), aes(x = scaled_feature)) +
  geom_histogram(binwidth = 0.2, fill = "#00BFC4", color = "#FFFFFF", alpha = 0.8) +
  labs(title = paste("Distribution of", feature_name, "After Scaling"),
       x = "Scaled Years of Experience",
       y = "Frequency") +
  theme_minimal() +
  theme(plot.title = element_text(size = 14, face = "bold", hjust = 0.5),
        axis.title.x = element_text(size = 12),
        axis.title.y = element_text(size = 12),
        axis.text.x = element_text(size = 11),
        axis.text.y = element_text(size = 11),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        plot.margin = unit(c(1, 1, 1, 1), "lines"))  # Maintain consistent margins

# Combine the plots side by side
combined_plot <- p_before_scaling + p_after_scaling + plot_layout(ncol = 2)
print(combined_plot)
ggsave("combined_plot.png", plot = combined_plot, width = 10, height = 5, dpi = 300)


```

After scaling, `exp76` is transformed to have a standard scale similar to other variables in the model. This normalization allows for a fair comparison and interaction within the model, ensuring that no single variable unduly influences the outcome due to its scale. 

The transformation that occurs when features like `exp76` are normalized is pivotal for models like LASSO regression, which rely on regularization techniques sensitive to the scale of input variables. By ensuring that the regularization penalty is applied uniformly across all features, scaling enhances the model's ability to identify truly significant predictors and avoids undue influence from variables simply because of their scale.

This step is crucial for maintaining the integrity and accuracy of our analysis, particularly in studies focused on understanding continuous outcomes such as wage disparities, where the precise quantification of each variable’s impact is essential.

**Visualizing the Impact of ($\lambda$) on Model Performance**

The regularization parameter ($\lambda$) in LASSO regression critically influences model accuracy by balancing model complexity and predictive performance. Selecting the optimal ($\lambda$) is essential to minimize overfitting and ensure the model generalizes well [Figure 6: Cross-Validation CurveFigure 6: Cross-Validation Curve](Cross_Validation_plot.png).


```{r cross-validation-visualization, echo=TRUE, fig.cap="Figure 6: Cross-Validation Curve",fig.width=10, fig.height=5}
library(glmnet)
library(ggplot2)
library(dplyr)

set.seed(123) # For reproducibility
cv_outcome <- cv.glmnet(features_scaled, y, alpha = 1, nfolds = 10)

# Creating a data frame from the glmnet cross-validation output
cv_data <- as.data.frame(cv_outcome$cvm)
names(cv_data) <- c("mse")
cv_data$lambda <- log(cv_outcome$lambda)
cv_data$lambda_min <- log(cv_outcome$lambda.min)
cv_data$lambda_1se <- log(cv_outcome$lambda.1se)

# Plotting with ggplot2
Cross_Validation_plot <- ggplot(cv_data, aes(x = lambda, y = mse)) +
  geom_line(color = "blue", linewidth = 1) +
  geom_vline(xintercept = cv_data$lambda_min, linetype = "dashed", color = "red", linewidth = 1) +
  geom_vline(xintercept = cv_data$lambda_1se, linetype = "dashed", color = "green", linewidth = 1) +
  labs(title = "Cross-Validation Curve for LASSO Regression",
       subtitle = "Dashed lines represent the lambda.min and lambda.1se",
       x = "Log(Lambda)",
       y = "Mean Squared Error",
       caption = "Red: lambda.min, Green: lambda.1se") +
  theme_minimal() +
  theme(plot.title = element_text(size = 20, face = "bold"),
        plot.subtitle = element_text(size = 14),
        plot.caption = element_text(size = 12),
        axis.title = element_text(size = 16),
        axis.text = element_text(size = 12))
Cross_Validation_plot
ggsave("Cross_Validation_plot.png", plot = Cross_Validation_plot, width = 10, height = 5, dpi = 300)
```

### Role of Cross-Validation in Determining ($\lambda$)

Cross-validation is essential for selecting the right ($\lambda$) in LASSO regression. It helps balance the model by preventing overfitting with low ($\lambda$) values and underfitting with high ($\lambda$) values. This method splits the data into subsets, testing how each ($\lambda$) performs, thus ensuring the model's effectiveness on new, unseen data.

### Optimal ($\lambda$) Choices

Cross-validation identifies crucial ($\lambda$) values for optimal model performance:

- **Optimal ($\lambda$) (`lambda.min`)**: Achieves the lowest mean squared error (MSE), indicating the best balance between accuracy and complexity.
- **Conservative ($\lambda$) (`lambda.1se`)**: Slightly higher than `lambda.min`, it provides a simpler model that is robust and stable, useful for generalizing well to new data.

### Model Coefficients and Interpretation

The optimized LASSO model clearly delineates which factors significantly impact wages, simplifying the analysis by reducing less impactful predictors. This streamlined approach not only enhances interpretability but also focuses on the most influential variables affecting wages in 1976.

[Figure 7: Visualization of LASSO Coefficients](cv_outcome_plot.png) visualizes these coefficients, highlighting the relative importance of each predictor in the context of wage determination.


```{r optimal-coefficients-visualization, echo=TRUE, message=FALSE, warning=FALSE, fig.cap="Figure 7: Visualization of LASSO Coefficients",fig.width=10, fig.height=5}
library(knitr)
library(glmnet)

# Ensure the cv_outcome object is available and correctly specified
if (exists("cv_outcome")) {
  # Extract coefficients using a safe method
  coef_optimal <- tryCatch({
    coef(cv_outcome, s = "lambda.min", exact = FALSE)
  }, error = function(e) {
    message("Failed to extract coefficients: ", e$message)
    NULL  # Return NULL if there's an error
  })

  if (!is.null(coef_optimal) && inherits(coef_optimal, "dgCMatrix")) {
    # Convert the matrix to a data frame for better handling
    coef_matrix <- as.matrix(coef_optimal)
    coef_df <- as.data.frame(coef_matrix, stringsAsFactors = FALSE)
    coef_df$Variable <- rownames(coef_matrix)  # Add variable names as a new column
    rownames(coef_df) <- NULL  # Clean up row names

    # Rename the coefficient column for clarity
    names(coef_df)[1] <- "Coefficient"
    cv_outcome_plot <- ggplot(coef_df, aes(x = reorder(Variable, Coefficient), y = Coefficient, fill = Coefficient > 0)) +
        geom_col() +
        coord_flip() +  # Makes the plot horizontal for better readability
        labs(title = "Visualization of LASSO Coefficients", x = "Predictors", y = "Coefficient Value") +
        scale_fill_manual(values = c("red", "blue"), name = "Sign of Coefficient",
                          labels = c("Negative", "Positive")) +
        theme_minimal()
  } else {
    message("Coefficient data is not available or invalid.")
  }
} else {
  message("cv_outcome is not available. Please check model fitting.")
}
ggsave("cv_outcome_plot.png", plot = cv_outcome_plot, width = 10, height = 5, dpi = 300)
```

**Significant Predictors and Their Effects**

The LASSO model has highlighted key factors that influence wages, efficiently pinpointing the most impactful variables:

- **Baseline Wages**: The intercept sets the starting point for wage predictions, adjusted for average levels of predictors.
- **Educational Attainment (`grade76`)**: Higher education levels are strongly associated with increased wages, confirming the significant return on investment in education.
- **Racial Disparity (`black`)**: There is a noticeable wage gap affecting black individuals, indicating persistent racial inequalities in earnings.
- **Geographic Influence (`south76`)**: Residing in the South is linked to lower wages, reflecting regional economic differences.
- **Urban Premium (`smsa76`)** and **Long-term Urban Advantage (`smsa66`)**: Both highlight the wage benefits of living in urban areas, both currently and historically.
- **Family Stability (`momdad14`)**: Growing up with both parents is correlated with higher wages, suggesting the economic benefits of a stable family during childhood.
- **Parental Education (`momed`)**: Higher maternal education levels positively affect wages, underscoring the influence of parental education.
- **Age and Experience (`age76`)**: Older age, often accompanied by more experience, typically leads to higher wages, reflecting the value of longevity in the workforce.
- **Higher Education (`col4`)**: Possessing a college degree shows a positive but modest correlation with higher wages.


**Variables with Minimal Impact**

Several variables demonstrated minimal to no influence on wages,
exemplifying Lasso's ability to streamline the model by eliminating
non-impactful predictors. These include `exp76`, `region`, `sinmom14`,
`nodaded`, `nomomed`, `daded`, and `famed`. This reduction in variables
enhances the interpretability of our model without compromising the
accuracy of our predictions.

## Results & Conclusion

### Comparative Analysis of Coefficient Impact

Our analysis with the LASSO model has effectively highlighted the most significant factors influencing wages, using a method that focuses on simplicity and accuracy by penalizing less impactful predictors. However, to deepen our understanding of these results, we also employed Multiple Linear Regression (MLR) as a comparative tool. This step allows us to observe the differences in how each model handles the complexity of the dataset and the continuous nature of the wage variable.



```{r mlr-model}
library(glmnet)
library(caret)
library(knitr)
library(kableExtra)

# Ensure df_clean is prepared as earlier described
df_clean <- read_csv("RetSchool.csv", show_col_types = FALSE) %>%
  mutate(grade76 = ifelse(is.na(grade76), median(grade76, na.rm = TRUE), grade76),
         exp76 = ifelse(is.na(exp76), median(exp76, na.rm = TRUE), exp76)) %>%
  na.omit()

# Fitting Multiple Linear Regression (MLR)
model_mlr <- lm(wage76 ~ ., data = df_clean)
# Extract coefficients from the MLR model
coefficients_mlr <- coef(model_mlr)

# Fitting LASSO model
set.seed(123)  # for reproducibility
features_scaled <- as.matrix(df_clean[setdiff(names(df_clean), "wage76")])
cv_outcome <- cv.glmnet(features_scaled, df_clean$wage76, alpha = 1)

# Extracting coefficients from the LASSO model using predict with s="lambda.min"
lasso_coefs <- predict(cv_outcome, type="coefficients", s = "lambda.min")[1:ncol(features_scaled),,drop=FALSE]

# Filter non-zero coefficients
non_zero_lasso <- lasso_coefs[lasso_coefs[,1] != 0, , drop=FALSE]

# Align MLR coefficients with non-zero LASSO coefficients
non_zero_names <- rownames(non_zero_lasso)
matching_mlr_coefs <- coefficients_mlr[non_zero_names]

# Create a data frame for comparison
coefficients_comparison <- data.frame(
  Predictor = non_zero_names,
  Coefficient_MLR = as.numeric(matching_mlr_coefs),
  Coefficient_LASSO = as.numeric(non_zero_lasso)
)

# Display the comparison using knitr
kable(coefficients_comparison, format = "html", caption = "Comparison of MLR and LASSO Coefficients", align = 'c') %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = F, font_size = 12) %>%
  scroll_box(width = "100%", height = "500px")
```

Comparing LASSO to MLR provides several benefits:

- **Baseline Comparison**: MLR, which does not include a regularization term, serves as a baseline to appreciate how much LASSO's penalties affect the coefficients of each predictor.
- **Insight into Overfitting**: By observing how predictors behave in MLR, which is prone to overfitting especially in datasets with many variables, we can better understand the necessity and effectiveness of LASSO’s regularization approach.
- **Variable Importance**: This comparison clearly delineates which variables are genuinely important for predicting wages, as significant predictors in MLR that are penalized to zero in LASSO may not be as crucial as initially thought.

### Significant Predictors of Wages

The analysis of both LASSO and MLR models sheds light on the robustness of each predictor's impact on wages. Here, we provide a side-by-side view of the coefficients, illustrating how each model values the same predictors:

- **Consistency and Differences**: Where LASSO might zero out a predictor, MLR may still attribute it with a significant coefficient. Such differences are key in understanding the potential for overfitting in MLR versus the more conservative and potentially more reliable predictions from LASSO.
- **Focus on Continuous Outcomes**: Both models emphasize the importance of continuous predictors like `age76` and `grade76`, but LASSO does so while maintaining model parsimony, avoiding the pitfalls of including too many variables as MLR might.

**Visual Comparison of Model Outcomes**

Visualizing the differences between the coefficients in Multiple Linear Regression (MLR) and LASSO offers a clear, intuitive understanding of how regularization in LASSO affects each predictor's influence compared to traditional regression methods.

```{r visualization, echo=FALSE, echo=FALSE, message=FALSE, warning=FALSE, fig.cap="Figure 8: Comparison of Coefficients from MLR and LASSO",fig.width=10, fig.height=7.5}
library(ggplot2)
library(dplyr)

# Assuming coefficients_comparison is a dataframe containing coefficients from both models
coefficients_long <- coefficients_comparison %>%
  pivot_longer(cols = c(Coefficient_MLR, Coefficient_LASSO), names_to = "Model", values_to = "Coefficient")

Predictors_plot <- ggplot(coefficients_long, aes(x = Predictor, y = Coefficient, fill = Model)) +
  geom_bar(stat = "identity", position = position_dodge()) +
  labs(title = "Coefficient Comparison between MLR and LASSO",
       x = "Predictors",
       y = "Coefficient Value") +
  scale_fill_manual(values = c("skyblue", "orange"), labels = c("MLR", "LASSO")) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 65, hjust = 1))
ggsave("Predictors_plot.png", plot = Predictors_plot, width = 10, height = 5, dpi = 300)
```

The chart below, titled "[Figure 8: Comparison of Coefficients from MLR and LASSO](Predictors_plot.png)," illustrates how LASSO's regularization alters the coefficients:

- **Reduction in Coefficient Size**: Often, LASSO's coefficients are smaller than those in MLR. This reduction indicates that LASSO, through its shrinkage technique, lessens the influence of many predictors on the wage outcome. It's a conservative approach that helps prevent the model from fitting too closely to the training data, thus reducing the risk of overfitting.

- **Selective Feature Retention**: LASSO may reduce some predictors' coefficients to zero—effectively removing them from the model—if they are not statistically significant. This feature selection is critical in complex datasets like RetSchool, where simplifying the model can lead to clearer, more actionable insights.

- **Stability Across Models**: Predictors whose coefficients are consistent across both MLR and LASSO are likely very reliable indicators of wage variations, underscoring their importance regardless of the modeling approach used.

- **Enhanced Interpretability**: LASSO improves the model's clarity by focusing only on significant predictors. This makes it easier for analysts and decision-makers to understand which factors truly influence wages, facilitating more informed decisions.


#### Advantages of LASSO Over MLR in Educational Data Analysis

In the context of the RetSchool dataset, which explores factors influencing educational returns in the labor market, LASSO offers several distinct advantages:

- **Effective Feature Selection**: Unlike MLR, which might incorporate every predictor into the analysis, LASSO strategically eliminates less relevant variables. This focus helps distill the model to the most impactful factors, reducing complexity and improving the clarity of results.

- **Managing Multicollinearity**: LASSO addresses the challenge of multicollinearity common in educational data, where variables such as years of education, urban residency, and parental background might be interrelated. By penalizing coefficients, LASSO minimizes redundant information, ensuring that each included predictor contributes uniquely to understanding wage disparities.

- **Improving Model Interpretability**: The simplification LASSO brings allows stakeholders to better understand the dynamics within the data. For example, it becomes clearer how factors like education level, race, and geographic location impact wages independently of each other.


### Conclusion and Insights from the Return to School Dataset

Our application of LASSO regression has uncovered significant factors that influenced wages in 1976, notably highlighting how educational attainment and age directly impact earnings. These findings not only confirm the importance of these predictors but also help us understand the extent to which they affect wage disparities.

#### Key Insights:
- **Educational Impact on Earnings**: Higher educational levels consistently lead to increased wages, affirming that education is a crucial investment with substantial returns.

- **Age and Earnings**: Older individuals generally earn more, likely reflecting the combined effects of increased experience and education over time.

#### Advantages of Using LASSO Regression:

LASSO regression has effectively simplified the analysis by focusing on the most impactful factors, reducing complexity and enhancing our model’s interpretability:

- **Selective Feature Retention**: By minimizing the influence of less significant variables, LASSO has allowed us to concentrate on factors that truly affect wages.

- **Overfitting Mitigation**: This approach ensures our predictions are robust, avoiding the common pitfall of overfitting associated with more traditional regression models.

#### Visualization of Continuous Variables:
We have illustrated the impacts of education and age through detailed visualizations:

- **Impact of Education**: Demonstrates a clear, positive correlation between education and wages, with incremental educational achievements leading to higher earnings.

- **Role of Age in Earnings**: Shows that wage increases with age, highlighting the value of experience and possibly greater educational attainment over time.



```{r continuous-impact, fig.cap="Figure 9:Impact of Continuous Variables on Wages",fig.width=10, fig.height=5}
plot_grade76 <- ggplot(df_clean, aes(x = grade76, y = wage76)) +
  geom_point(aes(color = age76), alpha = 0.6) +
  geom_smooth(method = "lm", color = "blue", se = TRUE) +
  labs(title = "Impact of Education on Wages by Age",
       subtitle = "Each point represents an observation colored by age",
       x = "Years of Education (grade76)",
       y = "Wage in 1976",
       color = "Age") +
  theme_minimal() +
  theme(legend.position = "bottom")

# Create a scatter plot of age76 vs. wage76 with a regression line
plot_age76 <- ggplot(df_clean, aes(x = age76, y = wage76)) +
  geom_point(aes(color = grade76), alpha = 0.6) +
  geom_smooth(method = "lm", color = "red", se = TRUE) +
  labs(title = "Impact of Age on Wages by Education",
       subtitle = "Each point represents an observation colored by years of education",
       x = "Age in 1976",
       y = "Wage in 1976",
       color = "Education Level (grade76)") +
  theme_minimal() +
  theme(legend.position = "bottom")
library(gridExtra)
grade76_plot <- grid.arrange(plot_grade76, plot_age76, ncol = 2)
ggsave("grade76_plot.png", plot = grade76_plot, width = 10, height = 5, dpi = 300)
```

These visual aids [Figure 9:Impact of Continuous Variables on Wages](grade76_plot.png) are crucial for demonstrating how incremental changes in age and education relate to changes in wages, providing a compelling argument for the continuous benefits of education.

### Implications and Future Directions

This analysis provides valuable insights for policymakers and educational institutions, suggesting that enhancing access to education can lead to significant economic benefits. Future research could explore the long-term trends in these variables or investigate other factors that might influence wages, such as technological changes or economic conditions.

In conclusion, the detailed examination of the Return to School dataset using LASSO regression offers actionable insights that could significantly influence future educational investments and economic policy planning. Our study confirms the powerful impact of demographic factors on wages and sets the stage for further research in this vital area.











